{
  "repo_name": "zylon-ai/private-gpt",
  "repo_url": "https://github.com/zylon-ai/private-gpt",
  "description": "Interact with your documents using the power of GPT, 100% privately, no data leaks",
  "stars": 55463,
  "language": "Python",
  "created_at": "2023-05-02T09:15:31Z",
  "updated_at": "2025-03-19T06:30:31Z",
  "files": {
    "tests/__init__.py": "\"\"\"Tests.\"\"\"\n",
    "tests/conftest.py": "import os\nimport pathlib\nfrom glob import glob\n\nroot_path = pathlib.Path(__file__).parents[1]\n# This is to prevent a bug in intellij that uses the wrong working directory\nos.chdir(root_path)\n\n\ndef _as_module(fixture_path: str) -> str:\n    return fixture_path.replace(\"/\", \".\").replace(\"\\\\\", \".\").replace(\".py\", \"\")\n\n\npytest_plugins = [_as_module(fixture) for fixture in glob(\"tests/fixtures/[!_]*.py\")]\n",
    "tests/fixtures/__init__.py": "\"\"\"Global fixtures.\"\"\"\n",
    "tests/fixtures/auto_close_qdrant.py": "import pytest\n\nfrom private_gpt.components.vector_store.vector_store_component import (\n    VectorStoreComponent,\n)\nfrom tests.fixtures.mock_injector import MockInjector\n\n\n@pytest.fixture(autouse=True)\ndef _auto_close_vector_store_client(injector: MockInjector) -> None:\n    \"\"\"Auto close VectorStore client after each test.\n\n    VectorStore client (qdrant/chromadb) opens a connection the\n    Database that causes issues when running tests too fast,\n    so close explicitly after each test.\n    \"\"\"\n    yield\n    injector.get(VectorStoreComponent).close()\n",
    "tests/fixtures/fast_api_test_client.py": "import pytest\nfrom fastapi.testclient import TestClient\n\nfrom private_gpt.launcher import create_app\nfrom tests.fixtures.mock_injector import MockInjector\n\n\n@pytest.fixture\ndef test_client(request: pytest.FixtureRequest, injector: MockInjector) -> TestClient:\n    if request is not None and hasattr(request, \"param\"):\n        injector.bind_settings(request.param or {})\n\n    app_under_test = create_app(injector.test_injector)\n    return TestClient(app_under_test)\n",
    "tests/fixtures/ingest_helper.py": "from pathlib import Path\n\nimport pytest\nfrom fastapi.testclient import TestClient\n\nfrom private_gpt.server.ingest.ingest_router import IngestResponse\n\n\nclass IngestHelper:\n    def __init__(self, test_client: TestClient):\n        self.test_client = test_client\n\n    def ingest_file(self, path: Path) -> IngestResponse:\n        files = {\"file\": (path.name, path.open(\"rb\"))}\n\n        response = self.test_client.post(\"/v1/ingest/file\", files=files)\n        assert response.status_code == 200\n        ingest_result = IngestResponse.model_validate(response.json())\n        return ingest_result\n\n\n@pytest.fixture\ndef ingest_helper(test_client: TestClient) -> IngestHelper:\n    return IngestHelper(test_client)\n",
    "tests/fixtures/mock_injector.py": "from collections.abc import Callable\nfrom typing import Any\nfrom unittest.mock import MagicMock\n\nimport pytest\nfrom injector import Provider, ScopeDecorator, singleton\n\nfrom private_gpt.di import create_application_injector\nfrom private_gpt.settings.settings import Settings, unsafe_settings\nfrom private_gpt.settings.settings_loader import merge_settings\nfrom private_gpt.utils.typing import T\n\n\nclass MockInjector:\n    def __init__(self) -> None:\n        self.test_injector = create_application_injector()\n\n    def bind_mock(\n        self,\n        interface: type[T],\n        mock: (T | (Callable[..., T] | Provider[T])) | None = None,\n        *,\n        scope: ScopeDecorator = singleton,\n    ) -> T:\n        if mock is None:\n            mock = MagicMock()\n        self.test_injector.binder.bind(interface, to=mock, scope=scope)\n        return mock  # type: ignore\n\n    def bind_settings(self, settings: dict[str, Any]) -> Settings:\n        merged = merge_settings([unsafe_settings, settings])\n        new_settings = Settings(**merged)\n        self.test_injector.binder.bind(Settings, new_settings)\n        return new_settings\n\n    def get(self, interface: type[T]) -> T:\n        return self.test_injector.get(interface)\n\n\n@pytest.fixture\ndef injector() -> MockInjector:\n    return MockInjector()\n",
    "tests/server/chat/test_chat_routes.py": "from fastapi.testclient import TestClient\n\nfrom private_gpt.open_ai.openai_models import OpenAICompletion, OpenAIMessage\nfrom private_gpt.server.chat.chat_router import ChatBody\n\n\ndef test_chat_route_produces_a_stream(test_client: TestClient) -> None:\n    body = ChatBody(\n        messages=[OpenAIMessage(content=\"test\", role=\"user\")],\n        use_context=False,\n        stream=True,\n    )\n    response = test_client.post(\"/v1/chat/completions\", json=body.model_dump())\n\n    raw_events = response.text.split(\"\\n\\n\")\n    events = [\n        item.removeprefix(\"data: \") for item in raw_events if item.startswith(\"data: \")\n    ]\n    assert response.status_code == 200\n    assert \"text/event-stream\" in response.headers[\"content-type\"]\n    assert len(events) > 0\n    assert events[-1] == \"[DONE]\"\n\n\ndef test_chat_route_produces_a_single_value(test_client: TestClient) -> None:\n    body = ChatBody(\n        messages=[OpenAIMessage(content=\"test\", role=\"user\")],\n        use_context=False,\n        stream=False,\n    )\n    response = test_client.post(\"/v1/chat/completions\", json=body.model_dump())\n\n    # No asserts, if it validates it's good\n    OpenAICompletion.model_validate(response.json())\n    assert response.status_code == 200\n",
    "tests/server/chunks/test_chunk_routes.py": "from pathlib import Path\n\nfrom fastapi.testclient import TestClient\n\nfrom private_gpt.server.chunks.chunks_router import ChunksBody, ChunksResponse\nfrom tests.fixtures.ingest_helper import IngestHelper\n\n\ndef test_chunks_retrieval(test_client: TestClient, ingest_helper: IngestHelper) -> None:\n    # Make sure there is at least some chunk to query in the database\n    path = Path(__file__).parents[0] / \"chunk_test.txt\"\n    ingest_helper.ingest_file(path)\n\n    body = ChunksBody(text=\"b483dd15-78c4-4d67-b546-21a0d690bf43\")\n    response = test_client.post(\"/v1/chunks\", json=body.model_dump())\n    assert response.status_code == 200\n    chunk_response = ChunksResponse.model_validate(response.json())\n    assert len(chunk_response.data) > 0\n",
    "tests/server/embeddings/test_embedding_routes.py": "from fastapi.testclient import TestClient\n\nfrom private_gpt.server.embeddings.embeddings_router import (\n    EmbeddingsBody,\n    EmbeddingsResponse,\n)\n\n\ndef test_embeddings_generation(test_client: TestClient) -> None:\n    body = EmbeddingsBody(input=\"Embed me\")\n    response = test_client.post(\"/v1/embeddings\", json=body.model_dump())\n\n    assert response.status_code == 200\n    embedding_response = EmbeddingsResponse.model_validate(response.json())\n    assert len(embedding_response.data) > 0\n    assert len(embedding_response.data[0].embedding) > 0\n"
  },
  "requirements": null
}