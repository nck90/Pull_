{
  "repo_name": "PaddlePaddle/PaddleOCR",
  "repo_url": "https://github.com/PaddlePaddle/PaddleOCR",
  "description": "Awesome multilingual OCR toolkits based on PaddlePaddle (practical ultra lightweight OCR system, support 80+ languages recognition, provide data annotation and synthesis tools, support training and deployment among server, mobile, embedded and IoT devices)",
  "stars": 47478,
  "language": "Python",
  "created_at": "2020-05-08T10:38:16Z",
  "updated_at": "2025-03-19T06:54:53Z",
  "files": {
    "deploy/slim/auto_compression/test_ocr.py": "# Copyright (c) 2023 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport argparse\nimport time\nimport os\nimport sys\nimport cv2\nimport numpy as np\nimport paddle\nimport logging\nimport numpy as np\nimport argparse\nfrom tqdm import tqdm\nimport paddle\nfrom paddleslim.common import load_config as load_slim_config\nfrom paddleslim.common import get_logger\n\nimport sys\n\nsys.path.append(\"../../../\")\nfrom ppocr.data import build_dataloader\nfrom ppocr.postprocess import build_post_process\nfrom ppocr.metrics import build_metric\n\nfrom paddle.inference import create_predictor, PrecisionType\nfrom paddle.inference import Config as PredictConfig\n\nlogger = get_logger(__name__, level=logging.INFO)\n\n\ndef find_images_with_bounding_size(dataset: paddle.io.Dataset):\n    max_length_index = -1\n    max_width_index = -1\n    min_length_index = -1\n    min_width_index = -1\n\n    max_length = float(\"-inf\")\n    max_width = float(\"-inf\")\n    min_length = float(\"inf\")\n    min_width = float(\"inf\")\n    for idx, data in enumerate(dataset):\n        image = np.array(data[0])\n        h, w = image.shape[-2:]\n        if h > max_length:\n            max_length = h\n            max_length_index = idx\n        if w > max_width:\n            max_width = w\n            max_width_index = idx\n        if h < min_length:\n            min_length = h\n            min_length_index = idx\n        if w < min_width:\n            min_width = w\n            min_width_index = idx\n    print(f\"Found max image length: {max_length}, index: {max_length_index}\")\n    print(f\"Found max image width: {max_width}, index: {max_width_index}\")\n    print(f\"Found min image length: {min_length}, index: {min_length_index}\")\n    print(f\"Found min image width: {min_width}, index: {min_width_index}\")\n    return paddle.io.Subset(\n        dataset, [max_width_index, max_length_index, min_width_index, min_length_index]\n    )\n\n\ndef load_predictor(args):\n    \"\"\"\n    load predictor func\n    \"\"\"\n    rerun_flag = False\n    model_file = os.path.join(args.model_path, args.model_filename)\n    params_file = os.path.join(args.model_path, args.params_filename)\n    pred_cfg = PredictConfig(model_file, params_file)\n    pred_cfg.enable_memory_optim()\n    pred_cfg.switch_ir_optim(True)\n    if args.device == \"GPU\":\n        pred_cfg.enable_use_gpu(100, 0)\n    else:\n        pred_cfg.disable_gpu()\n        pred_cfg.set_cpu_math_library_num_threads(args.cpu_threads)\n        if args.use_mkldnn:\n            pred_cfg.enable_mkldnn()\n            if args.precision == \"int8\":\n                pred_cfg.enable_mkldnn_int8({\"conv2d\"})\n\n            if global_config[\"model_type\"] == \"rec\":\n                # delete pass which influence the accuracy, please refer to https://github.com/PaddlePaddle/Paddle/issues/55290\n                pred_cfg.delete_pass(\"fc_mkldnn_pass\")\n                pred_cfg.delete_pass(\"fc_act_mkldnn_fuse_pass\")\n\n    if args.use_trt:\n        # To collect the dynamic shapes of inputs for TensorRT engine\n        dynamic_shape_file = os.path.join(args.model_path, \"dynamic_shape.txt\")\n        if os.path.exists(dynamic_shape_file):\n            pred_cfg.enable_tuned_tensorrt_dynamic_shape(dynamic_shape_file, True)\n            print(\"trt set dynamic shape done!\")\n            precision_map = {\n                \"fp16\": PrecisionType.Half,\n                \"fp32\": PrecisionType.Float32,\n                \"int8\": PrecisionType.Int8,\n            }\n            if (\n                args.precision == \"int8\"\n                and \"ppocrv4_det_server_qat_dist.yaml\" in args.config_path\n            ):\n                # Use the following settings only when the hardware is a Tesla V100. If you are using\n                # a RTX 3090, use the settings in the else branch.\n                pred_cfg.enable_tensorrt_engine(\n                    workspace_size=1 << 30,\n                    max_batch_size=1,\n                    min_subgraph_size=30,\n                    precision_mode=precision_map[args.precision],\n                    use_static=True,\n                    use_calib_mode=False,\n                )\n                pred_cfg.exp_disable_tensorrt_ops([\"elementwise_add\"])\n            else:\n                pred_cfg.enable_tensorrt_engine(\n                    workspace_size=1 << 30,\n                    max_batch_size=1,\n                    min_subgraph_size=4,\n                    precision_mode=precision_map[args.precision],\n                    use_static=True,\n                    use_calib_mode=False,\n                )\n        else:\n            # pred_cfg.disable_gpu()\n            # pred_cfg.set_cpu_math_library_num_threads(24)\n            pred_cfg.collect_shape_range_info(dynamic_shape_file)\n            print(\"Start collect dynamic shape...\")\n            rerun_flag = True\n\n    predictor = create_predictor(pred_cfg)\n    return predictor, rerun_flag\n\n\ndef eval(args):\n    \"\"\"\n    eval mIoU func\n    \"\"\"\n    # DataLoader need run on cpu\n    paddle.set_device(\"cpu\")\n    devices = paddle.device.get_device().split(\":\")[0]\n\n    val_loader = build_dataloader(all_config, \"Eval\", devices, logger)\n    post_process_class = build_post_process(all_config[\"PostProcess\"], global_config)\n    eval_class = build_metric(all_config[\"Metric\"])\n    model_type = global_config[\"model_type\"]\n\n    predictor, rerun_flag = load_predictor(args)\n\n    if rerun_flag:\n        eval_dataset = find_images_with_bounding_size(val_loader.dataset)\n        batch_sampler = paddle.io.BatchSampler(\n            eval_dataset, batch_size=1, shuffle=False, drop_last=False\n        )\n        val_loader = paddle.io.DataLoader(\n            eval_dataset, batch_sampler=batch_sampler, num_workers=4, return_list=True\n        )\n\n    input_names = predictor.get_input_names()\n    input_handle = predictor.get_input_handle(input_names[0])\n    output_names = predictor.get_output_names()\n    output_handle = predictor.get_output_handle(output_names[0])\n    sample_nums = len(val_loader)\n    predict_time = 0.0\n    time_min = float(\"inf\")\n    time_max = float(\"-inf\")\n    print(\"Start evaluating ( total_iters: {}).\".format(sample_nums))\n\n    for batch_id, batch in enumerate(val_loader):\n        images = np.array(batch[0])\n\n        batch_numpy = []\n        for item in batch:\n            batch_numpy.append(np.array(item))\n\n        # ori_shape = np.array(batch_numpy).shape[-2:]\n        input_handle.reshape(images.shape)\n        input_handle.copy_from_cpu(images)\n        start_time = time.time()\n\n        predictor.run()\n        preds = output_handle.copy_to_cpu()\n\n        end_time = time.time()\n        timed = end_time - start_time\n        time_min = min(time_min, timed)\n        time_max = max(time_max, timed)\n        predict_time += timed\n\n        if model_type == \"det\":\n            preds_map = {\"maps\": preds}\n            post_result = post_process_class(preds_map, batch_numpy[1])\n            eval_class(post_result, batch_numpy)\n        elif model_type == \"rec\":\n            post_result = post_process_class(preds, batch_numpy[1])\n            eval_class(post_result, batch_numpy)\n\n        if rerun_flag:\n            if batch_id == 3:\n                print(\n                    \"***** Collect dynamic shape done, Please rerun the program to get correct results. *****\"\n                )\n                return\n        if batch_id % 100 == 0:\n            print(\"Eval iter:\", batch_id)\n            sys.stdout.flush()\n\n    metric = eval_class.get_metric()\n\n    time_avg = predict_time / sample_nums\n    print(\n        \"[Benchmark] Inference time(ms): min={}, max={}, avg={}\".format(\n            round(time_min * 1000, 2),\n            round(time_max * 1000, 1),\n            round(time_avg * 1000, 1),\n        )\n    )\n    for k, v in metric.items():\n        print(\"{}:{}\".format(k, v))\n    sys.stdout.flush()\n\n\ndef main():\n    global all_config, global_config\n    all_config = load_slim_config(args.config_path)\n    global_config = all_config[\"Global\"]\n    eval(args)\n\n\nif __name__ == \"__main__\":\n    paddle.enable_static()\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--model_path\", type=str, help=\"inference model filepath\")\n    parser.add_argument(\n        \"--config_path\",\n        type=str,\n        default=\"./configs/ppocrv3_det_qat_dist.yaml\",\n        help=\"path of compression strategy config.\",\n    )\n    parser.add_argument(\n        \"--model_filename\",\n        type=str,\n        default=\"inference.pdmodel\",\n        help=\"model file name\",\n    )\n    parser.add_argument(\n        \"--params_filename\",\n        type=str,\n        default=\"inference.pdiparams\",\n        help=\"params file name\",\n    )\n    parser.add_argument(\n        \"--device\",\n        type=str,\n        default=\"GPU\",\n        choices=[\"CPU\", \"GPU\"],\n        help=\"Choose the device you want to run, it can be: CPU/GPU, default is GPU\",\n    )\n    parser.add_argument(\n        \"--precision\",\n        type=str,\n        default=\"fp32\",\n        choices=[\"fp32\", \"fp16\", \"int8\"],\n        help=\"The precision of inference. It can be 'fp32', 'fp16' or 'int8'. Default is 'fp16'.\",\n    )\n    parser.add_argument(\n        \"--use_trt\",\n        type=bool,\n        default=False,\n        help=\"Whether to use tensorrt engine or not.\",\n    )\n    parser.add_argument(\n        \"--use_mkldnn\", type=bool, default=False, help=\"Whether use mkldnn or not.\"\n    )\n    parser.add_argument(\n        \"--cpu_threads\", type=int, default=10, help=\"Num of cpu threads.\"\n    )\n    args = parser.parse_args()\n    main()\n",
    "test_tipc/compare_results.py": "import numpy as np\nimport os\nimport subprocess\nimport json\nimport argparse\nimport glob\n\n\ndef init_args():\n    parser = argparse.ArgumentParser()\n    # params for testing assert allclose\n    parser.add_argument(\"--atol\", type=float, default=1e-3)\n    parser.add_argument(\"--rtol\", type=float, default=1e-3)\n    parser.add_argument(\"--gt_file\", type=str, default=\"\")\n    parser.add_argument(\"--log_file\", type=str, default=\"\")\n    parser.add_argument(\"--precision\", type=str, default=\"fp32\")\n    return parser\n\n\ndef parse_args():\n    parser = init_args()\n    return parser.parse_args()\n\n\ndef run_shell_command(cmd):\n    p = subprocess.Popen(\n        cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True\n    )\n    out, err = p.communicate()\n\n    if p.returncode == 0:\n        return out.decode(\"utf-8\")\n    else:\n        return None\n\n\ndef parser_results_from_log_by_name(log_path, names_list):\n    if not os.path.exists(log_path):\n        raise ValueError(\"The log file {} does not exists!\".format(log_path))\n\n    if names_list is None or len(names_list) < 1:\n        return []\n\n    parser_results = {}\n    for name in names_list:\n        cmd = \"grep {} {}\".format(name, log_path)\n        outs = run_shell_command(cmd)\n        outs = outs.split(\"\\n\")[0]\n        result = outs.split(\"{}\".format(name))[-1]\n        try:\n            result = json.loads(result)\n        except:\n            result = np.array([int(r) for r in result.split()]).reshape(-1, 4)\n        parser_results[name] = result\n    return parser_results\n\n\ndef load_gt_from_file(gt_file):\n    if not os.path.exists(gt_file):\n        raise ValueError(\"The log file {} does not exists!\".format(gt_file))\n    with open(gt_file, \"r\") as f:\n        data = f.readlines()\n        f.close()\n    parser_gt = {}\n    for line in data:\n        image_name, result = line.strip(\"\\n\").split(\"\\t\")\n        image_name = image_name.split(\"/\")[-1]\n        try:\n            result = json.loads(result)\n        except:\n            result = np.array([int(r) for r in result.split()]).reshape(-1, 4)\n        parser_gt[image_name] = result\n    return parser_gt\n\n\ndef load_gt_from_txts(gt_file):\n    gt_list = glob.glob(gt_file)\n    gt_collection = {}\n    for gt_f in gt_list:\n        gt_dict = load_gt_from_file(gt_f)\n        basename = os.path.basename(gt_f)\n        if \"fp32\" in basename:\n            gt_collection[\"fp32\"] = [gt_dict, gt_f]\n        elif \"fp16\" in basename:\n            gt_collection[\"fp16\"] = [gt_dict, gt_f]\n        elif \"int8\" in basename:\n            gt_collection[\"int8\"] = [gt_dict, gt_f]\n        else:\n            continue\n    return gt_collection\n\n\ndef collect_predict_from_logs(log_path, key_list):\n    log_list = glob.glob(log_path)\n    pred_collection = {}\n    for log_f in log_list:\n        pred_dict = parser_results_from_log_by_name(log_f, key_list)\n        key = os.path.basename(log_f)\n        pred_collection[key] = pred_dict\n\n    return pred_collection\n\n\ndef testing_assert_allclose(dict_x, dict_y, atol=1e-7, rtol=1e-7):\n    for k in dict_x:\n        np.testing.assert_allclose(\n            np.array(dict_x[k]), np.array(dict_y[k]), atol=atol, rtol=rtol\n        )\n\n\nif __name__ == \"__main__\":\n    # Usage:\n    # python3.7 tests/compare_results.py --gt_file=./tests/results/*.txt  --log_file=./tests/output/infer_*.log\n\n    args = parse_args()\n\n    gt_collection = load_gt_from_txts(args.gt_file)\n    key_list = gt_collection[\"fp32\"][0].keys()\n\n    pred_collection = collect_predict_from_logs(args.log_file, key_list)\n    for filename in pred_collection.keys():\n        if \"fp32\" in filename:\n            gt_dict, gt_filename = gt_collection[\"fp32\"]\n        elif \"fp16\" in filename:\n            gt_dict, gt_filename = gt_collection[\"fp16\"]\n        elif \"int8\" in filename:\n            gt_dict, gt_filename = gt_collection[\"int8\"]\n        else:\n            continue\n        pred_dict = pred_collection[filename]\n\n        try:\n            testing_assert_allclose(gt_dict, pred_dict, atol=args.atol, rtol=args.rtol)\n            print(\n                \"Assert allclose passed! The results of {} and {} are consistent!\".format(\n                    filename, gt_filename\n                )\n            )\n        except Exception as E:\n            print(E)\n            raise ValueError(\n                \"The results of {} and the results of {} are inconsistent!\".format(\n                    filename, gt_filename\n                )\n            )\n",
    "test_tipc/supplementary/config.py": "import numpy as np\nimport os\nimport sys\nimport platform\nimport yaml\nimport time\nimport shutil\nimport paddle\nimport paddle.distributed as dist\nfrom tqdm import tqdm\nfrom argparse import ArgumentParser, RawDescriptionHelpFormatter\nfrom utils import get_logger, print_dict\n\n\nclass ArgsParser(ArgumentParser):\n    def __init__(self):\n        super(ArgsParser, self).__init__(formatter_class=RawDescriptionHelpFormatter)\n        self.add_argument(\"-c\", \"--config\", help=\"configuration file to use\")\n        self.add_argument(\"-o\", \"--opt\", nargs=\"+\", help=\"set configuration options\")\n        self.add_argument(\n            \"-p\",\n            \"--profiler_options\",\n            type=str,\n            default=None,\n            help='The option of profiler, which should be in format \"key1=value1;key2=value2;key3=value3\".',\n        )\n\n    def parse_args(self, argv=None):\n        args = super(ArgsParser, self).parse_args(argv)\n        assert args.config is not None, \"Please specify --config=configure_file_path.\"\n        args.opt = self._parse_opt(args.opt)\n        return args\n\n    def _parse_opt(self, opts):\n        config = {}\n        if not opts:\n            return config\n        for s in opts:\n            s = s.strip()\n            k, v = s.split(\"=\")\n            config[k] = yaml.load(v, Loader=yaml.Loader)\n        return config\n\n\nclass AttrDict(dict):\n    \"\"\"Single level attribute dict, NOT recursive\"\"\"\n\n    def __init__(self, **kwargs):\n        super(AttrDict, self).__init__()\n        super(AttrDict, self).update(kwargs)\n\n    def __getattr__(self, key):\n        if key in self:\n            return self[key]\n        raise AttributeError(\"object has no attribute '{}'\".format(key))\n\n\nglobal_config = AttrDict()\n\ndefault_config = {\n    \"Global\": {\n        \"debug\": False,\n    }\n}\n\n\ndef load_config(file_path):\n    \"\"\"\n    Load config from yml/yaml file.\n    Args:\n        file_path (str): Path of the config file to be loaded.\n    Returns: global config\n    \"\"\"\n    merge_config(default_config)\n    _, ext = os.path.splitext(file_path)\n    assert ext in [\".yml\", \".yaml\"], \"only support yaml files for now\"\n    merge_config(yaml.load(open(file_path, \"rb\"), Loader=yaml.Loader))\n    return global_config\n\n\ndef merge_config(config):\n    \"\"\"\n    Merge config into global config.\n    Args:\n        config (dict): Config to be merged.\n    Returns: global config\n    \"\"\"\n    for key, value in config.items():\n        if \".\" not in key:\n            if isinstance(value, dict) and key in global_config:\n                global_config[key].update(value)\n            else:\n                global_config[key] = value\n        else:\n            sub_keys = key.split(\".\")\n            assert (\n                sub_keys[0] in global_config\n            ), \"the sub_keys can only be one of global_config: {}, but get: {}, please check your running command\".format(\n                global_config.keys(), sub_keys[0]\n            )\n            cur = global_config[sub_keys[0]]\n            for idx, sub_key in enumerate(sub_keys[1:]):\n                if idx == len(sub_keys) - 2:\n                    cur[sub_key] = value\n                else:\n                    cur = cur[sub_key]\n\n\ndef preprocess(is_train=False):\n    FLAGS = ArgsParser().parse_args()\n    profiler_options = FLAGS.profiler_options\n    config = load_config(FLAGS.config)\n    merge_config(FLAGS.opt)\n    profile_dic = {\"profiler_options\": FLAGS.profiler_options}\n    merge_config(profile_dic)\n\n    if is_train:\n        # save_config\n        save_model_dir = config[\"save_model_dir\"]\n        os.makedirs(save_model_dir, exist_ok=True)\n        with open(os.path.join(save_model_dir, \"config.yml\"), \"w\") as f:\n            yaml.dump(dict(config), f, default_flow_style=False, sort_keys=False)\n        log_file = \"{}/train.log\".format(save_model_dir)\n    else:\n        log_file = None\n    logger = get_logger(log_file=log_file)\n\n    # check if set use_gpu=True in paddlepaddle cpu version\n    use_gpu = config[\"use_gpu\"]\n\n    print_dict(config, logger)\n\n    return config, logger\n\n\nif __name__ == \"__main__\":\n    config, logger = preprocess(is_train=False)\n    # print(config)\n",
    "test_tipc/supplementary/custom_op/test.py": "import paddle\nimport paddle.nn as nn\nfrom paddle.vision.transforms import Compose, Normalize\nfrom paddle.utils.cpp_extension import load\nfrom paddle.inference import Config\nfrom paddle.inference import create_predictor\nimport numpy as np\n\nEPOCH_NUM = 4\nBATCH_SIZE = 64\n\n# jit compile custom op\ncustom_ops = load(\n    name=\"custom_jit_ops\", sources=[\"custom_relu_op.cc\", \"custom_relu_op.cu\"]\n)\n\n\nclass LeNet(nn.Layer):\n    def __init__(self):\n        super(LeNet, self).__init__()\n        self.conv1 = nn.Conv2D(\n            in_channels=1, out_channels=6, kernel_size=5, stride=1, padding=2\n        )\n        self.max_pool1 = nn.MaxPool2D(kernel_size=2, stride=2)\n        self.conv2 = nn.Conv2D(in_channels=6, out_channels=16, kernel_size=5, stride=1)\n        self.max_pool2 = nn.MaxPool2D(kernel_size=2, stride=2)\n        self.linear1 = nn.Linear(in_features=16 * 5 * 5, out_features=120)\n        self.linear2 = nn.Linear(in_features=120, out_features=84)\n        self.linear3 = nn.Linear(in_features=84, out_features=10)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = custom_ops.custom_relu(x)\n        x = self.max_pool1(x)\n        x = custom_ops.custom_relu(x)\n        x = self.conv2(x)\n        x = self.max_pool2(x)\n        x = paddle.flatten(x, start_axis=1, stop_axis=-1)\n        x = self.linear1(x)\n        x = custom_ops.custom_relu(x)\n        x = self.linear2(x)\n        x = custom_ops.custom_relu(x)\n        x = self.linear3(x)\n        return x\n\n\n# set device\npaddle.set_device(\"gpu\")\n\n# model\nnet = LeNet()\nloss_fn = nn.CrossEntropyLoss()\nopt = paddle.optimizer.Adam(learning_rate=0.001, parameters=net.parameters())\n\n# data loader\ntransform = Compose([Normalize(mean=[127.5], std=[127.5], data_format=\"CHW\")])\ntrain_dataset = paddle.vision.datasets.MNIST(mode=\"train\", transform=transform)\ntrain_loader = paddle.io.DataLoader(\n    train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True, num_workers=2\n)\n\n# train\nfor epoch_id in range(EPOCH_NUM):\n    for batch_id, (image, label) in enumerate(train_loader()):\n        out = net(image)\n        loss = loss_fn(out, label)\n        loss.backward()\n\n        if batch_id % 300 == 0:\n            print(\n                \"Epoch {} batch {}: loss = {}\".format(\n                    epoch_id, batch_id, np.mean(loss.numpy())\n                )\n            )\n\n        opt.step()\n        opt.clear_grad()\n",
    "test_tipc/supplementary/data.py": "import numpy as np\nimport paddle\nimport os\nimport cv2\nimport glob\n\n\ndef transform(data, ops=None):\n    \"\"\"transform\"\"\"\n    if ops is None:\n        ops = []\n    for op in ops:\n        data = op(data)\n        if data is None:\n            return None\n    return data\n\n\ndef create_operators(op_param_list, global_config=None):\n    \"\"\"\n    create operators based on the config\n    Args:\n        params(list): a dict list, used to create some operators\n    \"\"\"\n    assert isinstance(op_param_list, list), \"operator config should be a list\"\n    ops = []\n    for operator in op_param_list:\n        assert isinstance(operator, dict) and len(operator) == 1, \"yaml format error\"\n        op_name = list(operator)[0]\n        param = {} if operator[op_name] is None else operator[op_name]\n        if global_config is not None:\n            param.update(global_config)\n        op = eval(op_name)(**param)\n        ops.append(op)\n    return ops\n\n\nclass DecodeImage(object):\n    \"\"\"decode image\"\"\"\n\n    def __init__(self, img_mode=\"RGB\", channel_first=False, **kwargs):\n        self.img_mode = img_mode\n        self.channel_first = channel_first\n\n    def __call__(self, data):\n        img = data[\"image\"]\n        assert type(img) is bytes and len(img) > 0, \"invalid input 'img' in DecodeImage\"\n        img = np.frombuffer(img, dtype=\"uint8\")\n        img = cv2.imdecode(img, 1)\n        if img is None:\n            return None\n        if self.img_mode == \"GRAY\":\n            img = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n        elif self.img_mode == \"RGB\":\n            assert img.shape[2] == 3, \"invalid shape of image[%s]\" % (img.shape)\n            img = img[:, :, ::-1]\n\n        if self.channel_first:\n            img = img.transpose((2, 0, 1))\n\n        data[\"image\"] = img\n        data[\"src_image\"] = img\n        return data\n\n\nclass NormalizeImage(object):\n    \"\"\"normalize image such as subtract mean, divide std\"\"\"\n\n    def __init__(self, scale=None, mean=None, std=None, order=\"chw\", **kwargs):\n        if isinstance(scale, str):\n            scale = eval(scale)\n        self.scale = np.float32(scale if scale is not None else 1.0 / 255.0)\n        mean = mean if mean is not None else [0.485, 0.456, 0.406]\n        std = std if std is not None else [0.229, 0.224, 0.225]\n\n        shape = (3, 1, 1) if order == \"chw\" else (1, 1, 3)\n        self.mean = np.array(mean).reshape(shape).astype(\"float32\")\n        self.std = np.array(std).reshape(shape).astype(\"float32\")\n\n    def __call__(self, data):\n        img = data[\"image\"]\n        from PIL import Image\n\n        if isinstance(img, Image.Image):\n            img = np.array(img)\n        assert isinstance(img, np.ndarray), \"invalid input 'img' in NormalizeImage\"\n        data[\"image\"] = (img.astype(\"float32\") * self.scale - self.mean) / self.std\n        return data\n\n\nclass ToCHWImage(object):\n    \"\"\"convert hwc image to chw image\"\"\"\n\n    def __init__(self, **kwargs):\n        pass\n\n    def __call__(self, data):\n        img = data[\"image\"]\n        from PIL import Image\n\n        if isinstance(img, Image.Image):\n            img = np.array(img)\n        data[\"image\"] = img.transpose((2, 0, 1))\n\n        src_img = data[\"src_image\"]\n        from PIL import Image\n\n        if isinstance(img, Image.Image):\n            src_img = np.array(src_img)\n        data[\"src_image\"] = img.transpose((2, 0, 1))\n\n        return data\n\n\nclass SimpleDataset(nn.Dataset):\n    def __init__(self, config, mode, logger, seed=None):\n        self.logger = logger\n        self.mode = mode.lower()\n\n        data_dir = config[\"Train\"][\"data_dir\"]\n\n        imgs_list = self.get_image_list(data_dir)\n\n        self.ops = create_operators(cfg[\"transforms\"], None)\n\n    def get_image_list(self, img_dir):\n        imgs = glob.glob(os.path.join(img_dir, \"*.png\"))\n        if len(imgs) == 0:\n            raise ValueError(f\"not any images founded in {img_dir}\")\n        return imgs\n\n    def __getitem__(self, idx):\n        return None\n",
    "test_tipc/supplementary/data_loader.py": "import numpy as np\nfrom paddle.vision.datasets import Cifar100\nfrom paddle.vision.transforms import Normalize\nimport signal\nimport os\nfrom paddle.io import Dataset, DataLoader, DistributedBatchSampler\n\n\ndef term_mp(sig_num, frame):\n    \"\"\"kill all child processes\"\"\"\n    pid = os.getpid()\n    pgid = os.getpgid(os.getpid())\n    print(\"main proc {} exit, kill process group \" \"{}\".format(pid, pgid))\n    os.killpg(pgid, signal.SIGKILL)\n    return\n\n\ndef build_dataloader(mode, batch_size=4, seed=None, num_workers=0, device=\"gpu:0\"):\n    normalize = Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5], data_format=\"HWC\")\n\n    if mode.lower() == \"train\":\n        dataset = Cifar100(mode=mode, transform=normalize)\n    elif mode.lower() in [\"test\", \"valid\", \"eval\"]:\n        dataset = Cifar100(mode=\"test\", transform=normalize)\n    else:\n        raise ValueError(f\"{mode} should be one of ['train', 'test']\")\n\n    # define batch sampler\n    batch_sampler = DistributedBatchSampler(\n        dataset=dataset, batch_size=batch_size, shuffle=False, drop_last=True\n    )\n\n    data_loader = DataLoader(\n        dataset=dataset,\n        batch_sampler=batch_sampler,\n        places=device,\n        num_workers=num_workers,\n        return_list=True,\n        use_shared_memory=False,\n    )\n\n    # support exit using ctrl+c\n    signal.signal(signal.SIGINT, term_mp)\n    signal.signal(signal.SIGTERM, term_mp)\n\n    return data_loader\n\n\n# cifar100 = Cifar100(mode='train', transform=normalize)\n\n# data = cifar100[0]\n\n# image, label = data\n\n# reader = build_dataloader('train')\n\n# for idx, data in enumerate(reader):\n#     print(idx, data[0].shape, data[1].shape)\n#     if idx >= 10:\n#         break\n",
    "test_tipc/supplementary/load_cifar.py": "import pickle as p\nimport numpy as np\nfrom PIL import Image\n\n\ndef load_CIFAR_batch(filename):\n    \"\"\"load single batch of cifar\"\"\"\n    with open(filename, \"rb\") as f:\n        datadict = p.load(f, encoding=\"bytes\")\n        # 以字典的形式取出数据\n        X = datadict[b\"data\"]\n        Y = datadict[b\"fine_labels\"]\n        try:\n            X = X.reshape(10000, 3, 32, 32)\n        except:\n            X = X.reshape(50000, 3, 32, 32)\n        Y = np.array(Y)\n        print(Y.shape)\n        return X, Y\n\n\nif __name__ == \"__main__\":\n    mode = \"train\"\n    imgX, imgY = load_CIFAR_batch(f\"./cifar-100-python/{mode}\")\n    with open(f\"./cifar-100-python/{mode}_imgs/img_label.txt\", \"a+\") as f:\n        for i in range(imgY.shape[0]):\n            f.write(\"img\" + str(i) + \" \" + str(imgY[i]) + \"\\n\")\n\n    for i in range(imgX.shape[0]):\n        imgs = imgX[i]\n        img0 = imgs[0]\n        img1 = imgs[1]\n        img2 = imgs[2]\n        i0 = Image.fromarray(img0)\n        i1 = Image.fromarray(img1)\n        i2 = Image.fromarray(img2)\n        img = Image.merge(\"RGB\", (i0, i1, i2))\n        name = \"img\" + str(i) + \".png\"\n        img.save(f\"./cifar-100-python/{mode}_imgs/\" + name, \"png\")\n    print(\"save successfully!\")\n",
    "test_tipc/supplementary/loss.py": "import paddle\nimport paddle.nn.functional as F\n\n\nclass Loss(object):\n    \"\"\"\n    Loss\n    \"\"\"\n\n    def __init__(self, class_dim=1000, epsilon=None):\n        assert class_dim > 1, \"class_dim=%d is not larger than 1\" % (class_dim)\n        self._class_dim = class_dim\n        if epsilon is not None and epsilon >= 0.0 and epsilon <= 1.0:\n            self._epsilon = epsilon\n            self._label_smoothing = True\n        else:\n            self._epsilon = None\n            self._label_smoothing = False\n\n    def _labelsmoothing(self, target):\n        if target.shape[-1] != self._class_dim:\n            one_hot_target = F.one_hot(target, self._class_dim)\n        else:\n            one_hot_target = target\n        soft_target = F.label_smooth(one_hot_target, epsilon=self._epsilon)\n        soft_target = paddle.reshape(soft_target, shape=[-1, self._class_dim])\n        return soft_target\n\n    def _crossentropy(self, input, target, use_pure_fp16=False):\n        if self._label_smoothing:\n            target = self._labelsmoothing(target)\n            input = -F.log_softmax(input, axis=-1)\n            cost = paddle.sum(target * input, axis=-1)\n        else:\n            cost = F.cross_entropy(input=input, label=target)\n        if use_pure_fp16:\n            avg_cost = paddle.sum(cost)\n        else:\n            avg_cost = paddle.mean(cost)\n        return avg_cost\n\n    def __call__(self, input, target):\n        return self._crossentropy(input, target)\n\n\ndef build_loss(config, epsilon=None):\n    class_dim = config[\"class_dim\"]\n    loss_func = Loss(class_dim=class_dim, epsilon=epsilon)\n    return loss_func\n\n\nclass LossDistill(Loss):\n    def __init__(self, model_name_list, class_dim=1000, epsilon=None):\n        assert class_dim > 1, \"class_dim=%d is not larger than 1\" % (class_dim)\n        self._class_dim = class_dim\n        if epsilon is not None and epsilon >= 0.0 and epsilon <= 1.0:\n            self._epsilon = epsilon\n            self._label_smoothing = True\n        else:\n            self._epsilon = None\n            self._label_smoothing = False\n\n        self.model_name_list = model_name_list\n        assert len(self.model_name_list) > 1, \"error\"\n\n    def __call__(self, input, target):\n        losses = {}\n        for k in self.model_name_list:\n            inp = input[k]\n            losses[k] = self._crossentropy(inp, target)\n        return losses\n\n\nclass KLJSLoss(object):\n    def __init__(self, mode=\"kl\"):\n        assert mode in [\n            \"kl\",\n            \"js\",\n            \"KL\",\n            \"JS\",\n        ], \"mode can only be one of ['kl', 'js', 'KL', 'JS']\"\n        self.mode = mode\n\n    def __call__(self, p1, p2, reduction=\"mean\"):\n        p1 = F.softmax(p1, axis=-1)\n        p2 = F.softmax(p2, axis=-1)\n\n        loss = paddle.multiply(p2, paddle.log((p2 + 1e-5) / (p1 + 1e-5) + 1e-5))\n\n        if self.mode.lower() == \"js\":\n            loss += paddle.multiply(p1, paddle.log((p1 + 1e-5) / (p2 + 1e-5) + 1e-5))\n            loss *= 0.5\n        if reduction == \"mean\":\n            loss = paddle.mean(loss)\n        elif reduction == \"none\" or reduction is None:\n            return loss\n        else:\n            loss = paddle.sum(loss)\n        return loss\n\n\nclass DMLLoss(object):\n    def __init__(self, model_name_pairs, mode=\"js\"):\n        self.model_name_pairs = self._check_model_name_pairs(model_name_pairs)\n        self.kljs_loss = KLJSLoss(mode=mode)\n\n    def _check_model_name_pairs(self, model_name_pairs):\n        if not isinstance(model_name_pairs, list):\n            return []\n        elif isinstance(model_name_pairs[0], list) and isinstance(\n            model_name_pairs[0][0], str\n        ):\n            return model_name_pairs\n        else:\n            return [model_name_pairs]\n\n    def __call__(self, predicts, target=None):\n        loss_dict = dict()\n        for pairs in self.model_name_pairs:\n            p1 = predicts[pairs[0]]\n            p2 = predicts[pairs[1]]\n\n            loss_dict[pairs[0] + \"_\" + pairs[1]] = self.kljs_loss(p1, p2)\n\n        return loss_dict\n\n\n# def build_distill_loss(config, epsilon=None):\n#     class_dim = config['class_dim']\n#     loss = LossDistill(model_name_list=['student', 'student1'], )\n#     return loss_func\n",
    "test_tipc/supplementary/metric.py": "import paddle\nimport paddle.nn.functional as F\nfrom collections import OrderedDict\n\n\ndef create_metric(\n    out,\n    label,\n    architecture=None,\n    topk=5,\n    classes_num=1000,\n    use_distillation=False,\n    mode=\"train\",\n):\n    \"\"\"\n    Create measures of model accuracy, such as top1 and top5\n\n    Args:\n        out(variable): model output variable\n        feeds(dict): dict of model input variables(included label)\n        topk(int): usually top5\n        classes_num(int): num of classes\n        use_distillation(bool): whether to use distillation training\n        mode(str): mode, train/valid\n\n    Returns:\n        fetches(dict): dict of measures\n    \"\"\"\n    # if architecture[\"name\"] == \"GoogLeNet\":\n    #     assert len(out) == 3, \"GoogLeNet should have 3 outputs\"\n    #     out = out[0]\n    # else:\n    #     # just need student label to get metrics\n    #     if use_distillation:\n    #         out = out[1]\n    softmax_out = F.softmax(out)\n\n    fetches = OrderedDict()\n    # set top1 to fetches\n    top1 = paddle.metric.accuracy(softmax_out, label=label, k=1)\n    # set topk to fetches\n    k = min(topk, classes_num)\n    topk = paddle.metric.accuracy(softmax_out, label=label, k=k)\n\n    # multi cards' eval\n    if mode != \"train\" and paddle.distributed.get_world_size() > 1:\n        top1 = (\n            paddle.distributed.all_reduce(top1, op=paddle.distributed.ReduceOp.SUM)\n            / paddle.distributed.get_world_size()\n        )\n        topk = (\n            paddle.distributed.all_reduce(topk, op=paddle.distributed.ReduceOp.SUM)\n            / paddle.distributed.get_world_size()\n        )\n\n    fetches[\"top1\"] = top1\n    topk_name = \"top{}\".format(k)\n    fetches[topk_name] = topk\n\n    return fetches\n"
  },
  "requirements": "shapely\nscikit-image\npyclipper\nlmdb\ntqdm\nnumpy\nrapidfuzz\nopencv-python\nopencv-contrib-python\ncython\nPillow\npyyaml\nrequests\nalbumentations\n# to be compatible with albumentations\nalbucore\npackaging\n"
}