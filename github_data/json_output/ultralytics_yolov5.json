{
  "repo_name": "ultralytics/yolov5",
  "repo_url": "https://github.com/ultralytics/yolov5",
  "description": "YOLOv5 🚀 in PyTorch > ONNX > CoreML > TFLite",
  "stars": 52977,
  "language": "Python",
  "created_at": "2020-05-18T03:45:11Z",
  "updated_at": "2025-03-19T06:44:59Z",
  "files": {
    "benchmarks.py": "# Ultralytics 🚀 AGPL-3.0 License - https://ultralytics.com/license\n\"\"\"\nRun YOLOv5 benchmarks on all supported export formats.\n\nFormat                      | `export.py --include`         | Model\n---                         | ---                           | ---\nPyTorch                     | -                             | yolov5s.pt\nTorchScript                 | `torchscript`                 | yolov5s.torchscript\nONNX                        | `onnx`                        | yolov5s.onnx\nOpenVINO                    | `openvino`                    | yolov5s_openvino_model/\nTensorRT                    | `engine`                      | yolov5s.engine\nCoreML                      | `coreml`                      | yolov5s.mlpackage\nTensorFlow SavedModel       | `saved_model`                 | yolov5s_saved_model/\nTensorFlow GraphDef         | `pb`                          | yolov5s.pb\nTensorFlow Lite             | `tflite`                      | yolov5s.tflite\nTensorFlow Edge TPU         | `edgetpu`                     | yolov5s_edgetpu.tflite\nTensorFlow.js               | `tfjs`                        | yolov5s_web_model/\n\nRequirements:\n    $ pip install -r requirements.txt coremltools onnx onnx-simplifier onnxruntime openvino-dev tensorflow-cpu  # CPU\n    $ pip install -r requirements.txt coremltools onnx onnx-simplifier onnxruntime-gpu openvino-dev tensorflow  # GPU\n    $ pip install -U nvidia-tensorrt --index-url https://pypi.ngc.nvidia.com  # TensorRT\n\nUsage:\n    $ python benchmarks.py --weights yolov5s.pt --img 640\n\"\"\"\n\nimport argparse\nimport platform\nimport sys\nimport time\nfrom pathlib import Path\n\nimport pandas as pd\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0]  # YOLOv5 root directory\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\n# ROOT = ROOT.relative_to(Path.cwd())  # relative\n\nimport export\nfrom models.experimental import attempt_load\nfrom models.yolo import SegmentationModel\nfrom segment.val import run as val_seg\nfrom utils import notebook_init\nfrom utils.general import LOGGER, check_yaml, file_size, print_args\nfrom utils.torch_utils import select_device\nfrom val import run as val_det\n\n\ndef run(\n    weights=ROOT / \"yolov5s.pt\",  # weights path\n    imgsz=640,  # inference size (pixels)\n    batch_size=1,  # batch size\n    data=ROOT / \"data/coco128.yaml\",  # dataset.yaml path\n    device=\"\",  # cuda device, i.e. 0 or 0,1,2,3 or cpu\n    half=False,  # use FP16 half-precision inference\n    test=False,  # test exports only\n    pt_only=False,  # test PyTorch only\n    hard_fail=False,  # throw error on benchmark failure\n):\n    \"\"\"\n    Run YOLOv5 benchmarks on multiple export formats and log results for model performance evaluation.\n\n    Args:\n        weights (Path | str): Path to the model weights file (default: ROOT / \"yolov5s.pt\").\n        imgsz (int): Inference size in pixels (default: 640).\n        batch_size (int): Batch size for inference (default: 1).\n        data (Path | str): Path to the dataset.yaml file (default: ROOT / \"data/coco128.yaml\").\n        device (str): CUDA device, e.g., '0' or '0,1,2,3' or 'cpu' (default: \"\").\n        half (bool): Use FP16 half-precision inference (default: False).\n        test (bool): Test export formats only (default: False).\n        pt_only (bool): Test PyTorch format only (default: False).\n        hard_fail (bool): Throw an error on benchmark failure if True (default: False).\n\n    Returns:\n        None. Logs information about the benchmark results, including the format, size, mAP50-95, and inference time.\n\n    Notes:\n        Supported export formats and models include PyTorch, TorchScript, ONNX, OpenVINO, TensorRT, CoreML,\n            TensorFlow SavedModel, TensorFlow GraphDef, TensorFlow Lite, and TensorFlow Edge TPU. Edge TPU and TF.js\n            are unsupported.\n\n    Example:\n        ```python\n        $ python benchmarks.py --weights yolov5s.pt --img 640\n        ```\n\n    Usage:\n        Install required packages:\n          $ pip install -r requirements.txt coremltools onnx onnx-simplifier onnxruntime openvino-dev tensorflow-cpu  # CPU support\n          $ pip install -r requirements.txt coremltools onnx onnx-simplifier onnxruntime-gpu openvino-dev tensorflow   # GPU support\n          $ pip install -U nvidia-tensorrt --index-url https://pypi.ngc.nvidia.com  # TensorRT\n\n        Run benchmarks:\n          $ python benchmarks.py --weights yolov5s.pt --img 640\n    \"\"\"\n    y, t = [], time.time()\n    device = select_device(device)\n    model_type = type(attempt_load(weights, fuse=False))  # DetectionModel, SegmentationModel, etc.\n    for i, (name, f, suffix, cpu, gpu) in export.export_formats().iterrows():  # index, (name, file, suffix, CPU, GPU)\n        try:\n            assert i not in (9, 10), \"inference not supported\"  # Edge TPU and TF.js are unsupported\n            assert i != 5 or platform.system() == \"Darwin\", \"inference only supported on macOS>=10.13\"  # CoreML\n            if \"cpu\" in device.type:\n                assert cpu, \"inference not supported on CPU\"\n            if \"cuda\" in device.type:\n                assert gpu, \"inference not supported on GPU\"\n\n            # Export\n            if f == \"-\":\n                w = weights  # PyTorch format\n            else:\n                w = export.run(\n                    weights=weights, imgsz=[imgsz], include=[f], batch_size=batch_size, device=device, half=half\n                )[-1]  # all others\n            assert suffix in str(w), \"export failed\"\n\n            # Validate\n            if model_type == SegmentationModel:\n                result = val_seg(data, w, batch_size, imgsz, plots=False, device=device, task=\"speed\", half=half)\n                metric = result[0][7]  # (box(p, r, map50, map), mask(p, r, map50, map), *loss(box, obj, cls))\n            else:  # DetectionModel:\n                result = val_det(data, w, batch_size, imgsz, plots=False, device=device, task=\"speed\", half=half)\n                metric = result[0][3]  # (p, r, map50, map, *loss(box, obj, cls))\n            speed = result[2][1]  # times (preprocess, inference, postprocess)\n            y.append([name, round(file_size(w), 1), round(metric, 4), round(speed, 2)])  # MB, mAP, t_inference\n        except Exception as e:\n            if hard_fail:\n                assert type(e) is AssertionError, f\"Benchmark --hard-fail for {name}: {e}\"\n            LOGGER.warning(f\"WARNING ⚠️ Benchmark failure for {name}: {e}\")\n            y.append([name, None, None, None])  # mAP, t_inference\n        if pt_only and i == 0:\n            break  # break after PyTorch\n\n    # Print results\n    LOGGER.info(\"\\n\")\n    parse_opt()\n    notebook_init()  # print system info\n    c = [\"Format\", \"Size (MB)\", \"mAP50-95\", \"Inference time (ms)\"] if map else [\"Format\", \"Export\", \"\", \"\"]\n    py = pd.DataFrame(y, columns=c)\n    LOGGER.info(f\"\\nBenchmarks complete ({time.time() - t:.2f}s)\")\n    LOGGER.info(str(py if map else py.iloc[:, :2]))\n    if hard_fail and isinstance(hard_fail, str):\n        metrics = py[\"mAP50-95\"].array  # values to compare to floor\n        floor = eval(hard_fail)  # minimum metric floor to pass, i.e. = 0.29 mAP for YOLOv5n\n        assert all(x > floor for x in metrics if pd.notna(x)), f\"HARD FAIL: mAP50-95 < floor {floor}\"\n    return py\n\n\ndef test(\n    weights=ROOT / \"yolov5s.pt\",  # weights path\n    imgsz=640,  # inference size (pixels)\n    batch_size=1,  # batch size\n    data=ROOT / \"data/coco128.yaml\",  # dataset.yaml path\n    device=\"\",  # cuda device, i.e. 0 or 0,1,2,3 or cpu\n    half=False,  # use FP16 half-precision inference\n    test=False,  # test exports only\n    pt_only=False,  # test PyTorch only\n    hard_fail=False,  # throw error on benchmark failure\n):\n    \"\"\"\n    Run YOLOv5 export tests for all supported formats and log the results, including export statuses.\n\n    Args:\n        weights (Path | str): Path to the model weights file (.pt format). Default is 'ROOT / \"yolov5s.pt\"'.\n        imgsz (int): Inference image size (in pixels). Default is 640.\n        batch_size (int): Batch size for testing. Default is 1.\n        data (Path | str): Path to the dataset configuration file (.yaml format). Default is 'ROOT / \"data/coco128.yaml\"'.\n        device (str): Device for running the tests, can be 'cpu' or a specific CUDA device ('0', '0,1,2,3', etc.). Default is an empty string.\n        half (bool): Use FP16 half-precision for inference if True. Default is False.\n        test (bool): Test export formats only without running inference. Default is False.\n        pt_only (bool): Test only the PyTorch model if True. Default is False.\n        hard_fail (bool): Raise error on export or test failure if True. Default is False.\n\n    Returns:\n        pd.DataFrame: DataFrame containing the results of the export tests, including format names and export statuses.\n\n    Examples:\n        ```python\n        $ python benchmarks.py --weights yolov5s.pt --img 640\n        ```\n\n    Notes:\n        Supported export formats and models include PyTorch, TorchScript, ONNX, OpenVINO, TensorRT, CoreML, TensorFlow\n        SavedModel, TensorFlow GraphDef, TensorFlow Lite, and TensorFlow Edge TPU. Edge TPU and TF.js are unsupported.\n\n    Usage:\n        Install required packages:\n            $ pip install -r requirements.txt coremltools onnx onnx-simplifier onnxruntime openvino-dev tensorflow-cpu  # CPU support\n            $ pip install -r requirements.txt coremltools onnx onnx-simplifier onnxruntime-gpu openvino-dev tensorflow   # GPU support\n            $ pip install -U nvidia-tensorrt --index-url https://pypi.ngc.nvidia.com  # TensorRT\n        Run export tests:\n            $ python benchmarks.py --weights yolov5s.pt --img 640\n    \"\"\"\n    y, t = [], time.time()\n    device = select_device(device)\n    for i, (name, f, suffix, gpu) in export.export_formats().iterrows():  # index, (name, file, suffix, gpu-capable)\n        try:\n            w = (\n                weights\n                if f == \"-\"\n                else export.run(weights=weights, imgsz=[imgsz], include=[f], device=device, half=half)[-1]\n            )  # weights\n            assert suffix in str(w), \"export failed\"\n            y.append([name, True])\n        except Exception:\n            y.append([name, False])  # mAP, t_inference\n\n    # Print results\n    LOGGER.info(\"\\n\")\n    parse_opt()\n    notebook_init()  # print system info\n    py = pd.DataFrame(y, columns=[\"Format\", \"Export\"])\n    LOGGER.info(f\"\\nExports complete ({time.time() - t:.2f}s)\")\n    LOGGER.info(str(py))\n    return py\n\n\ndef parse_opt():\n    \"\"\"\n    Parses command-line arguments for YOLOv5 model inference configuration.\n\n    Args:\n        weights (str): The path to the weights file. Defaults to 'ROOT / \"yolov5s.pt\"'.\n        imgsz (int): Inference size in pixels. Defaults to 640.\n        batch_size (int): Batch size. Defaults to 1.\n        data (str): Path to the dataset YAML file. Defaults to 'ROOT / \"data/coco128.yaml\"'.\n        device (str): CUDA device, e.g., '0' or '0,1,2,3' or 'cpu'. Defaults to an empty string (auto-select).\n        half (bool): Use FP16 half-precision inference. This is a flag and defaults to False.\n        test (bool): Test exports only. This is a flag and defaults to False.\n        pt_only (bool): Test PyTorch only. This is a flag and defaults to False.\n        hard_fail (bool | str): Throw an error on benchmark failure. Can be a boolean or a string representing a minimum\n            metric floor, e.g., '0.29'. Defaults to False.\n\n    Returns:\n        argparse.Namespace: Parsed command-line arguments encapsulated in an argparse Namespace object.\n\n    Notes:\n        The function modifies the 'opt.data' by checking and validating the YAML path using 'check_yaml()'.\n        The parsed arguments are printed for reference using 'print_args()'.\n    \"\"\"\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--weights\", type=str, default=ROOT / \"yolov5s.pt\", help=\"weights path\")\n    parser.add_argument(\"--imgsz\", \"--img\", \"--img-size\", type=int, default=640, help=\"inference size (pixels)\")\n    parser.add_argument(\"--batch-size\", type=int, default=1, help=\"batch size\")\n    parser.add_argument(\"--data\", type=str, default=ROOT / \"data/coco128.yaml\", help=\"dataset.yaml path\")\n    parser.add_argument(\"--device\", default=\"\", help=\"cuda device, i.e. 0 or 0,1,2,3 or cpu\")\n    parser.add_argument(\"--half\", action=\"store_true\", help=\"use FP16 half-precision inference\")\n    parser.add_argument(\"--test\", action=\"store_true\", help=\"test exports only\")\n    parser.add_argument(\"--pt-only\", action=\"store_true\", help=\"test PyTorch only\")\n    parser.add_argument(\"--hard-fail\", nargs=\"?\", const=True, default=False, help=\"Exception on error or < min metric\")\n    opt = parser.parse_args()\n    opt.data = check_yaml(opt.data)  # check YAML\n    print_args(vars(opt))\n    return opt\n\n\ndef main(opt):\n    \"\"\"\n    Executes YOLOv5 benchmark tests or main training/inference routines based on the provided command-line arguments.\n\n    Args:\n        opt (argparse.Namespace): Parsed command-line arguments including options for weights, image size, batch size, data\n            configuration, device, and other flags for inference settings.\n\n    Returns:\n        None: This function does not return any value. It leverages side-effects such as logging and running benchmarks.\n\n    Example:\n        ```python\n        if __name__ == \"__main__\":\n            opt = parse_opt()\n            main(opt)\n        ```\n\n    Notes:\n        - For a complete list of supported export formats and their respective requirements, refer to the\n          [Ultralytics YOLOv5 Export Formats](https://github.com/ultralytics/yolov5#export-formats).\n        - Ensure that you have installed all necessary dependencies by following the installation instructions detailed in\n          the [main repository](https://github.com/ultralytics/yolov5#installation).\n\n        ```shell\n        # Running benchmarks on default weights and image size\n        $ python benchmarks.py --weights yolov5s.pt --img 640\n        ```\n    \"\"\"\n    test(**vars(opt)) if opt.test else run(**vars(opt))\n\n\nif __name__ == \"__main__\":\n    opt = parse_opt()\n    main(opt)\n",
    "classify/predict.py": "# Ultralytics 🚀 AGPL-3.0 License - https://ultralytics.com/license\n\"\"\"\nRun YOLOv5 classification inference on images, videos, directories, globs, YouTube, webcam, streams, etc.\n\nUsage - sources:\n    $ python classify/predict.py --weights yolov5s-cls.pt --source 0                               # webcam\n                                                                   img.jpg                         # image\n                                                                   vid.mp4                         # video\n                                                                   screen                          # screenshot\n                                                                   path/                           # directory\n                                                                   list.txt                        # list of images\n                                                                   list.streams                    # list of streams\n                                                                   'path/*.jpg'                    # glob\n                                                                   'https://youtu.be/LNwODJXcvt4'  # YouTube\n                                                                   'rtsp://example.com/media.mp4'  # RTSP, RTMP, HTTP stream\n\nUsage - formats:\n    $ python classify/predict.py --weights yolov5s-cls.pt                 # PyTorch\n                                           yolov5s-cls.torchscript        # TorchScript\n                                           yolov5s-cls.onnx               # ONNX Runtime or OpenCV DNN with --dnn\n                                           yolov5s-cls_openvino_model     # OpenVINO\n                                           yolov5s-cls.engine             # TensorRT\n                                           yolov5s-cls.mlmodel            # CoreML (macOS-only)\n                                           yolov5s-cls_saved_model        # TensorFlow SavedModel\n                                           yolov5s-cls.pb                 # TensorFlow GraphDef\n                                           yolov5s-cls.tflite             # TensorFlow Lite\n                                           yolov5s-cls_edgetpu.tflite     # TensorFlow Edge TPU\n                                           yolov5s-cls_paddle_model       # PaddlePaddle\n\"\"\"\n\nimport argparse\nimport os\nimport platform\nimport sys\nfrom pathlib import Path\n\nimport torch\nimport torch.nn.functional as F\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[1]  # YOLOv5 root directory\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative\n\nfrom ultralytics.utils.plotting import Annotator\n\nfrom models.common import DetectMultiBackend\nfrom utils.augmentations import classify_transforms\nfrom utils.dataloaders import IMG_FORMATS, VID_FORMATS, LoadImages, LoadScreenshots, LoadStreams\nfrom utils.general import (\n    LOGGER,\n    Profile,\n    check_file,\n    check_img_size,\n    check_imshow,\n    check_requirements,\n    colorstr,\n    cv2,\n    increment_path,\n    print_args,\n    strip_optimizer,\n)\nfrom utils.torch_utils import select_device, smart_inference_mode\n\n\n@smart_inference_mode()\ndef run(\n    weights=ROOT / \"yolov5s-cls.pt\",  # model.pt path(s)\n    source=ROOT / \"data/images\",  # file/dir/URL/glob/screen/0(webcam)\n    data=ROOT / \"data/coco128.yaml\",  # dataset.yaml path\n    imgsz=(224, 224),  # inference size (height, width)\n    device=\"\",  # cuda device, i.e. 0 or 0,1,2,3 or cpu\n    view_img=False,  # show results\n    save_txt=False,  # save results to *.txt\n    nosave=False,  # do not save images/videos\n    augment=False,  # augmented inference\n    visualize=False,  # visualize features\n    update=False,  # update all models\n    project=ROOT / \"runs/predict-cls\",  # save results to project/name\n    name=\"exp\",  # save results to project/name\n    exist_ok=False,  # existing project/name ok, do not increment\n    half=False,  # use FP16 half-precision inference\n    dnn=False,  # use OpenCV DNN for ONNX inference\n    vid_stride=1,  # video frame-rate stride\n):\n    \"\"\"Conducts YOLOv5 classification inference on diverse input sources and saves results.\"\"\"\n    source = str(source)\n    save_img = not nosave and not source.endswith(\".txt\")  # save inference images\n    is_file = Path(source).suffix[1:] in (IMG_FORMATS + VID_FORMATS)\n    is_url = source.lower().startswith((\"rtsp://\", \"rtmp://\", \"http://\", \"https://\"))\n    webcam = source.isnumeric() or source.endswith(\".streams\") or (is_url and not is_file)\n    screenshot = source.lower().startswith(\"screen\")\n    if is_url and is_file:\n        source = check_file(source)  # download\n\n    # Directories\n    save_dir = increment_path(Path(project) / name, exist_ok=exist_ok)  # increment run\n    (save_dir / \"labels\" if save_txt else save_dir).mkdir(parents=True, exist_ok=True)  # make dir\n\n    # Load model\n    device = select_device(device)\n    model = DetectMultiBackend(weights, device=device, dnn=dnn, data=data, fp16=half)\n    stride, names, pt = model.stride, model.names, model.pt\n    imgsz = check_img_size(imgsz, s=stride)  # check image size\n\n    # Dataloader\n    bs = 1  # batch_size\n    if webcam:\n        view_img = check_imshow(warn=True)\n        dataset = LoadStreams(source, img_size=imgsz, transforms=classify_transforms(imgsz[0]), vid_stride=vid_stride)\n        bs = len(dataset)\n    elif screenshot:\n        dataset = LoadScreenshots(source, img_size=imgsz, stride=stride, auto=pt)\n    else:\n        dataset = LoadImages(source, img_size=imgsz, transforms=classify_transforms(imgsz[0]), vid_stride=vid_stride)\n    vid_path, vid_writer = [None] * bs, [None] * bs\n\n    # Run inference\n    model.warmup(imgsz=(1 if pt else bs, 3, *imgsz))  # warmup\n    seen, windows, dt = 0, [], (Profile(device=device), Profile(device=device), Profile(device=device))\n    for path, im, im0s, vid_cap, s in dataset:\n        with dt[0]:\n            im = torch.Tensor(im).to(model.device)\n            im = im.half() if model.fp16 else im.float()  # uint8 to fp16/32\n            if len(im.shape) == 3:\n                im = im[None]  # expand for batch dim\n\n        # Inference\n        with dt[1]:\n            results = model(im)\n\n        # Post-process\n        with dt[2]:\n            pred = F.softmax(results, dim=1)  # probabilities\n\n        # Process predictions\n        for i, prob in enumerate(pred):  # per image\n            seen += 1\n            if webcam:  # batch_size >= 1\n                p, im0, frame = path[i], im0s[i].copy(), dataset.count\n                s += f\"{i}: \"\n            else:\n                p, im0, frame = path, im0s.copy(), getattr(dataset, \"frame\", 0)\n\n            p = Path(p)  # to Path\n            save_path = str(save_dir / p.name)  # im.jpg\n            txt_path = str(save_dir / \"labels\" / p.stem) + (\"\" if dataset.mode == \"image\" else f\"_{frame}\")  # im.txt\n\n            s += \"{:g}x{:g} \".format(*im.shape[2:])  # print string\n            annotator = Annotator(im0, example=str(names), pil=True)\n\n            # Print results\n            top5i = prob.argsort(0, descending=True)[:5].tolist()  # top 5 indices\n            s += f\"{', '.join(f'{names[j]} {prob[j]:.2f}' for j in top5i)}, \"\n\n            # Write results\n            text = \"\\n\".join(f\"{prob[j]:.2f} {names[j]}\" for j in top5i)\n            if save_img or view_img:  # Add bbox to image\n                annotator.text([32, 32], text, txt_color=(255, 255, 255))\n            if save_txt:  # Write to file\n                with open(f\"{txt_path}.txt\", \"a\") as f:\n                    f.write(text + \"\\n\")\n\n            # Stream results\n            im0 = annotator.result()\n            if view_img:\n                if platform.system() == \"Linux\" and p not in windows:\n                    windows.append(p)\n                    cv2.namedWindow(str(p), cv2.WINDOW_NORMAL | cv2.WINDOW_KEEPRATIO)  # allow window resize (Linux)\n                    cv2.resizeWindow(str(p), im0.shape[1], im0.shape[0])\n                cv2.imshow(str(p), im0)\n                cv2.waitKey(1)  # 1 millisecond\n\n            # Save results (image with detections)\n            if save_img:\n                if dataset.mode == \"image\":\n                    cv2.imwrite(save_path, im0)\n                else:  # 'video' or 'stream'\n                    if vid_path[i] != save_path:  # new video\n                        vid_path[i] = save_path\n                        if isinstance(vid_writer[i], cv2.VideoWriter):\n                            vid_writer[i].release()  # release previous video writer\n                        if vid_cap:  # video\n                            fps = vid_cap.get(cv2.CAP_PROP_FPS)\n                            w = int(vid_cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n                            h = int(vid_cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n                        else:  # stream\n                            fps, w, h = 30, im0.shape[1], im0.shape[0]\n                        save_path = str(Path(save_path).with_suffix(\".mp4\"))  # force *.mp4 suffix on results videos\n                        vid_writer[i] = cv2.VideoWriter(save_path, cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (w, h))\n                    vid_writer[i].write(im0)\n\n        # Print time (inference-only)\n        LOGGER.info(f\"{s}{dt[1].dt * 1e3:.1f}ms\")\n\n    # Print results\n    t = tuple(x.t / seen * 1e3 for x in dt)  # speeds per image\n    LOGGER.info(f\"Speed: %.1fms pre-process, %.1fms inference, %.1fms NMS per image at shape {(1, 3, *imgsz)}\" % t)\n    if save_txt or save_img:\n        s = f\"\\n{len(list(save_dir.glob('labels/*.txt')))} labels saved to {save_dir / 'labels'}\" if save_txt else \"\"\n        LOGGER.info(f\"Results saved to {colorstr('bold', save_dir)}{s}\")\n    if update:\n        strip_optimizer(weights[0])  # update model (to fix SourceChangeWarning)\n\n\ndef parse_opt():\n    \"\"\"Parses command line arguments for YOLOv5 inference settings including model, source, device, and image size.\"\"\"\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--weights\", nargs=\"+\", type=str, default=ROOT / \"yolov5s-cls.pt\", help=\"model path(s)\")\n    parser.add_argument(\"--source\", type=str, default=ROOT / \"data/images\", help=\"file/dir/URL/glob/screen/0(webcam)\")\n    parser.add_argument(\"--data\", type=str, default=ROOT / \"data/coco128.yaml\", help=\"(optional) dataset.yaml path\")\n    parser.add_argument(\"--imgsz\", \"--img\", \"--img-size\", nargs=\"+\", type=int, default=[224], help=\"inference size h,w\")\n    parser.add_argument(\"--device\", default=\"\", help=\"cuda device, i.e. 0 or 0,1,2,3 or cpu\")\n    parser.add_argument(\"--view-img\", action=\"store_true\", help=\"show results\")\n    parser.add_argument(\"--save-txt\", action=\"store_true\", help=\"save results to *.txt\")\n    parser.add_argument(\"--nosave\", action=\"store_true\", help=\"do not save images/videos\")\n    parser.add_argument(\"--augment\", action=\"store_true\", help=\"augmented inference\")\n    parser.add_argument(\"--visualize\", action=\"store_true\", help=\"visualize features\")\n    parser.add_argument(\"--update\", action=\"store_true\", help=\"update all models\")\n    parser.add_argument(\"--project\", default=ROOT / \"runs/predict-cls\", help=\"save results to project/name\")\n    parser.add_argument(\"--name\", default=\"exp\", help=\"save results to project/name\")\n    parser.add_argument(\"--exist-ok\", action=\"store_true\", help=\"existing project/name ok, do not increment\")\n    parser.add_argument(\"--half\", action=\"store_true\", help=\"use FP16 half-precision inference\")\n    parser.add_argument(\"--dnn\", action=\"store_true\", help=\"use OpenCV DNN for ONNX inference\")\n    parser.add_argument(\"--vid-stride\", type=int, default=1, help=\"video frame-rate stride\")\n    opt = parser.parse_args()\n    opt.imgsz *= 2 if len(opt.imgsz) == 1 else 1  # expand\n    print_args(vars(opt))\n    return opt\n\n\ndef main(opt):\n    \"\"\"Executes YOLOv5 model inference with options for ONNX DNN and video frame-rate stride adjustments.\"\"\"\n    check_requirements(ROOT / \"requirements.txt\", exclude=(\"tensorboard\", \"thop\"))\n    run(**vars(opt))\n\n\nif __name__ == \"__main__\":\n    opt = parse_opt()\n    main(opt)\n",
    "classify/train.py": "# Ultralytics 🚀 AGPL-3.0 License - https://ultralytics.com/license\n\"\"\"\nTrain a YOLOv5 classifier model on a classification dataset.\n\nUsage - Single-GPU training:\n    $ python classify/train.py --model yolov5s-cls.pt --data imagenette160 --epochs 5 --img 224\n\nUsage - Multi-GPU DDP training:\n    $ python -m torch.distributed.run --nproc_per_node 4 --master_port 2022 classify/train.py --model yolov5s-cls.pt --data imagenet --epochs 5 --img 224 --device 0,1,2,3\n\nDatasets:           --data mnist, fashion-mnist, cifar10, cifar100, imagenette, imagewoof, imagenet, or 'path/to/data'\nYOLOv5-cls models:  --model yolov5n-cls.pt, yolov5s-cls.pt, yolov5m-cls.pt, yolov5l-cls.pt, yolov5x-cls.pt\nTorchvision models: --model resnet50, efficientnet_b0, etc. See https://pytorch.org/vision/stable/models.html\n\"\"\"\n\nimport argparse\nimport os\nimport subprocess\nimport sys\nimport time\nfrom copy import deepcopy\nfrom datetime import datetime\nfrom pathlib import Path\n\nimport torch\nimport torch.distributed as dist\nimport torch.hub as hub\nimport torch.optim.lr_scheduler as lr_scheduler\nimport torchvision\nfrom torch.cuda import amp\nfrom tqdm import tqdm\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[1]  # YOLOv5 root directory\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative\n\nfrom classify import val as validate\nfrom models.experimental import attempt_load\nfrom models.yolo import ClassificationModel, DetectionModel\nfrom utils.dataloaders import create_classification_dataloader\nfrom utils.general import (\n    DATASETS_DIR,\n    LOGGER,\n    TQDM_BAR_FORMAT,\n    WorkingDirectory,\n    check_git_info,\n    check_git_status,\n    check_requirements,\n    colorstr,\n    download,\n    increment_path,\n    init_seeds,\n    print_args,\n    yaml_save,\n)\nfrom utils.loggers import GenericLogger\nfrom utils.plots import imshow_cls\nfrom utils.torch_utils import (\n    ModelEMA,\n    de_parallel,\n    model_info,\n    reshape_classifier_output,\n    select_device,\n    smart_DDP,\n    smart_optimizer,\n    smartCrossEntropyLoss,\n    torch_distributed_zero_first,\n)\n\nLOCAL_RANK = int(os.getenv(\"LOCAL_RANK\", -1))  # https://pytorch.org/docs/stable/elastic/run.html\nRANK = int(os.getenv(\"RANK\", -1))\nWORLD_SIZE = int(os.getenv(\"WORLD_SIZE\", 1))\nGIT_INFO = check_git_info()\n\n\ndef train(opt, device):\n    \"\"\"Trains a YOLOv5 model, managing datasets, model optimization, logging, and saving checkpoints.\"\"\"\n    init_seeds(opt.seed + 1 + RANK, deterministic=True)\n    save_dir, data, bs, epochs, nw, imgsz, pretrained = (\n        opt.save_dir,\n        Path(opt.data),\n        opt.batch_size,\n        opt.epochs,\n        min(os.cpu_count() - 1, opt.workers),\n        opt.imgsz,\n        str(opt.pretrained).lower() == \"true\",\n    )\n    cuda = device.type != \"cpu\"\n\n    # Directories\n    wdir = save_dir / \"weights\"\n    wdir.mkdir(parents=True, exist_ok=True)  # make dir\n    last, best = wdir / \"last.pt\", wdir / \"best.pt\"\n\n    # Save run settings\n    yaml_save(save_dir / \"opt.yaml\", vars(opt))\n\n    # Logger\n    logger = GenericLogger(opt=opt, console_logger=LOGGER) if RANK in {-1, 0} else None\n\n    # Download Dataset\n    with torch_distributed_zero_first(LOCAL_RANK), WorkingDirectory(ROOT):\n        data_dir = data if data.is_dir() else (DATASETS_DIR / data)\n        if not data_dir.is_dir():\n            LOGGER.info(f\"\\nDataset not found ⚠️, missing path {data_dir}, attempting download...\")\n            t = time.time()\n            if str(data) == \"imagenet\":\n                subprocess.run([\"bash\", str(ROOT / \"data/scripts/get_imagenet.sh\")], shell=True, check=True)\n            else:\n                url = f\"https://github.com/ultralytics/assets/releases/download/v0.0.0/{data}.zip\"\n                download(url, dir=data_dir.parent)\n            s = f\"Dataset download success ✅ ({time.time() - t:.1f}s), saved to {colorstr('bold', data_dir)}\\n\"\n            LOGGER.info(s)\n\n    # Dataloaders\n    nc = len([x for x in (data_dir / \"train\").glob(\"*\") if x.is_dir()])  # number of classes\n    trainloader = create_classification_dataloader(\n        path=data_dir / \"train\",\n        imgsz=imgsz,\n        batch_size=bs // WORLD_SIZE,\n        augment=True,\n        cache=opt.cache,\n        rank=LOCAL_RANK,\n        workers=nw,\n    )\n\n    test_dir = data_dir / \"test\" if (data_dir / \"test\").exists() else data_dir / \"val\"  # data/test or data/val\n    if RANK in {-1, 0}:\n        testloader = create_classification_dataloader(\n            path=test_dir,\n            imgsz=imgsz,\n            batch_size=bs // WORLD_SIZE * 2,\n            augment=False,\n            cache=opt.cache,\n            rank=-1,\n            workers=nw,\n        )\n\n    # Model\n    with torch_distributed_zero_first(LOCAL_RANK), WorkingDirectory(ROOT):\n        if Path(opt.model).is_file() or opt.model.endswith(\".pt\"):\n            model = attempt_load(opt.model, device=\"cpu\", fuse=False)\n        elif opt.model in torchvision.models.__dict__:  # TorchVision models i.e. resnet50, efficientnet_b0\n            model = torchvision.models.__dict__[opt.model](weights=\"IMAGENET1K_V1\" if pretrained else None)\n        else:\n            m = hub.list(\"ultralytics/yolov5\")  # + hub.list('pytorch/vision')  # models\n            raise ModuleNotFoundError(f\"--model {opt.model} not found. Available models are: \\n\" + \"\\n\".join(m))\n        if isinstance(model, DetectionModel):\n            LOGGER.warning(\"WARNING ⚠️ pass YOLOv5 classifier model with '-cls' suffix, i.e. '--model yolov5s-cls.pt'\")\n            model = ClassificationModel(model=model, nc=nc, cutoff=opt.cutoff or 10)  # convert to classification model\n        reshape_classifier_output(model, nc)  # update class count\n    for m in model.modules():\n        if not pretrained and hasattr(m, \"reset_parameters\"):\n            m.reset_parameters()\n        if isinstance(m, torch.nn.Dropout) and opt.dropout is not None:\n            m.p = opt.dropout  # set dropout\n    for p in model.parameters():\n        p.requires_grad = True  # for training\n    model = model.to(device)\n\n    # Info\n    if RANK in {-1, 0}:\n        model.names = trainloader.dataset.classes  # attach class names\n        model.transforms = testloader.dataset.torch_transforms  # attach inference transforms\n        model_info(model)\n        if opt.verbose:\n            LOGGER.info(model)\n        images, labels = next(iter(trainloader))\n        file = imshow_cls(images[:25], labels[:25], names=model.names, f=save_dir / \"train_images.jpg\")\n        logger.log_images(file, name=\"Train Examples\")\n        logger.log_graph(model, imgsz)  # log model\n\n    # Optimizer\n    optimizer = smart_optimizer(model, opt.optimizer, opt.lr0, momentum=0.9, decay=opt.decay)\n\n    # Scheduler\n    lrf = 0.01  # final lr (fraction of lr0)\n\n    # lf = lambda x: ((1 + math.cos(x * math.pi / epochs)) / 2) * (1 - lrf) + lrf  # cosine\n    def lf(x):\n        \"\"\"Linear learning rate scheduler function, scaling learning rate from initial value to `lrf` over `epochs`.\"\"\"\n        return (1 - x / epochs) * (1 - lrf) + lrf  # linear\n\n    scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lf)\n    # scheduler = lr_scheduler.OneCycleLR(optimizer, max_lr=lr0, total_steps=epochs, pct_start=0.1,\n    #                                    final_div_factor=1 / 25 / lrf)\n\n    # EMA\n    ema = ModelEMA(model) if RANK in {-1, 0} else None\n\n    # DDP mode\n    if cuda and RANK != -1:\n        model = smart_DDP(model)\n\n    # Train\n    t0 = time.time()\n    criterion = smartCrossEntropyLoss(label_smoothing=opt.label_smoothing)  # loss function\n    best_fitness = 0.0\n    scaler = amp.GradScaler(enabled=cuda)\n    val = test_dir.stem  # 'val' or 'test'\n    LOGGER.info(\n        f\"Image sizes {imgsz} train, {imgsz} test\\n\"\n        f\"Using {nw * WORLD_SIZE} dataloader workers\\n\"\n        f\"Logging results to {colorstr('bold', save_dir)}\\n\"\n        f\"Starting {opt.model} training on {data} dataset with {nc} classes for {epochs} epochs...\\n\\n\"\n        f\"{'Epoch':>10}{'GPU_mem':>10}{'train_loss':>12}{f'{val}_loss':>12}{'top1_acc':>12}{'top5_acc':>12}\"\n    )\n    for epoch in range(epochs):  # loop over the dataset multiple times\n        tloss, vloss, fitness = 0.0, 0.0, 0.0  # train loss, val loss, fitness\n        model.train()\n        if RANK != -1:\n            trainloader.sampler.set_epoch(epoch)\n        pbar = enumerate(trainloader)\n        if RANK in {-1, 0}:\n            pbar = tqdm(enumerate(trainloader), total=len(trainloader), bar_format=TQDM_BAR_FORMAT)\n        for i, (images, labels) in pbar:  # progress bar\n            images, labels = images.to(device, non_blocking=True), labels.to(device)\n\n            # Forward\n            with amp.autocast(enabled=cuda):  # stability issues when enabled\n                loss = criterion(model(images), labels)\n\n            # Backward\n            scaler.scale(loss).backward()\n\n            # Optimize\n            scaler.unscale_(optimizer)  # unscale gradients\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients\n            scaler.step(optimizer)\n            scaler.update()\n            optimizer.zero_grad()\n            if ema:\n                ema.update(model)\n\n            if RANK in {-1, 0}:\n                # Print\n                tloss = (tloss * i + loss.item()) / (i + 1)  # update mean losses\n                mem = \"%.3gG\" % (torch.cuda.memory_reserved() / 1e9 if torch.cuda.is_available() else 0)  # (GB)\n                pbar.desc = f\"{f'{epoch + 1}/{epochs}':>10}{mem:>10}{tloss:>12.3g}\" + \" \" * 36\n\n                # Test\n                if i == len(pbar) - 1:  # last batch\n                    top1, top5, vloss = validate.run(\n                        model=ema.ema, dataloader=testloader, criterion=criterion, pbar=pbar\n                    )  # test accuracy, loss\n                    fitness = top1  # define fitness as top1 accuracy\n\n        # Scheduler\n        scheduler.step()\n\n        # Log metrics\n        if RANK in {-1, 0}:\n            # Best fitness\n            if fitness > best_fitness:\n                best_fitness = fitness\n\n            # Log\n            metrics = {\n                \"train/loss\": tloss,\n                f\"{val}/loss\": vloss,\n                \"metrics/accuracy_top1\": top1,\n                \"metrics/accuracy_top5\": top5,\n                \"lr/0\": optimizer.param_groups[0][\"lr\"],\n            }  # learning rate\n            logger.log_metrics(metrics, epoch)\n\n            # Save model\n            final_epoch = epoch + 1 == epochs\n            if (not opt.nosave) or final_epoch:\n                ckpt = {\n                    \"epoch\": epoch,\n                    \"best_fitness\": best_fitness,\n                    \"model\": deepcopy(ema.ema).half(),  # deepcopy(de_parallel(model)).half(),\n                    \"ema\": None,  # deepcopy(ema.ema).half(),\n                    \"updates\": ema.updates,\n                    \"optimizer\": None,  # optimizer.state_dict(),\n                    \"opt\": vars(opt),\n                    \"git\": GIT_INFO,  # {remote, branch, commit} if a git repo\n                    \"date\": datetime.now().isoformat(),\n                }\n\n                # Save last, best and delete\n                torch.save(ckpt, last)\n                if best_fitness == fitness:\n                    torch.save(ckpt, best)\n                del ckpt\n\n    # Train complete\n    if RANK in {-1, 0} and final_epoch:\n        LOGGER.info(\n            f\"\\nTraining complete ({(time.time() - t0) / 3600:.3f} hours)\"\n            f\"\\nResults saved to {colorstr('bold', save_dir)}\"\n            f\"\\nPredict:         python classify/predict.py --weights {best} --source im.jpg\"\n            f\"\\nValidate:        python classify/val.py --weights {best} --data {data_dir}\"\n            f\"\\nExport:          python export.py --weights {best} --include onnx\"\n            f\"\\nPyTorch Hub:     model = torch.hub.load('ultralytics/yolov5', 'custom', '{best}')\"\n            f\"\\nVisualize:       https://netron.app\\n\"\n        )\n\n        # Plot examples\n        images, labels = (x[:25] for x in next(iter(testloader)))  # first 25 images and labels\n        pred = torch.max(ema.ema(images.to(device)), 1)[1]\n        file = imshow_cls(images, labels, pred, de_parallel(model).names, verbose=False, f=save_dir / \"test_images.jpg\")\n\n        # Log results\n        meta = {\"epochs\": epochs, \"top1_acc\": best_fitness, \"date\": datetime.now().isoformat()}\n        logger.log_images(file, name=\"Test Examples (true-predicted)\", epoch=epoch)\n        logger.log_model(best, epochs, metadata=meta)\n\n\ndef parse_opt(known=False):\n    \"\"\"Parses command line arguments for YOLOv5 training including model path, dataset, epochs, and more, returning\n    parsed arguments.\n    \"\"\"\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--model\", type=str, default=\"yolov5s-cls.pt\", help=\"initial weights path\")\n    parser.add_argument(\"--data\", type=str, default=\"imagenette160\", help=\"cifar10, cifar100, mnist, imagenet, ...\")\n    parser.add_argument(\"--epochs\", type=int, default=10, help=\"total training epochs\")\n    parser.add_argument(\"--batch-size\", type=int, default=64, help=\"total batch size for all GPUs\")\n    parser.add_argument(\"--imgsz\", \"--img\", \"--img-size\", type=int, default=224, help=\"train, val image size (pixels)\")\n    parser.add_argument(\"--nosave\", action=\"store_true\", help=\"only save final checkpoint\")\n    parser.add_argument(\"--cache\", type=str, nargs=\"?\", const=\"ram\", help='--cache images in \"ram\" (default) or \"disk\"')\n    parser.add_argument(\"--device\", default=\"\", help=\"cuda device, i.e. 0 or 0,1,2,3 or cpu\")\n    parser.add_argument(\"--workers\", type=int, default=8, help=\"max dataloader workers (per RANK in DDP mode)\")\n    parser.add_argument(\"--project\", default=ROOT / \"runs/train-cls\", help=\"save to project/name\")\n    parser.add_argument(\"--name\", default=\"exp\", help=\"save to project/name\")\n    parser.add_argument(\"--exist-ok\", action=\"store_true\", help=\"existing project/name ok, do not increment\")\n    parser.add_argument(\"--pretrained\", nargs=\"?\", const=True, default=True, help=\"start from i.e. --pretrained False\")\n    parser.add_argument(\"--optimizer\", choices=[\"SGD\", \"Adam\", \"AdamW\", \"RMSProp\"], default=\"Adam\", help=\"optimizer\")\n    parser.add_argument(\"--lr0\", type=float, default=0.001, help=\"initial learning rate\")\n    parser.add_argument(\"--decay\", type=float, default=5e-5, help=\"weight decay\")\n    parser.add_argument(\"--label-smoothing\", type=float, default=0.1, help=\"Label smoothing epsilon\")\n    parser.add_argument(\"--cutoff\", type=int, default=None, help=\"Model layer cutoff index for Classify() head\")\n    parser.add_argument(\"--dropout\", type=float, default=None, help=\"Dropout (fraction)\")\n    parser.add_argument(\"--verbose\", action=\"store_true\", help=\"Verbose mode\")\n    parser.add_argument(\"--seed\", type=int, default=0, help=\"Global training seed\")\n    parser.add_argument(\"--local_rank\", type=int, default=-1, help=\"Automatic DDP Multi-GPU argument, do not modify\")\n    return parser.parse_known_args()[0] if known else parser.parse_args()\n\n\ndef main(opt):\n    \"\"\"Executes YOLOv5 training with given options, handling device setup and DDP mode; includes pre-training checks.\"\"\"\n    if RANK in {-1, 0}:\n        print_args(vars(opt))\n        check_git_status()\n        check_requirements(ROOT / \"requirements.txt\")\n\n    # DDP mode\n    device = select_device(opt.device, batch_size=opt.batch_size)\n    if LOCAL_RANK != -1:\n        assert opt.batch_size != -1, \"AutoBatch is coming soon for classification, please pass a valid --batch-size\"\n        assert opt.batch_size % WORLD_SIZE == 0, f\"--batch-size {opt.batch_size} must be multiple of WORLD_SIZE\"\n        assert torch.cuda.device_count() > LOCAL_RANK, \"insufficient CUDA devices for DDP command\"\n        torch.cuda.set_device(LOCAL_RANK)\n        device = torch.device(\"cuda\", LOCAL_RANK)\n        dist.init_process_group(backend=\"nccl\" if dist.is_nccl_available() else \"gloo\")\n\n    # Parameters\n    opt.save_dir = increment_path(Path(opt.project) / opt.name, exist_ok=opt.exist_ok)  # increment run\n\n    # Train\n    train(opt, device)\n\n\ndef run(**kwargs):\n    \"\"\"\n    Executes YOLOv5 model training or inference with specified parameters, returning updated options.\n\n    Example: from yolov5 import classify; classify.train.run(data=mnist, imgsz=320, model='yolov5m')\n    \"\"\"\n    opt = parse_opt(True)\n    for k, v in kwargs.items():\n        setattr(opt, k, v)\n    main(opt)\n    return opt\n\n\nif __name__ == \"__main__\":\n    opt = parse_opt()\n    main(opt)\n",
    "classify/val.py": "# Ultralytics 🚀 AGPL-3.0 License - https://ultralytics.com/license\n\"\"\"\nValidate a trained YOLOv5 classification model on a classification dataset.\n\nUsage:\n    $ bash data/scripts/get_imagenet.sh --val  # download ImageNet val split (6.3G, 50000 images)\n    $ python classify/val.py --weights yolov5m-cls.pt --data ../datasets/imagenet --img 224  # validate ImageNet\n\nUsage - formats:\n    $ python classify/val.py --weights yolov5s-cls.pt                 # PyTorch\n                                       yolov5s-cls.torchscript        # TorchScript\n                                       yolov5s-cls.onnx               # ONNX Runtime or OpenCV DNN with --dnn\n                                       yolov5s-cls_openvino_model     # OpenVINO\n                                       yolov5s-cls.engine             # TensorRT\n                                       yolov5s-cls.mlmodel            # CoreML (macOS-only)\n                                       yolov5s-cls_saved_model        # TensorFlow SavedModel\n                                       yolov5s-cls.pb                 # TensorFlow GraphDef\n                                       yolov5s-cls.tflite             # TensorFlow Lite\n                                       yolov5s-cls_edgetpu.tflite     # TensorFlow Edge TPU\n                                       yolov5s-cls_paddle_model       # PaddlePaddle\n\"\"\"\n\nimport argparse\nimport os\nimport sys\nfrom pathlib import Path\n\nimport torch\nfrom tqdm import tqdm\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[1]  # YOLOv5 root directory\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative\n\nfrom models.common import DetectMultiBackend\nfrom utils.dataloaders import create_classification_dataloader\nfrom utils.general import (\n    LOGGER,\n    TQDM_BAR_FORMAT,\n    Profile,\n    check_img_size,\n    check_requirements,\n    colorstr,\n    increment_path,\n    print_args,\n)\nfrom utils.torch_utils import select_device, smart_inference_mode\n\n\n@smart_inference_mode()\ndef run(\n    data=ROOT / \"../datasets/mnist\",  # dataset dir\n    weights=ROOT / \"yolov5s-cls.pt\",  # model.pt path(s)\n    batch_size=128,  # batch size\n    imgsz=224,  # inference size (pixels)\n    device=\"\",  # cuda device, i.e. 0 or 0,1,2,3 or cpu\n    workers=8,  # max dataloader workers (per RANK in DDP mode)\n    verbose=False,  # verbose output\n    project=ROOT / \"runs/val-cls\",  # save to project/name\n    name=\"exp\",  # save to project/name\n    exist_ok=False,  # existing project/name ok, do not increment\n    half=False,  # use FP16 half-precision inference\n    dnn=False,  # use OpenCV DNN for ONNX inference\n    model=None,\n    dataloader=None,\n    criterion=None,\n    pbar=None,\n):\n    \"\"\"Validates a YOLOv5 classification model on a dataset, computing metrics like top1 and top5 accuracy.\"\"\"\n    # Initialize/load model and set device\n    training = model is not None\n    if training:  # called by train.py\n        device, pt, jit, engine = next(model.parameters()).device, True, False, False  # get model device, PyTorch model\n        half &= device.type != \"cpu\"  # half precision only supported on CUDA\n        model.half() if half else model.float()\n    else:  # called directly\n        device = select_device(device, batch_size=batch_size)\n\n        # Directories\n        save_dir = increment_path(Path(project) / name, exist_ok=exist_ok)  # increment run\n        save_dir.mkdir(parents=True, exist_ok=True)  # make dir\n\n        # Load model\n        model = DetectMultiBackend(weights, device=device, dnn=dnn, fp16=half)\n        stride, pt, jit, engine = model.stride, model.pt, model.jit, model.engine\n        imgsz = check_img_size(imgsz, s=stride)  # check image size\n        half = model.fp16  # FP16 supported on limited backends with CUDA\n        if engine:\n            batch_size = model.batch_size\n        else:\n            device = model.device\n            if not (pt or jit):\n                batch_size = 1  # export.py models default to batch-size 1\n                LOGGER.info(f\"Forcing --batch-size 1 square inference (1,3,{imgsz},{imgsz}) for non-PyTorch models\")\n\n        # Dataloader\n        data = Path(data)\n        test_dir = data / \"test\" if (data / \"test\").exists() else data / \"val\"  # data/test or data/val\n        dataloader = create_classification_dataloader(\n            path=test_dir, imgsz=imgsz, batch_size=batch_size, augment=False, rank=-1, workers=workers\n        )\n\n    model.eval()\n    pred, targets, loss, dt = [], [], 0, (Profile(device=device), Profile(device=device), Profile(device=device))\n    n = len(dataloader)  # number of batches\n    action = \"validating\" if dataloader.dataset.root.stem == \"val\" else \"testing\"\n    desc = f\"{pbar.desc[:-36]}{action:>36}\" if pbar else f\"{action}\"\n    bar = tqdm(dataloader, desc, n, not training, bar_format=TQDM_BAR_FORMAT, position=0)\n    with torch.cuda.amp.autocast(enabled=device.type != \"cpu\"):\n        for images, labels in bar:\n            with dt[0]:\n                images, labels = images.to(device, non_blocking=True), labels.to(device)\n\n            with dt[1]:\n                y = model(images)\n\n            with dt[2]:\n                pred.append(y.argsort(1, descending=True)[:, :5])\n                targets.append(labels)\n                if criterion:\n                    loss += criterion(y, labels)\n\n    loss /= n\n    pred, targets = torch.cat(pred), torch.cat(targets)\n    correct = (targets[:, None] == pred).float()\n    acc = torch.stack((correct[:, 0], correct.max(1).values), dim=1)  # (top1, top5) accuracy\n    top1, top5 = acc.mean(0).tolist()\n\n    if pbar:\n        pbar.desc = f\"{pbar.desc[:-36]}{loss:>12.3g}{top1:>12.3g}{top5:>12.3g}\"\n    if verbose:  # all classes\n        LOGGER.info(f\"{'Class':>24}{'Images':>12}{'top1_acc':>12}{'top5_acc':>12}\")\n        LOGGER.info(f\"{'all':>24}{targets.shape[0]:>12}{top1:>12.3g}{top5:>12.3g}\")\n        for i, c in model.names.items():\n            acc_i = acc[targets == i]\n            top1i, top5i = acc_i.mean(0).tolist()\n            LOGGER.info(f\"{c:>24}{acc_i.shape[0]:>12}{top1i:>12.3g}{top5i:>12.3g}\")\n\n        # Print results\n        t = tuple(x.t / len(dataloader.dataset.samples) * 1e3 for x in dt)  # speeds per image\n        shape = (1, 3, imgsz, imgsz)\n        LOGGER.info(f\"Speed: %.1fms pre-process, %.1fms inference, %.1fms post-process per image at shape {shape}\" % t)\n        LOGGER.info(f\"Results saved to {colorstr('bold', save_dir)}\")\n\n    return top1, top5, loss\n\n\ndef parse_opt():\n    \"\"\"Parses and returns command line arguments for YOLOv5 model evaluation and inference settings.\"\"\"\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--data\", type=str, default=ROOT / \"../datasets/mnist\", help=\"dataset path\")\n    parser.add_argument(\"--weights\", nargs=\"+\", type=str, default=ROOT / \"yolov5s-cls.pt\", help=\"model.pt path(s)\")\n    parser.add_argument(\"--batch-size\", type=int, default=128, help=\"batch size\")\n    parser.add_argument(\"--imgsz\", \"--img\", \"--img-size\", type=int, default=224, help=\"inference size (pixels)\")\n    parser.add_argument(\"--device\", default=\"\", help=\"cuda device, i.e. 0 or 0,1,2,3 or cpu\")\n    parser.add_argument(\"--workers\", type=int, default=8, help=\"max dataloader workers (per RANK in DDP mode)\")\n    parser.add_argument(\"--verbose\", nargs=\"?\", const=True, default=True, help=\"verbose output\")\n    parser.add_argument(\"--project\", default=ROOT / \"runs/val-cls\", help=\"save to project/name\")\n    parser.add_argument(\"--name\", default=\"exp\", help=\"save to project/name\")\n    parser.add_argument(\"--exist-ok\", action=\"store_true\", help=\"existing project/name ok, do not increment\")\n    parser.add_argument(\"--half\", action=\"store_true\", help=\"use FP16 half-precision inference\")\n    parser.add_argument(\"--dnn\", action=\"store_true\", help=\"use OpenCV DNN for ONNX inference\")\n    opt = parser.parse_args()\n    print_args(vars(opt))\n    return opt\n\n\ndef main(opt):\n    \"\"\"Executes the YOLOv5 model prediction workflow, handling argument parsing and requirement checks.\"\"\"\n    check_requirements(ROOT / \"requirements.txt\", exclude=(\"tensorboard\", \"thop\"))\n    run(**vars(opt))\n\n\nif __name__ == \"__main__\":\n    opt = parse_opt()\n    main(opt)\n",
    "detect.py": "# Ultralytics 🚀 AGPL-3.0 License - https://ultralytics.com/license\n\"\"\"\nRun YOLOv5 detection inference on images, videos, directories, globs, YouTube, webcam, streams, etc.\n\nUsage - sources:\n    $ python detect.py --weights yolov5s.pt --source 0                               # webcam\n                                                     img.jpg                         # image\n                                                     vid.mp4                         # video\n                                                     screen                          # screenshot\n                                                     path/                           # directory\n                                                     list.txt                        # list of images\n                                                     list.streams                    # list of streams\n                                                     'path/*.jpg'                    # glob\n                                                     'https://youtu.be/LNwODJXcvt4'  # YouTube\n                                                     'rtsp://example.com/media.mp4'  # RTSP, RTMP, HTTP stream\n\nUsage - formats:\n    $ python detect.py --weights yolov5s.pt                 # PyTorch\n                                 yolov5s.torchscript        # TorchScript\n                                 yolov5s.onnx               # ONNX Runtime or OpenCV DNN with --dnn\n                                 yolov5s_openvino_model     # OpenVINO\n                                 yolov5s.engine             # TensorRT\n                                 yolov5s.mlpackage          # CoreML (macOS-only)\n                                 yolov5s_saved_model        # TensorFlow SavedModel\n                                 yolov5s.pb                 # TensorFlow GraphDef\n                                 yolov5s.tflite             # TensorFlow Lite\n                                 yolov5s_edgetpu.tflite     # TensorFlow Edge TPU\n                                 yolov5s_paddle_model       # PaddlePaddle\n\"\"\"\n\nimport argparse\nimport csv\nimport os\nimport platform\nimport sys\nfrom pathlib import Path\n\nimport torch\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0]  # YOLOv5 root directory\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative\n\nfrom ultralytics.utils.plotting import Annotator, colors, save_one_box\n\nfrom models.common import DetectMultiBackend\nfrom utils.dataloaders import IMG_FORMATS, VID_FORMATS, LoadImages, LoadScreenshots, LoadStreams\nfrom utils.general import (\n    LOGGER,\n    Profile,\n    check_file,\n    check_img_size,\n    check_imshow,\n    check_requirements,\n    colorstr,\n    cv2,\n    increment_path,\n    non_max_suppression,\n    print_args,\n    scale_boxes,\n    strip_optimizer,\n    xyxy2xywh,\n)\nfrom utils.torch_utils import select_device, smart_inference_mode\n\n\n@smart_inference_mode()\ndef run(\n    weights=ROOT / \"yolov5s.pt\",  # model path or triton URL\n    source=ROOT / \"data/images\",  # file/dir/URL/glob/screen/0(webcam)\n    data=ROOT / \"data/coco128.yaml\",  # dataset.yaml path\n    imgsz=(640, 640),  # inference size (height, width)\n    conf_thres=0.25,  # confidence threshold\n    iou_thres=0.45,  # NMS IOU threshold\n    max_det=1000,  # maximum detections per image\n    device=\"\",  # cuda device, i.e. 0 or 0,1,2,3 or cpu\n    view_img=False,  # show results\n    save_txt=False,  # save results to *.txt\n    save_format=0,  # save boxes coordinates in YOLO format or Pascal-VOC format (0 for YOLO and 1 for Pascal-VOC)\n    save_csv=False,  # save results in CSV format\n    save_conf=False,  # save confidences in --save-txt labels\n    save_crop=False,  # save cropped prediction boxes\n    nosave=False,  # do not save images/videos\n    classes=None,  # filter by class: --class 0, or --class 0 2 3\n    agnostic_nms=False,  # class-agnostic NMS\n    augment=False,  # augmented inference\n    visualize=False,  # visualize features\n    update=False,  # update all models\n    project=ROOT / \"runs/detect\",  # save results to project/name\n    name=\"exp\",  # save results to project/name\n    exist_ok=False,  # existing project/name ok, do not increment\n    line_thickness=3,  # bounding box thickness (pixels)\n    hide_labels=False,  # hide labels\n    hide_conf=False,  # hide confidences\n    half=False,  # use FP16 half-precision inference\n    dnn=False,  # use OpenCV DNN for ONNX inference\n    vid_stride=1,  # video frame-rate stride\n):\n    \"\"\"\n    Runs YOLOv5 detection inference on various sources like images, videos, directories, streams, etc.\n\n    Args:\n        weights (str | Path): Path to the model weights file or a Triton URL. Default is 'yolov5s.pt'.\n        source (str | Path): Input source, which can be a file, directory, URL, glob pattern, screen capture, or webcam\n            index. Default is 'data/images'.\n        data (str | Path): Path to the dataset YAML file. Default is 'data/coco128.yaml'.\n        imgsz (tuple[int, int]): Inference image size as a tuple (height, width). Default is (640, 640).\n        conf_thres (float): Confidence threshold for detections. Default is 0.25.\n        iou_thres (float): Intersection Over Union (IOU) threshold for non-max suppression. Default is 0.45.\n        max_det (int): Maximum number of detections per image. Default is 1000.\n        device (str): CUDA device identifier (e.g., '0' or '0,1,2,3') or 'cpu'. Default is an empty string, which uses the\n            best available device.\n        view_img (bool): If True, display inference results using OpenCV. Default is False.\n        save_txt (bool): If True, save results in a text file. Default is False.\n        save_csv (bool): If True, save results in a CSV file. Default is False.\n        save_conf (bool): If True, include confidence scores in the saved results. Default is False.\n        save_crop (bool): If True, save cropped prediction boxes. Default is False.\n        nosave (bool): If True, do not save inference images or videos. Default is False.\n        classes (list[int]): List of class indices to filter detections by. Default is None.\n        agnostic_nms (bool): If True, perform class-agnostic non-max suppression. Default is False.\n        augment (bool): If True, use augmented inference. Default is False.\n        visualize (bool): If True, visualize feature maps. Default is False.\n        update (bool): If True, update all models' weights. Default is False.\n        project (str | Path): Directory to save results. Default is 'runs/detect'.\n        name (str): Name of the current experiment; used to create a subdirectory within 'project'. Default is 'exp'.\n        exist_ok (bool): If True, existing directories with the same name are reused instead of being incremented. Default is\n            False.\n        line_thickness (int): Thickness of bounding box lines in pixels. Default is 3.\n        hide_labels (bool): If True, do not display labels on bounding boxes. Default is False.\n        hide_conf (bool): If True, do not display confidence scores on bounding boxes. Default is False.\n        half (bool): If True, use FP16 half-precision inference. Default is False.\n        dnn (bool): If True, use OpenCV DNN backend for ONNX inference. Default is False.\n        vid_stride (int): Stride for processing video frames, to skip frames between processing. Default is 1.\n\n    Returns:\n        None\n\n    Examples:\n        ```python\n        from ultralytics import run\n\n        # Run inference on an image\n        run(source='data/images/example.jpg', weights='yolov5s.pt', device='0')\n\n        # Run inference on a video with specific confidence threshold\n        run(source='data/videos/example.mp4', weights='yolov5s.pt', conf_thres=0.4, device='0')\n        ```\n    \"\"\"\n    source = str(source)\n    save_img = not nosave and not source.endswith(\".txt\")  # save inference images\n    is_file = Path(source).suffix[1:] in (IMG_FORMATS + VID_FORMATS)\n    is_url = source.lower().startswith((\"rtsp://\", \"rtmp://\", \"http://\", \"https://\"))\n    webcam = source.isnumeric() or source.endswith(\".streams\") or (is_url and not is_file)\n    screenshot = source.lower().startswith(\"screen\")\n    if is_url and is_file:\n        source = check_file(source)  # download\n\n    # Directories\n    save_dir = increment_path(Path(project) / name, exist_ok=exist_ok)  # increment run\n    (save_dir / \"labels\" if save_txt else save_dir).mkdir(parents=True, exist_ok=True)  # make dir\n\n    # Load model\n    device = select_device(device)\n    model = DetectMultiBackend(weights, device=device, dnn=dnn, data=data, fp16=half)\n    stride, names, pt = model.stride, model.names, model.pt\n    imgsz = check_img_size(imgsz, s=stride)  # check image size\n\n    # Dataloader\n    bs = 1  # batch_size\n    if webcam:\n        view_img = check_imshow(warn=True)\n        dataset = LoadStreams(source, img_size=imgsz, stride=stride, auto=pt, vid_stride=vid_stride)\n        bs = len(dataset)\n    elif screenshot:\n        dataset = LoadScreenshots(source, img_size=imgsz, stride=stride, auto=pt)\n    else:\n        dataset = LoadImages(source, img_size=imgsz, stride=stride, auto=pt, vid_stride=vid_stride)\n    vid_path, vid_writer = [None] * bs, [None] * bs\n\n    # Run inference\n    model.warmup(imgsz=(1 if pt or model.triton else bs, 3, *imgsz))  # warmup\n    seen, windows, dt = 0, [], (Profile(device=device), Profile(device=device), Profile(device=device))\n    for path, im, im0s, vid_cap, s in dataset:\n        with dt[0]:\n            im = torch.from_numpy(im).to(model.device)\n            im = im.half() if model.fp16 else im.float()  # uint8 to fp16/32\n            im /= 255  # 0 - 255 to 0.0 - 1.0\n            if len(im.shape) == 3:\n                im = im[None]  # expand for batch dim\n            if model.xml and im.shape[0] > 1:\n                ims = torch.chunk(im, im.shape[0], 0)\n\n        # Inference\n        with dt[1]:\n            visualize = increment_path(save_dir / Path(path).stem, mkdir=True) if visualize else False\n            if model.xml and im.shape[0] > 1:\n                pred = None\n                for image in ims:\n                    if pred is None:\n                        pred = model(image, augment=augment, visualize=visualize).unsqueeze(0)\n                    else:\n                        pred = torch.cat((pred, model(image, augment=augment, visualize=visualize).unsqueeze(0)), dim=0)\n                pred = [pred, None]\n            else:\n                pred = model(im, augment=augment, visualize=visualize)\n        # NMS\n        with dt[2]:\n            pred = non_max_suppression(pred, conf_thres, iou_thres, classes, agnostic_nms, max_det=max_det)\n\n        # Second-stage classifier (optional)\n        # pred = utils.general.apply_classifier(pred, classifier_model, im, im0s)\n\n        # Define the path for the CSV file\n        csv_path = save_dir / \"predictions.csv\"\n\n        # Create or append to the CSV file\n        def write_to_csv(image_name, prediction, confidence):\n            \"\"\"Writes prediction data for an image to a CSV file, appending if the file exists.\"\"\"\n            data = {\"Image Name\": image_name, \"Prediction\": prediction, \"Confidence\": confidence}\n            file_exists = os.path.isfile(csv_path)\n            with open(csv_path, mode=\"a\", newline=\"\") as f:\n                writer = csv.DictWriter(f, fieldnames=data.keys())\n                if not file_exists:\n                    writer.writeheader()\n                writer.writerow(data)\n\n        # Process predictions\n        for i, det in enumerate(pred):  # per image\n            seen += 1\n            if webcam:  # batch_size >= 1\n                p, im0, frame = path[i], im0s[i].copy(), dataset.count\n                s += f\"{i}: \"\n            else:\n                p, im0, frame = path, im0s.copy(), getattr(dataset, \"frame\", 0)\n\n            p = Path(p)  # to Path\n            save_path = str(save_dir / p.name)  # im.jpg\n            txt_path = str(save_dir / \"labels\" / p.stem) + (\"\" if dataset.mode == \"image\" else f\"_{frame}\")  # im.txt\n            s += \"{:g}x{:g} \".format(*im.shape[2:])  # print string\n            gn = torch.tensor(im0.shape)[[1, 0, 1, 0]]  # normalization gain whwh\n            imc = im0.copy() if save_crop else im0  # for save_crop\n            annotator = Annotator(im0, line_width=line_thickness, example=str(names))\n            if len(det):\n                # Rescale boxes from img_size to im0 size\n                det[:, :4] = scale_boxes(im.shape[2:], det[:, :4], im0.shape).round()\n\n                # Print results\n                for c in det[:, 5].unique():\n                    n = (det[:, 5] == c).sum()  # detections per class\n                    s += f\"{n} {names[int(c)]}{'s' * (n > 1)}, \"  # add to string\n\n                # Write results\n                for *xyxy, conf, cls in reversed(det):\n                    c = int(cls)  # integer class\n                    label = names[c] if hide_conf else f\"{names[c]}\"\n                    confidence = float(conf)\n                    confidence_str = f\"{confidence:.2f}\"\n\n                    if save_csv:\n                        write_to_csv(p.name, label, confidence_str)\n\n                    if save_txt:  # Write to file\n                        if save_format == 0:\n                            coords = (\n                                (xyxy2xywh(torch.tensor(xyxy).view(1, 4)) / gn).view(-1).tolist()\n                            )  # normalized xywh\n                        else:\n                            coords = (torch.tensor(xyxy).view(1, 4) / gn).view(-1).tolist()  # xyxy\n                        line = (cls, *coords, conf) if save_conf else (cls, *coords)  # label format\n                        with open(f\"{txt_path}.txt\", \"a\") as f:\n                            f.write((\"%g \" * len(line)).rstrip() % line + \"\\n\")\n\n                    if save_img or save_crop or view_img:  # Add bbox to image\n                        c = int(cls)  # integer class\n                        label = None if hide_labels else (names[c] if hide_conf else f\"{names[c]} {conf:.2f}\")\n                        annotator.box_label(xyxy, label, color=colors(c, True))\n                    if save_crop:\n                        save_one_box(xyxy, imc, file=save_dir / \"crops\" / names[c] / f\"{p.stem}.jpg\", BGR=True)\n\n            # Stream results\n            im0 = annotator.result()\n            if view_img:\n                if platform.system() == \"Linux\" and p not in windows:\n                    windows.append(p)\n                    cv2.namedWindow(str(p), cv2.WINDOW_NORMAL | cv2.WINDOW_KEEPRATIO)  # allow window resize (Linux)\n                    cv2.resizeWindow(str(p), im0.shape[1], im0.shape[0])\n                cv2.imshow(str(p), im0)\n                cv2.waitKey(1)  # 1 millisecond\n\n            # Save results (image with detections)\n            if save_img:\n                if dataset.mode == \"image\":\n                    cv2.imwrite(save_path, im0)\n                else:  # 'video' or 'stream'\n                    if vid_path[i] != save_path:  # new video\n                        vid_path[i] = save_path\n                        if isinstance(vid_writer[i], cv2.VideoWriter):\n                            vid_writer[i].release()  # release previous video writer\n                        if vid_cap:  # video\n                            fps = vid_cap.get(cv2.CAP_PROP_FPS)\n                            w = int(vid_cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n                            h = int(vid_cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n                        else:  # stream\n                            fps, w, h = 30, im0.shape[1], im0.shape[0]\n                        save_path = str(Path(save_path).with_suffix(\".mp4\"))  # force *.mp4 suffix on results videos\n                        vid_writer[i] = cv2.VideoWriter(save_path, cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (w, h))\n                    vid_writer[i].write(im0)\n\n        # Print time (inference-only)\n        LOGGER.info(f\"{s}{'' if len(det) else '(no detections), '}{dt[1].dt * 1e3:.1f}ms\")\n\n    # Print results\n    t = tuple(x.t / seen * 1e3 for x in dt)  # speeds per image\n    LOGGER.info(f\"Speed: %.1fms pre-process, %.1fms inference, %.1fms NMS per image at shape {(1, 3, *imgsz)}\" % t)\n    if save_txt or save_img:\n        s = f\"\\n{len(list(save_dir.glob('labels/*.txt')))} labels saved to {save_dir / 'labels'}\" if save_txt else \"\"\n        LOGGER.info(f\"Results saved to {colorstr('bold', save_dir)}{s}\")\n    if update:\n        strip_optimizer(weights[0])  # update model (to fix SourceChangeWarning)\n\n\ndef parse_opt():\n    \"\"\"\n    Parse command-line arguments for YOLOv5 detection, allowing custom inference options and model configurations.\n\n    Args:\n        --weights (str | list[str], optional): Model path or Triton URL. Defaults to ROOT / 'yolov5s.pt'.\n        --source (str, optional): File/dir/URL/glob/screen/0(webcam). Defaults to ROOT / 'data/images'.\n        --data (str, optional): Dataset YAML path. Provides dataset configuration information.\n        --imgsz (list[int], optional): Inference size (height, width). Defaults to [640].\n        --conf-thres (float, optional): Confidence threshold. Defaults to 0.25.\n        --iou-thres (float, optional): NMS IoU threshold. Defaults to 0.45.\n        --max-det (int, optional): Maximum number of detections per image. Defaults to 1000.\n        --device (str, optional): CUDA device, i.e., '0' or '0,1,2,3' or 'cpu'. Defaults to \"\".\n        --view-img (bool, optional): Flag to display results. Defaults to False.\n        --save-txt (bool, optional): Flag to save results to *.txt files. Defaults to False.\n        --save-csv (bool, optional): Flag to save results in CSV format. Defaults to False.\n        --save-conf (bool, optional): Flag to save confidences in labels saved via --save-txt. Defaults to False.\n        --save-crop (bool, optional): Flag to save cropped prediction boxes. Defaults to False.\n        --nosave (bool, optional): Flag to prevent saving images/videos. Defaults to False.\n        --classes (list[int], optional): List of classes to filter results by, e.g., '--classes 0 2 3'. Defaults to None.\n        --agnostic-nms (bool, optional): Flag for class-agnostic NMS. Defaults to False.\n        --augment (bool, optional): Flag for augmented inference. Defaults to False.\n        --visualize (bool, optional): Flag for visualizing features. Defaults to False.\n        --update (bool, optional): Flag to update all models in the model directory. Defaults to False.\n        --project (str, optional): Directory to save results. Defaults to ROOT / 'runs/detect'.\n        --name (str, optional): Sub-directory name for saving results within --project. Defaults to 'exp'.\n        --exist-ok (bool, optional): Flag to allow overwriting if the project/name already exists. Defaults to False.\n        --line-thickness (int, optional): Thickness (in pixels) of bounding boxes. Defaults to 3.\n        --hide-labels (bool, optional): Flag to hide labels in the output. Defaults to False.\n        --hide-conf (bool, optional): Flag to hide confidences in the output. Defaults to False.\n        --half (bool, optional): Flag to use FP16 half-precision inference. Defaults to False.\n        --dnn (bool, optional): Flag to use OpenCV DNN for ONNX inference. Defaults to False.\n        --vid-stride (int, optional): Video frame-rate stride, determining the number of frames to skip in between\n            consecutive frames. Defaults to 1.\n\n    Returns:\n        argparse.Namespace: Parsed command-line arguments as an argparse.Namespace object.\n\n    Example:\n        ```python\n        from ultralytics import YOLOv5\n        args = YOLOv5.parse_opt()\n        ```\n    \"\"\"\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--weights\", nargs=\"+\", type=str, default=ROOT / \"yolov5s.pt\", help=\"model path or triton URL\")\n    parser.add_argument(\"--source\", type=str, default=ROOT / \"data/images\", help=\"file/dir/URL/glob/screen/0(webcam)\")\n    parser.add_argument(\"--data\", type=str, default=ROOT / \"data/coco128.yaml\", help=\"(optional) dataset.yaml path\")\n    parser.add_argument(\"--imgsz\", \"--img\", \"--img-size\", nargs=\"+\", type=int, default=[640], help=\"inference size h,w\")\n    parser.add_argument(\"--conf-thres\", type=float, default=0.25, help=\"confidence threshold\")\n    parser.add_argument(\"--iou-thres\", type=float, default=0.45, help=\"NMS IoU threshold\")\n    parser.add_argument(\"--max-det\", type=int, default=1000, help=\"maximum detections per image\")\n    parser.add_argument(\"--device\", default=\"\", help=\"cuda device, i.e. 0 or 0,1,2,3 or cpu\")\n    parser.add_argument(\"--view-img\", action=\"store_true\", help=\"show results\")\n    parser.add_argument(\"--save-txt\", action=\"store_true\", help=\"save results to *.txt\")\n    parser.add_argument(\n        \"--save-format\",\n        type=int,\n        default=0,\n        help=\"whether to save boxes coordinates in YOLO format or Pascal-VOC format when save-txt is True, 0 for YOLO and 1 for Pascal-VOC\",\n    )\n    parser.add_argument(\"--save-csv\", action=\"store_true\", help=\"save results in CSV format\")\n    parser.add_argument(\"--save-conf\", action=\"store_true\", help=\"save confidences in --save-txt labels\")\n    parser.add_argument(\"--save-crop\", action=\"store_true\", help=\"save cropped prediction boxes\")\n    parser.add_argument(\"--nosave\", action=\"store_true\", help=\"do not save images/videos\")\n    parser.add_argument(\"--classes\", nargs=\"+\", type=int, help=\"filter by class: --classes 0, or --classes 0 2 3\")\n    parser.add_argument(\"--agnostic-nms\", action=\"store_true\", help=\"class-agnostic NMS\")\n    parser.add_argument(\"--augment\", action=\"store_true\", help=\"augmented inference\")\n    parser.add_argument(\"--visualize\", action=\"store_true\", help=\"visualize features\")\n    parser.add_argument(\"--update\", action=\"store_true\", help=\"update all models\")\n    parser.add_argument(\"--project\", default=ROOT / \"runs/detect\", help=\"save results to project/name\")\n    parser.add_argument(\"--name\", default=\"exp\", help=\"save results to project/name\")\n    parser.add_argument(\"--exist-ok\", action=\"store_true\", help=\"existing project/name ok, do not increment\")\n    parser.add_argument(\"--line-thickness\", default=3, type=int, help=\"bounding box thickness (pixels)\")\n    parser.add_argument(\"--hide-labels\", default=False, action=\"store_true\", help=\"hide labels\")\n    parser.add_argument(\"--hide-conf\", default=False, action=\"store_true\", help=\"hide confidences\")\n    parser.add_argument(\"--half\", action=\"store_true\", help=\"use FP16 half-precision inference\")\n    parser.add_argument(\"--dnn\", action=\"store_true\", help=\"use OpenCV DNN for ONNX inference\")\n    parser.add_argument(\"--vid-stride\", type=int, default=1, help=\"video frame-rate stride\")\n    opt = parser.parse_args()\n    opt.imgsz *= 2 if len(opt.imgsz) == 1 else 1  # expand\n    print_args(vars(opt))\n    return opt\n\n\ndef main(opt):\n    \"\"\"\n    Executes YOLOv5 model inference based on provided command-line arguments, validating dependencies before running.\n\n    Args:\n        opt (argparse.Namespace): Command-line arguments for YOLOv5 detection. See function `parse_opt` for details.\n\n    Returns:\n        None\n\n    Note:\n        This function performs essential pre-execution checks and initiates the YOLOv5 detection process based on user-specified\n        options. Refer to the usage guide and examples for more information about different sources and formats at:\n        https://github.com/ultralytics/ultralytics\n\n    Example usage:\n\n    ```python\n    if __name__ == \"__main__\":\n        opt = parse_opt()\n        main(opt)\n    ```\n    \"\"\"\n    check_requirements(ROOT / \"requirements.txt\", exclude=(\"tensorboard\", \"thop\"))\n    run(**vars(opt))\n\n\nif __name__ == \"__main__\":\n    opt = parse_opt()\n    main(opt)\n",
    "export.py": "# Ultralytics 🚀 AGPL-3.0 License - https://ultralytics.com/license\n\"\"\"\nExport a YOLOv5 PyTorch model to other formats. TensorFlow exports authored by https://github.com/zldrobit.\n\nFormat                      | `export.py --include`         | Model\n---                         | ---                           | ---\nPyTorch                     | -                             | yolov5s.pt\nTorchScript                 | `torchscript`                 | yolov5s.torchscript\nONNX                        | `onnx`                        | yolov5s.onnx\nOpenVINO                    | `openvino`                    | yolov5s_openvino_model/\nTensorRT                    | `engine`                      | yolov5s.engine\nCoreML                      | `coreml`                      | yolov5s.mlmodel\nTensorFlow SavedModel       | `saved_model`                 | yolov5s_saved_model/\nTensorFlow GraphDef         | `pb`                          | yolov5s.pb\nTensorFlow Lite             | `tflite`                      | yolov5s.tflite\nTensorFlow Edge TPU         | `edgetpu`                     | yolov5s_edgetpu.tflite\nTensorFlow.js               | `tfjs`                        | yolov5s_web_model/\nPaddlePaddle                | `paddle`                      | yolov5s_paddle_model/\n\nRequirements:\n    $ pip install -r requirements.txt coremltools onnx onnx-simplifier onnxruntime openvino-dev tensorflow-cpu  # CPU\n    $ pip install -r requirements.txt coremltools onnx onnx-simplifier onnxruntime-gpu openvino-dev tensorflow  # GPU\n\nUsage:\n    $ python export.py --weights yolov5s.pt --include torchscript onnx openvino engine coreml tflite ...\n\nInference:\n    $ python detect.py --weights yolov5s.pt                 # PyTorch\n                                 yolov5s.torchscript        # TorchScript\n                                 yolov5s.onnx               # ONNX Runtime or OpenCV DNN with --dnn\n                                 yolov5s_openvino_model     # OpenVINO\n                                 yolov5s.engine             # TensorRT\n                                 yolov5s.mlmodel            # CoreML (macOS-only)\n                                 yolov5s_saved_model        # TensorFlow SavedModel\n                                 yolov5s.pb                 # TensorFlow GraphDef\n                                 yolov5s.tflite             # TensorFlow Lite\n                                 yolov5s_edgetpu.tflite     # TensorFlow Edge TPU\n                                 yolov5s_paddle_model       # PaddlePaddle\n\nTensorFlow.js:\n    $ cd .. && git clone https://github.com/zldrobit/tfjs-yolov5-example.git && cd tfjs-yolov5-example\n    $ npm install\n    $ ln -s ../../yolov5/yolov5s_web_model public/yolov5s_web_model\n    $ npm start\n\"\"\"\n\nimport argparse\nimport contextlib\nimport json\nimport os\nimport platform\nimport re\nimport subprocess\nimport sys\nimport time\nimport warnings\nfrom pathlib import Path\n\nimport pandas as pd\nimport torch\nfrom torch.utils.mobile_optimizer import optimize_for_mobile\n\nFILE = Path(__file__).resolve()\nROOT = FILE.parents[0]  # YOLOv5 root directory\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nif platform.system() != \"Windows\":\n    ROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative\n\nfrom models.experimental import attempt_load\nfrom models.yolo import ClassificationModel, Detect, DetectionModel, SegmentationModel\nfrom utils.dataloaders import LoadImages\nfrom utils.general import (\n    LOGGER,\n    Profile,\n    check_dataset,\n    check_img_size,\n    check_requirements,\n    check_version,\n    check_yaml,\n    colorstr,\n    file_size,\n    get_default_args,\n    print_args,\n    url2file,\n    yaml_save,\n)\nfrom utils.torch_utils import select_device, smart_inference_mode\n\nMACOS = platform.system() == \"Darwin\"  # macOS environment\n\n\nclass iOSModel(torch.nn.Module):\n    \"\"\"An iOS-compatible wrapper for YOLOv5 models that normalizes input images based on their dimensions.\"\"\"\n\n    def __init__(self, model, im):\n        \"\"\"\n        Initializes an iOS compatible model with normalization based on image dimensions.\n\n        Args:\n            model (torch.nn.Module): The PyTorch model to be adapted for iOS compatibility.\n            im (torch.Tensor): An input tensor representing a batch of images with shape (B, C, H, W).\n\n        Returns:\n            None: This method does not return any value.\n\n        Notes:\n            This initializer configures normalization based on the input image dimensions, which is critical for\n            ensuring the model's compatibility and proper functionality on iOS devices. The normalization step\n            involves dividing by the image width if the image is square; otherwise, additional conditions might apply.\n        \"\"\"\n        super().__init__()\n        b, c, h, w = im.shape  # batch, channel, height, width\n        self.model = model\n        self.nc = model.nc  # number of classes\n        if w == h:\n            self.normalize = 1.0 / w\n        else:\n            self.normalize = torch.tensor([1.0 / w, 1.0 / h, 1.0 / w, 1.0 / h])  # broadcast (slower, smaller)\n            # np = model(im)[0].shape[1]  # number of points\n            # self.normalize = torch.tensor([1. / w, 1. / h, 1. / w, 1. / h]).expand(np, 4)  # explicit (faster, larger)\n\n    def forward(self, x):\n        \"\"\"\n        Run a forward pass on the input tensor, returning class confidences and normalized coordinates.\n\n        Args:\n            x (torch.Tensor): Input tensor containing the image data with shape (batch, channels, height, width).\n\n        Returns:\n            torch.Tensor: Concatenated tensor with normalized coordinates (xywh), confidence scores (conf),\n            and class probabilities (cls), having shape (N, 4 + 1 + C), where N is the number of predictions,\n            and C is the number of classes.\n\n        Examples:\n            ```python\n            model = iOSModel(pretrained_model, input_image)\n            output = model.forward(torch_input_tensor)\n            ```\n        \"\"\"\n        xywh, conf, cls = self.model(x)[0].squeeze().split((4, 1, self.nc), 1)\n        return cls * conf, xywh * self.normalize  # confidence (3780, 80), coordinates (3780, 4)\n\n\ndef export_formats():\n    r\"\"\"\n    Returns a DataFrame of supported YOLOv5 model export formats and their properties.\n\n    Returns:\n        pandas.DataFrame: A DataFrame containing supported export formats and their properties. The DataFrame\n        includes columns for format name, CLI argument suffix, file extension or directory name, and boolean flags\n        indicating if the export format supports training and detection.\n\n    Examples:\n        ```python\n        formats = export_formats()\n        print(f\"Supported export formats:\\n{formats}\")\n        ```\n\n    Notes:\n        The DataFrame contains the following columns:\n        - Format: The name of the model format (e.g., PyTorch, TorchScript, ONNX, etc.).\n        - Include Argument: The argument to use with the export script to include this format.\n        - File Suffix: File extension or directory name associated with the format.\n        - Supports Training: Whether the format supports training.\n        - Supports Detection: Whether the format supports detection.\n    \"\"\"\n    x = [\n        [\"PyTorch\", \"-\", \".pt\", True, True],\n        [\"TorchScript\", \"torchscript\", \".torchscript\", True, True],\n        [\"ONNX\", \"onnx\", \".onnx\", True, True],\n        [\"OpenVINO\", \"openvino\", \"_openvino_model\", True, False],\n        [\"TensorRT\", \"engine\", \".engine\", False, True],\n        [\"CoreML\", \"coreml\", \".mlpackage\", True, False],\n        [\"TensorFlow SavedModel\", \"saved_model\", \"_saved_model\", True, True],\n        [\"TensorFlow GraphDef\", \"pb\", \".pb\", True, True],\n        [\"TensorFlow Lite\", \"tflite\", \".tflite\", True, False],\n        [\"TensorFlow Edge TPU\", \"edgetpu\", \"_edgetpu.tflite\", False, False],\n        [\"TensorFlow.js\", \"tfjs\", \"_web_model\", False, False],\n        [\"PaddlePaddle\", \"paddle\", \"_paddle_model\", True, True],\n    ]\n    return pd.DataFrame(x, columns=[\"Format\", \"Argument\", \"Suffix\", \"CPU\", \"GPU\"])\n\n\ndef try_export(inner_func):\n    \"\"\"\n    Log success or failure, execution time, and file size for YOLOv5 model export functions wrapped with @try_export.\n\n    Args:\n        inner_func (Callable): The model export function to be wrapped by the decorator.\n\n    Returns:\n        Callable: The wrapped function that logs execution details. When executed, this wrapper function returns either:\n            - Tuple (str | torch.nn.Module): On success — the file path of the exported model and the model instance.\n            - Tuple (None, None): On failure — None values indicating export failure.\n\n    Examples:\n        ```python\n        @try_export\n        def export_onnx(model, filepath):\n            # implementation here\n            pass\n\n        exported_file, exported_model = export_onnx(yolo_model, 'path/to/save/model.onnx')\n        ```\n\n    Notes:\n        For additional requirements and model export formats, refer to the\n        [Ultralytics YOLOv5 GitHub repository](https://github.com/ultralytics/ultralytics).\n    \"\"\"\n    inner_args = get_default_args(inner_func)\n\n    def outer_func(*args, **kwargs):\n        \"\"\"Logs success/failure and execution details of model export functions wrapped with @try_export decorator.\"\"\"\n        prefix = inner_args[\"prefix\"]\n        try:\n            with Profile() as dt:\n                f, model = inner_func(*args, **kwargs)\n            LOGGER.info(f\"{prefix} export success ✅ {dt.t:.1f}s, saved as {f} ({file_size(f):.1f} MB)\")\n            return f, model\n        except Exception as e:\n            LOGGER.info(f\"{prefix} export failure ❌ {dt.t:.1f}s: {e}\")\n            return None, None\n\n    return outer_func\n\n\n@try_export\ndef export_torchscript(model, im, file, optimize, prefix=colorstr(\"TorchScript:\")):\n    \"\"\"\n    Export a YOLOv5 model to the TorchScript format.\n\n    Args:\n        model (torch.nn.Module): The YOLOv5 model to be exported.\n        im (torch.Tensor): Example input tensor to be used for tracing the TorchScript model.\n        file (Path): File path where the exported TorchScript model will be saved.\n        optimize (bool): If True, applies optimizations for mobile deployment.\n        prefix (str): Optional prefix for log messages. Default is 'TorchScript:'.\n\n    Returns:\n        (str | None, torch.jit.ScriptModule | None): A tuple containing the file path of the exported model\n            (as a string) and the TorchScript model (as a torch.jit.ScriptModule). If the export fails, both elements\n            of the tuple will be None.\n\n    Notes:\n        - This function uses tracing to create the TorchScript model.\n        - Metadata, including the input image shape, model stride, and class names, is saved in an extra file (`config.txt`)\n          within the TorchScript model package.\n        - For mobile optimization, refer to the PyTorch tutorial: https://pytorch.org/tutorials/recipes/mobile_interpreter.html\n\n    Example:\n        ```python\n        from pathlib import Path\n        import torch\n        from models.experimental import attempt_load\n        from utils.torch_utils import select_device\n\n        # Load model\n        weights = 'yolov5s.pt'\n        device = select_device('')\n        model = attempt_load(weights, device=device)\n\n        # Example input tensor\n        im = torch.zeros(1, 3, 640, 640).to(device)\n\n        # Export model\n        file = Path('yolov5s.torchscript')\n        export_torchscript(model, im, file, optimize=False)\n        ```\n    \"\"\"\n    LOGGER.info(f\"\\n{prefix} starting export with torch {torch.__version__}...\")\n    f = file.with_suffix(\".torchscript\")\n\n    ts = torch.jit.trace(model, im, strict=False)\n    d = {\"shape\": im.shape, \"stride\": int(max(model.stride)), \"names\": model.names}\n    extra_files = {\"config.txt\": json.dumps(d)}  # torch._C.ExtraFilesMap()\n    if optimize:  # https://pytorch.org/tutorials/recipes/mobile_interpreter.html\n        optimize_for_mobile(ts)._save_for_lite_interpreter(str(f), _extra_files=extra_files)\n    else:\n        ts.save(str(f), _extra_files=extra_files)\n    return f, None\n\n\n@try_export\ndef export_onnx(model, im, file, opset, dynamic, simplify, prefix=colorstr(\"ONNX:\")):\n    \"\"\"\n    Export a YOLOv5 model to ONNX format with dynamic axes support and optional model simplification.\n\n    Args:\n        model (torch.nn.Module): The YOLOv5 model to be exported.\n        im (torch.Tensor): A sample input tensor for model tracing, usually the shape is (1, 3, height, width).\n        file (pathlib.Path | str): The output file path where the ONNX model will be saved.\n        opset (int): The ONNX opset version to use for export.\n        dynamic (bool): If True, enables dynamic axes for batch, height, and width dimensions.\n        simplify (bool): If True, applies ONNX model simplification for optimization.\n        prefix (str): A prefix string for logging messages, defaults to 'ONNX:'.\n\n    Returns:\n        tuple[pathlib.Path | str, None]: The path to the saved ONNX model file and None (consistent with decorator).\n\n    Raises:\n        ImportError: If required libraries for export (e.g., 'onnx', 'onnx-simplifier') are not installed.\n        AssertionError: If the simplification check fails.\n\n    Notes:\n        The required packages for this function can be installed via:\n        ```\n        pip install onnx onnx-simplifier onnxruntime onnxruntime-gpu\n        ```\n\n    Example:\n        ```python\n        from pathlib import Path\n        import torch\n        from models.experimental import attempt_load\n        from utils.torch_utils import select_device\n\n        # Load model\n        weights = 'yolov5s.pt'\n        device = select_device('')\n        model = attempt_load(weights, map_location=device)\n\n        # Example input tensor\n        im = torch.zeros(1, 3, 640, 640).to(device)\n\n        # Export model\n        file_path = Path('yolov5s.onnx')\n        export_onnx(model, im, file_path, opset=12, dynamic=True, simplify=True)\n        ```\n    \"\"\"\n    check_requirements(\"onnx>=1.12.0\")\n    import onnx\n\n    LOGGER.info(f\"\\n{prefix} starting export with onnx {onnx.__version__}...\")\n    f = str(file.with_suffix(\".onnx\"))\n\n    output_names = [\"output0\", \"output1\"] if isinstance(model, SegmentationModel) else [\"output0\"]\n    if dynamic:\n        dynamic = {\"images\": {0: \"batch\", 2: \"height\", 3: \"width\"}}  # shape(1,3,640,640)\n        if isinstance(model, SegmentationModel):\n            dynamic[\"output0\"] = {0: \"batch\", 1: \"anchors\"}  # shape(1,25200,85)\n            dynamic[\"output1\"] = {0: \"batch\", 2: \"mask_height\", 3: \"mask_width\"}  # shape(1,32,160,160)\n        elif isinstance(model, DetectionModel):\n            dynamic[\"output0\"] = {0: \"batch\", 1: \"anchors\"}  # shape(1,25200,85)\n\n    torch.onnx.export(\n        model.cpu() if dynamic else model,  # --dynamic only compatible with cpu\n        im.cpu() if dynamic else im,\n        f,\n        verbose=False,\n        opset_version=opset,\n        do_constant_folding=True,  # WARNING: DNN inference with torch>=1.12 may require do_constant_folding=False\n        input_names=[\"images\"],\n        output_names=output_names,\n        dynamic_axes=dynamic or None,\n    )\n\n    # Checks\n    model_onnx = onnx.load(f)  # load onnx model\n    onnx.checker.check_model(model_onnx)  # check onnx model\n\n    # Metadata\n    d = {\"stride\": int(max(model.stride)), \"names\": model.names}\n    for k, v in d.items():\n        meta = model_onnx.metadata_props.add()\n        meta.key, meta.value = k, str(v)\n    onnx.save(model_onnx, f)\n\n    # Simplify\n    if simplify:\n        try:\n            cuda = torch.cuda.is_available()\n            check_requirements((\"onnxruntime-gpu\" if cuda else \"onnxruntime\", \"onnxslim\"))\n            import onnxslim\n\n            LOGGER.info(f\"{prefix} slimming with onnxslim {onnxslim.__version__}...\")\n            model_onnx = onnxslim.slim(model_onnx)\n            onnx.save(model_onnx, f)\n        except Exception as e:\n            LOGGER.info(f\"{prefix} simplifier failure: {e}\")\n    return f, model_onnx\n\n\n@try_export\ndef export_openvino(file, metadata, half, int8, data, prefix=colorstr(\"OpenVINO:\")):\n    \"\"\"\n    Export a YOLOv5 model to OpenVINO format with optional FP16 and INT8 quantization.\n\n    Args:\n        file (Path): Path to the output file where the OpenVINO model will be saved.\n        metadata (dict): Dictionary including model metadata such as names and strides.\n        half (bool): If True, export the model with FP16 precision.\n        int8 (bool): If True, export the model with INT8 quantization.\n        data (str): Path to the dataset YAML file required for INT8 quantization.\n        prefix (str): Prefix string for logging purposes (default is \"OpenVINO:\").\n\n    Returns:\n        (str, openvino.runtime.Model | None): The OpenVINO model file path and openvino.runtime.Model object if export is\n            successful; otherwise, None.\n\n    Notes:\n        - Requires `openvino-dev` package version 2023.0 or higher. Install with:\n          `$ pip install openvino-dev>=2023.0`\n        - For INT8 quantization, also requires `nncf` library version 2.5.0 or higher. Install with:\n          `$ pip install nncf>=2.5.0`\n\n    Examples:\n        ```python\n        from pathlib import Path\n        from ultralytics import YOLOv5\n\n        model = YOLOv5('yolov5s.pt')\n        export_openvino(Path('yolov5s.onnx'), metadata={'names': model.names, 'stride': model.stride}, half=True,\n                        int8=False, data='data.yaml')\n        ```\n\n        This will export the YOLOv5 model to OpenVINO with FP16 precision but without INT8 quantization, saving it to\n        the specified file path.\n    \"\"\"\n    check_requirements(\"openvino-dev>=2023.0\")  # requires openvino-dev: https://pypi.org/project/openvino-dev/\n    import openvino.runtime as ov  # noqa\n    from openvino.tools import mo  # noqa\n\n    LOGGER.info(f\"\\n{prefix} starting export with openvino {ov.__version__}...\")\n    f = str(file).replace(file.suffix, f\"_{'int8_' if int8 else ''}openvino_model{os.sep}\")\n    f_onnx = file.with_suffix(\".onnx\")\n    f_ov = str(Path(f) / file.with_suffix(\".xml\").name)\n\n    ov_model = mo.convert_model(f_onnx, model_name=file.stem, framework=\"onnx\", compress_to_fp16=half)  # export\n\n    if int8:\n        check_requirements(\"nncf>=2.5.0\")  # requires at least version 2.5.0 to use the post-training quantization\n        import nncf\n        import numpy as np\n\n        from utils.dataloaders import create_dataloader\n\n        def gen_dataloader(yaml_path, task=\"train\", imgsz=640, workers=4):\n            \"\"\"Generates a DataLoader for model training or validation based on the given YAML dataset configuration.\"\"\"\n            data_yaml = check_yaml(yaml_path)\n            data = check_dataset(data_yaml)\n            dataloader = create_dataloader(\n                data[task], imgsz=imgsz, batch_size=1, stride=32, pad=0.5, single_cls=False, rect=False, workers=workers\n            )[0]\n            return dataloader\n\n        # noqa: F811\n\n        def transform_fn(data_item):\n            \"\"\"\n            Quantization transform function.\n\n            Extracts and preprocess input data from dataloader item for quantization.\n\n            Args:\n               data_item: Tuple with data item produced by DataLoader during iteration\n\n            Returns:\n                input_tensor: Input data for quantization\n            \"\"\"\n            assert data_item[0].dtype == torch.uint8, \"input image must be uint8 for the quantization preprocessing\"\n\n            img = data_item[0].numpy().astype(np.float32)  # uint8 to fp16/32\n            img /= 255.0  # 0 - 255 to 0.0 - 1.0\n            return np.expand_dims(img, 0) if img.ndim == 3 else img\n\n        ds = gen_dataloader(data)\n        quantization_dataset = nncf.Dataset(ds, transform_fn)\n        ov_model = nncf.quantize(ov_model, quantization_dataset, preset=nncf.QuantizationPreset.MIXED)\n\n    ov.serialize(ov_model, f_ov)  # save\n    yaml_save(Path(f) / file.with_suffix(\".yaml\").name, metadata)  # add metadata.yaml\n    return f, None\n\n\n@try_export\ndef export_paddle(model, im, file, metadata, prefix=colorstr(\"PaddlePaddle:\")):\n    \"\"\"\n    Export a YOLOv5 PyTorch model to PaddlePaddle format using X2Paddle, saving the converted model and metadata.\n\n    Args:\n        model (torch.nn.Module): The YOLOv5 model to be exported.\n        im (torch.Tensor): Input tensor used for model tracing during export.\n        file (pathlib.Path): Path to the source file to be converted.\n        metadata (dict): Additional metadata to be saved alongside the model.\n        prefix (str): Prefix for logging information.\n\n    Returns:\n        tuple (str, None): A tuple where the first element is the path to the saved PaddlePaddle model, and the\n        second element is None.\n\n    Examples:\n        ```python\n        from pathlib import Path\n        import torch\n\n        # Assume 'model' is a pre-trained YOLOv5 model and 'im' is an example input tensor\n        model = ...  # Load your model here\n        im = torch.randn((1, 3, 640, 640))  # Dummy input tensor for tracing\n        file = Path(\"yolov5s.pt\")\n        metadata = {\"stride\": 32, \"names\": [\"person\", \"bicycle\", \"car\", \"motorbike\"]}\n\n        export_paddle(model=model, im=im, file=file, metadata=metadata)\n        ```\n\n    Notes:\n        Ensure that `paddlepaddle` and `x2paddle` are installed, as these are required for the export function. You can\n        install them via pip:\n        ```\n        $ pip install paddlepaddle x2paddle\n        ```\n    \"\"\"\n    check_requirements((\"paddlepaddle\", \"x2paddle\"))\n    import x2paddle\n    from x2paddle.convert import pytorch2paddle\n\n    LOGGER.info(f\"\\n{prefix} starting export with X2Paddle {x2paddle.__version__}...\")\n    f = str(file).replace(\".pt\", f\"_paddle_model{os.sep}\")\n\n    pytorch2paddle(module=model, save_dir=f, jit_type=\"trace\", input_examples=[im])  # export\n    yaml_save(Path(f) / file.with_suffix(\".yaml\").name, metadata)  # add metadata.yaml\n    return f, None\n\n\n@try_export\ndef export_coreml(model, im, file, int8, half, nms, mlmodel, prefix=colorstr(\"CoreML:\")):\n    \"\"\"\n    Export a YOLOv5 model to CoreML format with optional NMS, INT8, and FP16 support.\n\n    Args:\n        model (torch.nn.Module): The YOLOv5 model to be exported.\n        im (torch.Tensor): Example input tensor to trace the model.\n        file (pathlib.Path): Path object where the CoreML model will be saved.\n        int8 (bool): Flag indicating whether to use INT8 quantization (default is False).\n        half (bool): Flag indicating whether to use FP16 quantization (default is False).\n        nms (bool): Flag indicating whether to include Non-Maximum Suppression (default is False).\n        mlmodel (bool): Flag indicating whether to export as older *.mlmodel format (default is False).\n        prefix (str): Prefix string for logging purposes (default is 'CoreML:').\n\n    Returns:\n        tuple[pathlib.Path | None, None]: The path to the saved CoreML model file, or (None, None) if there is an error.\n\n    Notes:\n        The exported CoreML model will be saved with a .mlmodel extension.\n        Quantization is supported only on macOS.\n\n    Example:\n        ```python\n        from pathlib import Path\n        import torch\n        from models.yolo import Model\n        model = Model(cfg, ch=3, nc=80)\n        im = torch.randn(1, 3, 640, 640)\n        file = Path(\"yolov5s_coreml\")\n        export_coreml(model, im, file, int8=False, half=False, nms=True, mlmodel=False)\n        ```\n    \"\"\"\n    check_requirements(\"coremltools\")\n    import coremltools as ct\n\n    LOGGER.info(f\"\\n{prefix} starting export with coremltools {ct.__version__}...\")\n    if mlmodel:\n        f = file.with_suffix(\".mlmodel\")\n        convert_to = \"neuralnetwork\"\n        precision = None\n    else:\n        f = file.with_suffix(\".mlpackage\")\n        convert_to = \"mlprogram\"\n        precision = ct.precision.FLOAT16 if half else ct.precision.FLOAT32\n    if nms:\n        model = iOSModel(model, im)\n    ts = torch.jit.trace(model, im, strict=False)  # TorchScript model\n    ct_model = ct.convert(\n        ts,\n        inputs=[ct.ImageType(\"image\", shape=im.shape, scale=1 / 255, bias=[0, 0, 0])],\n        convert_to=convert_to,\n        compute_precision=precision,\n    )\n    bits, mode = (8, \"kmeans\") if int8 else (16, \"linear\") if half else (32, None)\n    if bits < 32:\n        if mlmodel:\n            with warnings.catch_warnings():\n                warnings.filterwarnings(\n                    \"ignore\", category=DeprecationWarning\n                )  # suppress numpy==1.20 float warning, fixed in coremltools==7.0\n                ct_model = ct.models.neural_network.quantization_utils.quantize_weights(ct_model, bits, mode)\n        elif bits == 8:\n            op_config = ct.optimize.coreml.OpPalettizerConfig(mode=mode, nbits=bits, weight_threshold=512)\n            config = ct.optimize.coreml.OptimizationConfig(global_config=op_config)\n            ct_model = ct.optimize.coreml.palettize_weights(ct_model, config)\n    ct_model.save(f)\n    return f, ct_model\n\n\n@try_export\ndef export_engine(\n    model, im, file, half, dynamic, simplify, workspace=4, verbose=False, cache=\"\", prefix=colorstr(\"TensorRT:\")\n):\n    \"\"\"\n    Export a YOLOv5 model to TensorRT engine format, requiring GPU and TensorRT>=7.0.0.\n\n    Args:\n        model (torch.nn.Module): YOLOv5 model to be exported.\n        im (torch.Tensor): Input tensor of shape (B, C, H, W).\n        file (pathlib.Path): Path to save the exported model.\n        half (bool): Set to True to export with FP16 precision.\n        dynamic (bool): Set to True to enable dynamic input shapes.\n        simplify (bool): Set to True to simplify the model during export.\n        workspace (int): Workspace size in GB (default is 4).\n        verbose (bool): Set to True for verbose logging output.\n        cache (str): Path to save the TensorRT timing cache.\n        prefix (str): Log message prefix.\n\n    Returns:\n        (pathlib.Path, None): Tuple containing the path to the exported model and None.\n\n    Raises:\n        AssertionError: If executed on CPU instead of GPU.\n        RuntimeError: If there is a failure in parsing the ONNX file.\n\n    Example:\n        ```python\n        from ultralytics import YOLOv5\n        import torch\n        from pathlib import Path\n\n        model = YOLOv5('yolov5s.pt')  # Load a pre-trained YOLOv5 model\n        input_tensor = torch.randn(1, 3, 640, 640).cuda()  # example input tensor on GPU\n        export_path = Path('yolov5s.engine')  # export destination\n\n        export_engine(model.model, input_tensor, export_path, half=True, dynamic=True, simplify=True, workspace=8, verbose=True)\n        ```\n    \"\"\"\n    assert im.device.type != \"cpu\", \"export running on CPU but must be on GPU, i.e. `python export.py --device 0`\"\n    try:\n        import tensorrt as trt\n    except Exception:\n        if platform.system() == \"Linux\":\n            check_requirements(\"nvidia-tensorrt\", cmds=\"-U --index-url https://pypi.ngc.nvidia.com\")\n        import tensorrt as trt\n\n    if trt.__version__[0] == \"7\":  # TensorRT 7 handling https://github.com/ultralytics/yolov5/issues/6012\n        grid = model.model[-1].anchor_grid\n        model.model[-1].anchor_grid = [a[..., :1, :1, :] for a in grid]\n        export_onnx(model, im, file, 12, dynamic, simplify)  # opset 12\n        model.model[-1].anchor_grid = grid\n    else:  # TensorRT >= 8\n        check_version(trt.__version__, \"8.0.0\", hard=True)  # require tensorrt>=8.0.0\n        export_onnx(model, im, file, 12, dynamic, simplify)  # opset 12\n    onnx = file.with_suffix(\".onnx\")\n\n    LOGGER.info(f\"\\n{prefix} starting export with TensorRT {trt.__version__}...\")\n    is_trt10 = int(trt.__version__.split(\".\")[0]) >= 10  # is TensorRT >= 10\n    assert onnx.exists(), f\"failed to export ONNX file: {onnx}\"\n    f = file.with_suffix(\".engine\")  # TensorRT engine file\n    logger = trt.Logger(trt.Logger.INFO)\n    if verbose:\n        logger.min_severity = trt.Logger.Severity.VERBOSE\n\n    builder = trt.Builder(logger)\n    config = builder.create_builder_config()\n    if is_trt10:\n        config.set_memory_pool_limit(trt.MemoryPoolType.WORKSPACE, workspace << 30)\n    else:  # TensorRT versions 7, 8\n        config.max_workspace_size = workspace * 1 << 30\n    if cache:  # enable timing cache\n        Path(cache).parent.mkdir(parents=True, exist_ok=True)\n        buf = Path(cache).read_bytes() if Path(cache).exists() else b\"\"\n        timing_cache = config.create_timing_cache(buf)\n        config.set_timing_cache(timing_cache, ignore_mismatch=True)\n    flag = 1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)\n    network = builder.create_network(flag)\n    parser = trt.OnnxParser(network, logger)\n    if not parser.parse_from_file(str(onnx)):\n        raise RuntimeError(f\"failed to load ONNX file: {onnx}\")\n\n    inputs = [network.get_input(i) for i in range(network.num_inputs)]\n    outputs = [network.get_output(i) for i in range(network.num_outputs)]\n    for inp in inputs:\n        LOGGER.info(f'{prefix} input \"{inp.name}\" with shape{inp.shape} {inp.dtype}')\n    for out in outputs:\n        LOGGER.info(f'{prefix} output \"{out.name}\" with shape{out.shape} {out.dtype}')\n\n    if dynamic:\n        if im.shape[0] <= 1:\n            LOGGER.warning(f\"{prefix} WARNING ⚠️ --dynamic model requires maximum --batch-size argument\")\n        profile = builder.create_optimization_profile()\n        for inp in inputs:\n            profile.set_shape(inp.name, (1, *im.shape[1:]), (max(1, im.shape[0] // 2), *im.shape[1:]), im.shape)\n        config.add_optimization_profile(profile)\n\n    LOGGER.info(f\"{prefix} building FP{16 if builder.platform_has_fast_fp16 and half else 32} engine as {f}\")\n    if builder.platform_has_fast_fp16 and half:\n        config.set_flag(trt.BuilderFlag.FP16)\n\n    build = builder.build_serialized_network if is_trt10 else builder.build_engine\n    with build(network, config) as engine, open(f, \"wb\") as t:\n        t.write(engine if is_trt10 else engine.serialize())\n    if cache:  # save timing cache\n        with open(cache, \"wb\") as c:\n            c.write(config.get_timing_cache().serialize())\n    return f, None\n\n\n@try_export\ndef export_saved_model(\n    model,\n    im,\n    file,\n    dynamic,\n    tf_nms=False,\n    agnostic_nms=False,\n    topk_per_class=100,\n    topk_all=100,\n    iou_thres=0.45,\n    conf_thres=0.25,\n    keras=False,\n    prefix=colorstr(\"TensorFlow SavedModel:\"),\n):\n    \"\"\"\n    Export a YOLOv5 model to the TensorFlow SavedModel format, supporting dynamic axes and non-maximum suppression\n    (NMS).\n\n    Args:\n        model (torch.nn.Module): The PyTorch model to convert.\n        im (torch.Tensor): Sample input tensor with shape (B, C, H, W) for tracing.\n        file (pathlib.Path): File path to save the exported model.\n        dynamic (bool): Flag to indicate whether dynamic axes should be used.\n        tf_nms (bool, optional): Enable TensorFlow non-maximum suppression (NMS). Default is False.\n        agnostic_nms (bool, optional): Enable class-agnostic NMS. Default is False.\n        topk_per_class (int, optional): Top K detections per class to keep before applying NMS. Default is 100.\n        topk_all (int, optional): Top K detections across all classes to keep before applying NMS. Default is 100.\n        iou_thres (float, optional): IoU threshold for NMS. Default is 0.45.\n        conf_thres (float, optional): Confidence threshold for detections. Default is 0.25.\n        keras (bool, optional): Save the model in Keras format if True. Default is False.\n        prefix (str, optional): Prefix for logging messages. Default is \"TensorFlow SavedModel:\".\n\n    Returns:\n        tuple[str, tf.keras.Model | None]: A tuple containing the path to the saved model folder and the Keras model instance,\n        or None if TensorFlow export fails.\n\n    Notes:\n        - The method supports TensorFlow versions up to 2.15.1.\n        - TensorFlow NMS may not be supported in older TensorFlow versions.\n        - If the TensorFlow version exceeds 2.13.1, it might cause issues when exporting to TFLite.\n          Refer to: https://github.com/ultralytics/yolov5/issues/12489\n\n    Example:\n        ```python\n        model, im = ...  # Initialize your PyTorch model and input tensor\n        export_saved_model(model, im, Path(\"yolov5_saved_model\"), dynamic=True)\n        ```\n    \"\"\"\n    # YOLOv5 TensorFlow SavedModel export\n    try:\n        import tensorflow as tf\n    except Exception:\n        check_requirements(f\"tensorflow{'' if torch.cuda.is_available() else '-macos' if MACOS else '-cpu'}<=2.15.1\")\n\n        import tensorflow as tf\n    from tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2\n\n    from models.tf import TFModel\n\n    LOGGER.info(f\"\\n{prefix} starting export with tensorflow {tf.__version__}...\")\n    if tf.__version__ > \"2.13.1\":\n        helper_url = \"https://github.com/ultralytics/yolov5/issues/12489\"\n        LOGGER.info(\n            f\"WARNING ⚠️ using Tensorflow {tf.__version__} > 2.13.1 might cause issue when exporting the model to tflite {helper_url}\"\n        )  # handling issue https://github.com/ultralytics/yolov5/issues/12489\n    f = str(file).replace(\".pt\", \"_saved_model\")\n    batch_size, ch, *imgsz = list(im.shape)  # BCHW\n\n    tf_model = TFModel(cfg=model.yaml, model=model, nc=model.nc, imgsz=imgsz)\n    im = tf.zeros((batch_size, *imgsz, ch))  # BHWC order for TensorFlow\n    _ = tf_model.predict(im, tf_nms, agnostic_nms, topk_per_class, topk_all, iou_thres, conf_thres)\n    inputs = tf.keras.Input(shape=(*imgsz, ch), batch_size=None if dynamic else batch_size)\n    outputs = tf_model.predict(inputs, tf_nms, agnostic_nms, topk_per_class, topk_all, iou_thres, conf_thres)\n    keras_model = tf.keras.Model(inputs=inputs, outputs=outputs)\n    keras_model.trainable = False\n    keras_model.summary()\n    if keras:\n        keras_model.save(f, save_format=\"tf\")\n    else:\n        spec = tf.TensorSpec(keras_model.inputs[0].shape, keras_model.inputs[0].dtype)\n        m = tf.function(lambda x: keras_model(x))  # full model\n        m = m.get_concrete_function(spec)\n        frozen_func = convert_variables_to_constants_v2(m)\n        tfm = tf.Module()\n        tfm.__call__ = tf.function(lambda x: frozen_func(x)[:4] if tf_nms else frozen_func(x), [spec])\n        tfm.__call__(im)\n        tf.saved_model.save(\n            tfm,\n            f,\n            options=tf.saved_model.SaveOptions(experimental_custom_gradients=False)\n            if check_version(tf.__version__, \"2.6\")\n            else tf.saved_model.SaveOptions(),\n        )\n    return f, keras_model\n\n\n@try_export\ndef export_pb(keras_model, file, prefix=colorstr(\"TensorFlow GraphDef:\")):\n    \"\"\"\n    Export YOLOv5 model to TensorFlow GraphDef (*.pb) format.\n\n    Args:\n        keras_model (tf.keras.Model): The Keras model to be converted.\n        file (Path): The output file path where the GraphDef will be saved.\n        prefix (str): Optional prefix string; defaults to a colored string indicating TensorFlow GraphDef export status.\n\n    Returns:\n        Tuple[Path, None]: The file path where the GraphDef model was saved and a None placeholder.\n\n    Notes:\n        For more details, refer to the guide on frozen graphs: https://github.com/leimao/Frozen_Graph_TensorFlow\n\n    Example:\n        ```python\n        from pathlib import Path\n        keras_model = ...  # assume an existing Keras model\n        file = Path(\"model.pb\")\n        export_pb(keras_model, file)\n        ```\n    \"\"\"\n    import tensorflow as tf\n    from tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2\n\n    LOGGER.info(f\"\\n{prefix} starting export with tensorflow {tf.__version__}...\")\n    f = file.with_suffix(\".pb\")\n\n    m = tf.function(lambda x: keras_model(x))  # full model\n    m = m.get_concrete_function(tf.TensorSpec(keras_model.inputs[0].shape, keras_model.inputs[0].dtype))\n    frozen_func = convert_variables_to_constants_v2(m)\n    frozen_func.graph.as_graph_def()\n    tf.io.write_graph(graph_or_graph_def=frozen_func.graph, logdir=str(f.parent), name=f.name, as_text=False)\n    return f, None\n\n\n@try_export\ndef export_tflite(\n    keras_model, im, file, int8, per_tensor, data, nms, agnostic_nms, prefix=colorstr(\"TensorFlow Lite:\")\n):\n    # YOLOv5 TensorFlow Lite export\n    \"\"\"\n    Export a YOLOv5 model to TensorFlow Lite format with optional INT8 quantization and NMS support.\n\n    Args:\n        keras_model (tf.keras.Model): The Keras model to be exported.\n        im (torch.Tensor): An input image tensor for normalization and model tracing.\n        file (Path): The file path to save the TensorFlow Lite model.\n        int8 (bool): Enables INT8 quantization if True.\n        per_tensor (bool): If True, disables per-channel quantization.\n        data (str): Path to the dataset for representative dataset generation in INT8 quantization.\n        nms (bool): Enables Non-Maximum Suppression (NMS) if True.\n        agnostic_nms (bool): Enables class-agnostic NMS if True.\n        prefix (str): Prefix for log messages.\n\n    Returns:\n        (str | None, tflite.Model | None): The file path of the exported TFLite model and the TFLite model instance, or None\n        if the export failed.\n\n    Example:\n        ```python\n        from pathlib import Path\n        import torch\n        import tensorflow as tf\n\n        # Load a Keras model wrapping a YOLOv5 model\n        keras_model = tf.keras.models.load_model('path/to/keras_model.h5')\n\n        # Example input tensor\n        im = torch.zeros(1, 3, 640, 640)\n\n        # Export the model\n        export_tflite(keras_model, im, Path('model.tflite'), int8=True, per_tensor=False, data='data/coco.yaml',\n                      nms=True, agnostic_nms=False)\n        ```\n\n    Notes:\n        - Ensure TensorFlow and TensorFlow Lite dependencies are installed.\n        - INT8 quantization requires a representative dataset to achieve optimal accuracy.\n        - TensorFlow Lite models are suitable for efficient inference on mobile and edge devices.\n    \"\"\"\n    import tensorflow as tf\n\n    LOGGER.info(f\"\\n{prefix} starting export with tensorflow {tf.__version__}...\")\n    batch_size, ch, *imgsz = list(im.shape)  # BCHW\n    f = str(file).replace(\".pt\", \"-fp16.tflite\")\n\n    converter = tf.lite.TFLiteConverter.from_keras_model(keras_model)\n    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]\n    converter.target_spec.supported_types = [tf.float16]\n    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n    if int8:\n        from models.tf import representative_dataset_gen\n\n        dataset = LoadImages(check_dataset(check_yaml(data))[\"train\"], img_size=imgsz, auto=False)\n        converter.representative_dataset = lambda: representative_dataset_gen(dataset, ncalib=100)\n        converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n        converter.target_spec.supported_types = []\n        converter.inference_input_type = tf.uint8  # or tf.int8\n        converter.inference_output_type = tf.uint8  # or tf.int8\n        converter.experimental_new_quantizer = True\n        if per_tensor:\n            converter._experimental_disable_per_channel = True\n        f = str(file).replace(\".pt\", \"-int8.tflite\")\n    if nms or agnostic_nms:\n        converter.target_spec.supported_ops.append(tf.lite.OpsSet.SELECT_TF_OPS)\n\n    tflite_model = converter.convert()\n    open(f, \"wb\").write(tflite_model)\n    return f, None\n\n\n@try_export\ndef export_edgetpu(file, prefix=colorstr(\"Edge TPU:\")):\n    \"\"\"\n    Exports a YOLOv5 model to Edge TPU compatible TFLite format; requires Linux and Edge TPU compiler.\n\n    Args:\n        file (Path): Path to the YOLOv5 model file to be exported (.pt format).\n        prefix (str, optional): Prefix for logging messages. Defaults to colorstr(\"Edge TPU:\").\n\n    Returns:\n        tuple[Path, None]: Path to the exported Edge TPU compatible TFLite model, None.\n\n    Raises:\n        AssertionError: If the system is not Linux.\n        subprocess.CalledProcessError: If any subprocess call to install or run the Edge TPU compiler fails.\n\n    Notes:\n        To use this function, ensure you have the Edge TPU compiler installed on your Linux system. You can find\n        installation instructions here: https://coral.ai/docs/edgetpu/compiler/.\n\n    Example:\n        ```python\n        from pathlib import Path\n        file = Path('yolov5s.pt')\n        export_edgetpu(file)\n        ```\n    \"\"\"\n    cmd = \"edgetpu_compiler --version\"\n    help_url = \"https://coral.ai/docs/edgetpu/compiler/\"\n    assert platform.system() == \"Linux\", f\"export only supported on Linux. See {help_url}\"\n    if subprocess.run(f\"{cmd} > /dev/null 2>&1\", shell=True).returncode != 0:\n        LOGGER.info(f\"\\n{prefix} export requires Edge TPU compiler. Attempting install from {help_url}\")\n        sudo = subprocess.run(\"sudo --version >/dev/null\", shell=True).returncode == 0  # sudo installed on system\n        for c in (\n            \"curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -\",\n            'echo \"deb https://packages.cloud.google.com/apt coral-edgetpu-stable main\" | sudo tee /etc/apt/sources.list.d/coral-edgetpu.list',\n            \"sudo apt-get update\",\n            \"sudo apt-get install edgetpu-compiler\",\n        ):\n            subprocess.run(c if sudo else c.replace(\"sudo \", \"\"), shell=True, check=True)\n    ver = subprocess.run(cmd, shell=True, capture_output=True, check=True).stdout.decode().split()[-1]\n\n    LOGGER.info(f\"\\n{prefix} starting export with Edge TPU compiler {ver}...\")\n    f = str(file).replace(\".pt\", \"-int8_edgetpu.tflite\")  # Edge TPU model\n    f_tfl = str(file).replace(\".pt\", \"-int8.tflite\")  # TFLite model\n\n    subprocess.run(\n        [\n            \"edgetpu_compiler\",\n            \"-s\",\n            \"-d\",\n            \"-k\",\n            \"10\",\n            \"--out_dir\",\n            str(file.parent),\n            f_tfl,\n        ],\n        check=True,\n    )\n    return f, None\n\n\n@try_export\ndef export_tfjs(file, int8, prefix=colorstr(\"TensorFlow.js:\")):\n    \"\"\"\n    Convert a YOLOv5 model to TensorFlow.js format with optional uint8 quantization.\n\n    Args:\n        file (Path): Path to the YOLOv5 model file to be converted, typically having a \".pt\" or \".onnx\" extension.\n        int8 (bool): If True, applies uint8 quantization during the conversion process.\n        prefix (str): Optional prefix for logging messages, default is 'TensorFlow.js:' with color formatting.\n\n    Returns:\n        (str, None): Tuple containing the output directory path as a string and None.\n\n    Notes:\n        - This function requires the `tensorflowjs` package. Install it using:\n          ```shell\n          pip install tensorflowjs\n          ```\n        - The converted TensorFlow.js model will be saved in a directory with the \"_web_model\" suffix appended to the original file name.\n        - The conversion involves running shell commands that invoke the TensorFlow.js converter tool.\n\n    Example:\n        ```python\n        from pathlib import Path\n        file = Path('yolov5.onnx')\n        export_tfjs(file, int8=False)\n        ```\n    \"\"\"\n    check_requirements(\"tensorflowjs\")\n    import tensorflowjs as tfjs\n\n    LOGGER.info(f\"\\n{prefix} starting export with tensorflowjs {tfjs.__version__}...\")\n    f = str(file).replace(\".pt\", \"_web_model\")  # js dir\n    f_pb = file.with_suffix(\".pb\")  # *.pb path\n    f_json = f\"{f}/model.json\"  # *.json path\n\n    args = [\n        \"tensorflowjs_converter\",\n        \"--input_format=tf_frozen_model\",\n        \"--quantize_uint8\" if int8 else \"\",\n        \"--output_node_names=Identity,Identity_1,Identity_2,Identity_3\",\n        str(f_pb),\n        f,\n    ]\n    subprocess.run([arg for arg in args if arg], check=True)\n\n    json = Path(f_json).read_text()\n    with open(f_json, \"w\") as j:  # sort JSON Identity_* in ascending order\n        subst = re.sub(\n            r'{\"outputs\": {\"Identity.?.?\": {\"name\": \"Identity.?.?\"}, '\n            r'\"Identity.?.?\": {\"name\": \"Identity.?.?\"}, '\n            r'\"Identity.?.?\": {\"name\": \"Identity.?.?\"}, '\n            r'\"Identity.?.?\": {\"name\": \"Identity.?.?\"}}}',\n            r'{\"outputs\": {\"Identity\": {\"name\": \"Identity\"}, '\n            r'\"Identity_1\": {\"name\": \"Identity_1\"}, '\n            r'\"Identity_2\": {\"name\": \"Identity_2\"}, '\n            r'\"Identity_3\": {\"name\": \"Identity_3\"}}}',\n            json,\n        )\n        j.write(subst)\n    return f, None\n\n\ndef add_tflite_metadata(file, metadata, num_outputs):\n    \"\"\"\n    Adds metadata to a TensorFlow Lite (TFLite) model file, supporting multiple outputs according to TensorFlow\n    guidelines.\n\n    Args:\n        file (str): Path to the TFLite model file to which metadata will be added.\n        metadata (dict): Metadata information to be added to the model, structured as required by the TFLite metadata schema.\n            Common keys include \"name\", \"description\", \"version\", \"author\", and \"license\".\n        num_outputs (int): Number of output tensors the model has, used to configure the metadata properly.\n\n    Returns:\n        None\n\n    Example:\n        ```python\n        metadata = {\n            \"name\": \"yolov5\",\n            \"description\": \"YOLOv5 object detection model\",\n            \"version\": \"1.0\",\n            \"author\": \"Ultralytics\",\n            \"license\": \"Apache License 2.0\"\n        }\n        add_tflite_metadata(\"model.tflite\", metadata, num_outputs=4)\n        ```\n\n    Note:\n        TFLite metadata can include information such as model name, version, author, and other relevant details.\n        For more details on the structure of the metadata, refer to TensorFlow Lite\n        [metadata guidelines](https://www.tensorflow.org/lite/models/convert/metadata).\n    \"\"\"\n    with contextlib.suppress(ImportError):\n        # check_requirements('tflite_support')\n        from tflite_support import flatbuffers\n        from tflite_support import metadata as _metadata\n        from tflite_support import metadata_schema_py_generated as _metadata_fb\n\n        tmp_file = Path(\"/tmp/meta.txt\")\n        with open(tmp_file, \"w\") as meta_f:\n            meta_f.write(str(metadata))\n\n        model_meta = _metadata_fb.ModelMetadataT()\n        label_file = _metadata_fb.AssociatedFileT()\n        label_file.name = tmp_file.name\n        model_meta.associatedFiles = [label_file]\n\n        subgraph = _metadata_fb.SubGraphMetadataT()\n        subgraph.inputTensorMetadata = [_metadata_fb.TensorMetadataT()]\n        subgraph.outputTensorMetadata = [_metadata_fb.TensorMetadataT()] * num_outputs\n        model_meta.subgraphMetadata = [subgraph]\n\n        b = flatbuffers.Builder(0)\n        b.Finish(model_meta.Pack(b), _metadata.MetadataPopulator.METADATA_FILE_IDENTIFIER)\n        metadata_buf = b.Output()\n\n        populator = _metadata.MetadataPopulator.with_model_file(file)\n        populator.load_metadata_buffer(metadata_buf)\n        populator.load_associated_files([str(tmp_file)])\n        populator.populate()\n        tmp_file.unlink()\n\n\ndef pipeline_coreml(model, im, file, names, y, mlmodel, prefix=colorstr(\"CoreML Pipeline:\")):\n    \"\"\"\n    Convert a PyTorch YOLOv5 model to CoreML format with Non-Maximum Suppression (NMS), handling different input/output\n    shapes, and saving the model.\n\n    Args:\n        model (torch.nn.Module): The YOLOv5 PyTorch model to be converted.\n        im (torch.Tensor): Example input tensor with shape (N, C, H, W), where N is the batch size, C is the number of channels,\n            H is the height, and W is the width.\n        file (Path): Path to save the converted CoreML model.\n        names (dict[int, str]): Dictionary mapping class indices to class names.\n        y (torch.Tensor): Output tensor from the PyTorch model's forward pass.\n        mlmodel (bool): Flag indicating whether to export as older *.mlmodel format (default is False).\n        prefix (str): Custom prefix for logging messages.\n\n    Returns:\n        (Path): Path to the saved CoreML model (.mlmodel).\n\n    Raises:\n        AssertionError: If the number of class names does not match the number of classes in the model.\n\n    Notes:\n        - This function requires `coremltools` to be installed.\n        - Running this function on a non-macOS environment might not support some features.\n        - Flexible input shapes and additional NMS options can be customized within the function.\n\n    Examples:\n        ```python\n        from pathlib import Path\n        import torch\n\n        model = torch.load('yolov5s.pt')  # Load YOLOv5 model\n        im = torch.zeros((1, 3, 640, 640))  # Example input tensor\n\n        names = {0: \"person\", 1: \"bicycle\", 2: \"car\", ...}  # Define class names\n\n        y = model(im)  # Perform forward pass to get model output\n\n        output_file = Path('yolov5s.mlmodel')  # Convert to CoreML\n        pipeline_coreml(model, im, output_file, names, y)\n        ```\n    \"\"\"\n    import coremltools as ct\n    from PIL import Image\n\n    f = file.with_suffix(\".mlmodel\") if mlmodel else file.with_suffix(\".mlpackage\")\n    print(f\"{prefix} starting pipeline with coremltools {ct.__version__}...\")\n    batch_size, ch, h, w = list(im.shape)  # BCHW\n    t = time.time()\n\n    # YOLOv5 Output shapes\n    spec = model.get_spec()\n    out0, out1 = iter(spec.description.output)\n    if platform.system() == \"Darwin\":\n        img = Image.new(\"RGB\", (w, h))  # img(192 width, 320 height)\n        # img = torch.zeros((*opt.img_size, 3)).numpy()  # img size(320,192,3) iDetection\n        out = model.predict({\"image\": img})\n        out0_shape, out1_shape = out[out0.name].shape, out[out1.name].shape\n    else:  # linux and windows can not run model.predict(), get sizes from pytorch output y\n        s = tuple(y[0].shape)\n        out0_shape, out1_shape = (s[1], s[2] - 5), (s[1], 4)  # (3780, 80), (3780, 4)\n\n    # Checks\n    nx, ny = spec.description.input[0].type.imageType.width, spec.description.input[0].type.imageType.height\n    na, nc = out0_shape\n    # na, nc = out0.type.multiArrayType.shape  # number anchors, classes\n    assert len(names) == nc, f\"{len(names)} names found for nc={nc}\"  # check\n\n    # Define output shapes (missing)\n    out0.type.multiArrayType.shape[:] = out0_shape  # (3780, 80)\n    out1.type.multiArrayType.shape[:] = out1_shape  # (3780, 4)\n    # spec.neuralNetwork.preprocessing[0].featureName = '0'\n\n    # Flexible input shapes\n    # from coremltools.models.neural_network import flexible_shape_utils\n    # s = [] # shapes\n    # s.append(flexible_shape_utils.NeuralNetworkImageSize(320, 192))\n    # s.append(flexible_shape_utils.NeuralNetworkImageSize(640, 384))  # (height, width)\n    # flexible_shape_utils.add_enumerated_image_sizes(spec, feature_name='image', sizes=s)\n    # r = flexible_shape_utils.NeuralNetworkImageSizeRange()  # shape ranges\n    # r.add_height_range((192, 640))\n    # r.add_width_range((192, 640))\n    # flexible_shape_utils.update_image_size_range(spec, feature_name='image', size_range=r)\n\n    # Print\n    print(spec.description)\n\n    # Model from spec\n    weights_dir = None\n    weights_dir = None if mlmodel else str(f / \"Data/com.apple.CoreML/weights\")\n    model = ct.models.MLModel(spec, weights_dir=weights_dir)\n\n    # 3. Create NMS protobuf\n    nms_spec = ct.proto.Model_pb2.Model()\n    nms_spec.specificationVersion = 5\n    for i in range(2):\n        decoder_output = model._spec.description.output[i].SerializeToString()\n        nms_spec.description.input.add()\n        nms_spec.description.input[i].ParseFromString(decoder_output)\n        nms_spec.description.output.add()\n        nms_spec.description.output[i].ParseFromString(decoder_output)\n\n    nms_spec.description.output[0].name = \"confidence\"\n    nms_spec.description.output[1].name = \"coordinates\"\n\n    output_sizes = [nc, 4]\n    for i in range(2):\n        ma_type = nms_spec.description.output[i].type.multiArrayType\n        ma_type.shapeRange.sizeRanges.add()\n        ma_type.shapeRange.sizeRanges[0].lowerBound = 0\n        ma_type.shapeRange.sizeRanges[0].upperBound = -1\n        ma_type.shapeRange.sizeRanges.add()\n        ma_type.shapeRange.sizeRanges[1].lowerBound = output_sizes[i]\n        ma_type.shapeRange.sizeRanges[1].upperBound = output_sizes[i]\n        del ma_type.shape[:]\n\n    nms = nms_spec.nonMaximumSuppression\n    nms.confidenceInputFeatureName = out0.name  # 1x507x80\n    nms.coordinatesInputFeatureName = out1.name  # 1x507x4\n    nms.confidenceOutputFeatureName = \"confidence\"\n    nms.coordinatesOutputFeatureName = \"coordinates\"\n    nms.iouThresholdInputFeatureName = \"iouThreshold\"\n    nms.confidenceThresholdInputFeatureName = \"confidenceThreshold\"\n    nms.iouThreshold = 0.45\n    nms.confidenceThreshold = 0.25\n    nms.pickTop.perClass = True\n    nms.stringClassLabels.vector.extend(names.values())\n    nms_model = ct.models.MLModel(nms_spec)\n\n    # 4. Pipeline models together\n    pipeline = ct.models.pipeline.Pipeline(\n        input_features=[\n            (\"image\", ct.models.datatypes.Array(3, ny, nx)),\n            (\"iouThreshold\", ct.models.datatypes.Double()),\n            (\"confidenceThreshold\", ct.models.datatypes.Double()),\n        ],\n        output_features=[\"confidence\", \"coordinates\"],\n    )\n    pipeline.add_model(model)\n    pipeline.add_model(nms_model)\n\n    # Correct datatypes\n    pipeline.spec.description.input[0].ParseFromString(model._spec.description.input[0].SerializeToString())\n    pipeline.spec.description.output[0].ParseFromString(nms_model._spec.description.output[0].SerializeToString())\n    pipeline.spec.description.output[1].ParseFromString(nms_model._spec.description.output[1].SerializeToString())\n\n    # Update metadata\n    pipeline.spec.specificationVersion = 5\n    pipeline.spec.description.metadata.versionString = \"https://github.com/ultralytics/yolov5\"\n    pipeline.spec.description.metadata.shortDescription = \"https://github.com/ultralytics/yolov5\"\n    pipeline.spec.description.metadata.author = \"glenn.jocher@ultralytics.com\"\n    pipeline.spec.description.metadata.license = \"https://github.com/ultralytics/yolov5/blob/master/LICENSE\"\n    pipeline.spec.description.metadata.userDefined.update(\n        {\n            \"classes\": \",\".join(names.values()),\n            \"iou_threshold\": str(nms.iouThreshold),\n            \"confidence_threshold\": str(nms.confidenceThreshold),\n        }\n    )\n\n    # Save the model\n    model = ct.models.MLModel(pipeline.spec, weights_dir=weights_dir)\n    model.input_description[\"image\"] = \"Input image\"\n    model.input_description[\"iouThreshold\"] = f\"(optional) IOU Threshold override (default: {nms.iouThreshold})\"\n    model.input_description[\"confidenceThreshold\"] = (\n        f\"(optional) Confidence Threshold override (default: {nms.confidenceThreshold})\"\n    )\n    model.output_description[\"confidence\"] = 'Boxes × Class confidence (see user-defined metadata \"classes\")'\n    model.output_description[\"coordinates\"] = \"Boxes × [x, y, width, height] (relative to image size)\"\n    model.save(f)  # pipelined\n    print(f\"{prefix} pipeline success ({time.time() - t:.2f}s), saved as {f} ({file_size(f):.1f} MB)\")\n\n\n@smart_inference_mode()\ndef run(\n    data=ROOT / \"data/coco128.yaml\",  # 'dataset.yaml path'\n    weights=ROOT / \"yolov5s.pt\",  # weights path\n    imgsz=(640, 640),  # image (height, width)\n    batch_size=1,  # batch size\n    device=\"cpu\",  # cuda device, i.e. 0 or 0,1,2,3 or cpu\n    include=(\"torchscript\", \"onnx\"),  # include formats\n    half=False,  # FP16 half-precision export\n    inplace=False,  # set YOLOv5 Detect() inplace=True\n    keras=False,  # use Keras\n    optimize=False,  # TorchScript: optimize for mobile\n    int8=False,  # CoreML/TF INT8 quantization\n    per_tensor=False,  # TF per tensor quantization\n    dynamic=False,  # ONNX/TF/TensorRT: dynamic axes\n    cache=\"\",  # TensorRT: timing cache path\n    simplify=False,  # ONNX: simplify model\n    mlmodel=False,  # CoreML: Export in *.mlmodel format\n    opset=12,  # ONNX: opset version\n    verbose=False,  # TensorRT: verbose log\n    workspace=4,  # TensorRT: workspace size (GB)\n    nms=False,  # TF: add NMS to model\n    agnostic_nms=False,  # TF: add agnostic NMS to model\n    topk_per_class=100,  # TF.js NMS: topk per class to keep\n    topk_all=100,  # TF.js NMS: topk for all classes to keep\n    iou_thres=0.45,  # TF.js NMS: IoU threshold\n    conf_thres=0.25,  # TF.js NMS: confidence threshold\n):\n    \"\"\"\n    Exports a YOLOv5 model to specified formats including ONNX, TensorRT, CoreML, and TensorFlow.\n\n    Args:\n        data (str | Path): Path to the dataset YAML configuration file. Default is 'data/coco128.yaml'.\n        weights (str | Path): Path to the pretrained model weights file. Default is 'yolov5s.pt'.\n        imgsz (tuple): Image size as (height, width). Default is (640, 640).\n        batch_size (int): Batch size for exporting the model. Default is 1.\n        device (str): Device to run the export on, e.g., '0' for GPU, 'cpu' for CPU. Default is 'cpu'.\n        include (tuple): Formats to include in the export. Default is ('torchscript', 'onnx').\n        half (bool): Flag to export model with FP16 half-precision. Default is False.\n        inplace (bool): Set the YOLOv5 Detect() module inplace=True. Default is False.\n        keras (bool): Flag to use Keras for TensorFlow SavedModel export. Default is False.\n        optimize (bool): Optimize TorchScript model for mobile deployment. Default is False.\n        int8 (bool): Apply INT8 quantization for CoreML or TensorFlow models. Default is False.\n        per_tensor (bool): Apply per tensor quantization for TensorFlow models. Default is False.\n        dynamic (bool): Enable dynamic axes for ONNX, TensorFlow, or TensorRT exports. Default is False.\n        cache (str): TensorRT timing cache path. Default is an empty string.\n        simplify (bool): Simplify the ONNX model during export. Default is False.\n        opset (int): ONNX opset version. Default is 12.\n        verbose (bool): Enable verbose logging for TensorRT export. Default is False.\n        workspace (int): TensorRT workspace size in GB. Default is 4.\n        nms (bool): Add non-maximum suppression (NMS) to the TensorFlow model. Default is False.\n        agnostic_nms (bool): Add class-agnostic NMS to the TensorFlow model. Default is False.\n        topk_per_class (int): Top-K boxes per class to keep for TensorFlow.js NMS. Default is 100.\n        topk_all (int): Top-K boxes for all classes to keep for TensorFlow.js NMS. Default is 100.\n        iou_thres (float): IoU threshold for NMS. Default is 0.45.\n        conf_thres (float): Confidence threshold for NMS. Default is 0.25.\n        mlmodel (bool): Flag to use *.mlmodel for CoreML export. Default is False.\n\n    Returns:\n        None\n\n    Notes:\n        - Model export is based on the specified formats in the 'include' argument.\n        - Be cautious of combinations where certain flags are mutually exclusive, such as `--half` and `--dynamic`.\n\n    Example:\n        ```python\n        run(\n            data=\"data/coco128.yaml\",\n            weights=\"yolov5s.pt\",\n            imgsz=(640, 640),\n            batch_size=1,\n            device=\"cpu\",\n            include=(\"torchscript\", \"onnx\"),\n            half=False,\n            inplace=False,\n            keras=False,\n            optimize=False,\n            int8=False,\n            per_tensor=False,\n            dynamic=False,\n            cache=\"\",\n            simplify=False,\n            opset=12,\n            verbose=False,\n            mlmodel=False,\n            workspace=4,\n            nms=False,\n            agnostic_nms=False,\n            topk_per_class=100,\n            topk_all=100,\n            iou_thres=0.45,\n            conf_thres=0.25,\n        )\n        ```\n    \"\"\"\n    t = time.time()\n    include = [x.lower() for x in include]  # to lowercase\n    fmts = tuple(export_formats()[\"Argument\"][1:])  # --include arguments\n    flags = [x in include for x in fmts]\n    assert sum(flags) == len(include), f\"ERROR: Invalid --include {include}, valid --include arguments are {fmts}\"\n    jit, onnx, xml, engine, coreml, saved_model, pb, tflite, edgetpu, tfjs, paddle = flags  # export booleans\n    file = Path(url2file(weights) if str(weights).startswith((\"http:/\", \"https:/\")) else weights)  # PyTorch weights\n\n    # Load PyTorch model\n    device = select_device(device)\n    if half:\n        assert device.type != \"cpu\" or coreml, \"--half only compatible with GPU export, i.e. use --device 0\"\n        assert not dynamic, \"--half not compatible with --dynamic, i.e. use either --half or --dynamic but not both\"\n    model = attempt_load(weights, device=device, inplace=True, fuse=True)  # load FP32 model\n\n    # Checks\n    imgsz *= 2 if len(imgsz) == 1 else 1  # expand\n    if optimize:\n        assert device.type == \"cpu\", \"--optimize not compatible with cuda devices, i.e. use --device cpu\"\n\n    # Input\n    gs = int(max(model.stride))  # grid size (max stride)\n    imgsz = [check_img_size(x, gs) for x in imgsz]  # verify img_size are gs-multiples\n    ch = next(model.parameters()).size(1)  # require input image channels\n    im = torch.zeros(batch_size, ch, *imgsz).to(device)  # image size(1,3,320,192) BCHW iDetection\n\n    # Update model\n    model.eval()\n    for k, m in model.named_modules():\n        if isinstance(m, Detect):\n            m.inplace = inplace\n            m.dynamic = dynamic\n            m.export = True\n\n    for _ in range(2):\n        y = model(im)  # dry runs\n    if half and not coreml:\n        im, model = im.half(), model.half()  # to FP16\n    shape = tuple((y[0] if isinstance(y, tuple) else y).shape)  # model output shape\n    metadata = {\"stride\": int(max(model.stride)), \"names\": model.names}  # model metadata\n    LOGGER.info(f\"\\n{colorstr('PyTorch:')} starting from {file} with output shape {shape} ({file_size(file):.1f} MB)\")\n\n    # Exports\n    f = [\"\"] * len(fmts)  # exported filenames\n    warnings.filterwarnings(action=\"ignore\", category=torch.jit.TracerWarning)  # suppress TracerWarning\n    if jit:  # TorchScript\n        f[0], _ = export_torchscript(model, im, file, optimize)\n    if engine:  # TensorRT required before ONNX\n        f[1], _ = export_engine(model, im, file, half, dynamic, simplify, workspace, verbose, cache)\n    if onnx or xml:  # OpenVINO requires ONNX\n        f[2], _ = export_onnx(model, im, file, opset, dynamic, simplify)\n    if xml:  # OpenVINO\n        f[3], _ = export_openvino(file, metadata, half, int8, data)\n    if coreml:  # CoreML\n        f[4], ct_model = export_coreml(model, im, file, int8, half, nms, mlmodel)\n        if nms:\n            pipeline_coreml(ct_model, im, file, model.names, y, mlmodel)\n    if any((saved_model, pb, tflite, edgetpu, tfjs)):  # TensorFlow formats\n        assert not tflite or not tfjs, \"TFLite and TF.js models must be exported separately, please pass only one type.\"\n        assert not isinstance(model, ClassificationModel), \"ClassificationModel export to TF formats not yet supported.\"\n        f[5], s_model = export_saved_model(\n            model.cpu(),\n            im,\n            file,\n            dynamic,\n            tf_nms=nms or agnostic_nms or tfjs,\n            agnostic_nms=agnostic_nms or tfjs,\n            topk_per_class=topk_per_class,\n            topk_all=topk_all,\n            iou_thres=iou_thres,\n            conf_thres=conf_thres,\n            keras=keras,\n        )\n        if pb or tfjs:  # pb prerequisite to tfjs\n            f[6], _ = export_pb(s_model, file)\n        if tflite or edgetpu:\n            f[7], _ = export_tflite(\n                s_model, im, file, int8 or edgetpu, per_tensor, data=data, nms=nms, agnostic_nms=agnostic_nms\n            )\n            if edgetpu:\n                f[8], _ = export_edgetpu(file)\n            add_tflite_metadata(f[8] or f[7], metadata, num_outputs=len(s_model.outputs))\n        if tfjs:\n            f[9], _ = export_tfjs(file, int8)\n    if paddle:  # PaddlePaddle\n        f[10], _ = export_paddle(model, im, file, metadata)\n\n    # Finish\n    f = [str(x) for x in f if x]  # filter out '' and None\n    if any(f):\n        cls, det, seg = (isinstance(model, x) for x in (ClassificationModel, DetectionModel, SegmentationModel))  # type\n        det &= not seg  # segmentation models inherit from SegmentationModel(DetectionModel)\n        dir = Path(\"segment\" if seg else \"classify\" if cls else \"\")\n        h = \"--half\" if half else \"\"  # --half FP16 inference arg\n        s = (\n            \"# WARNING ⚠️ ClassificationModel not yet supported for PyTorch Hub AutoShape inference\"\n            if cls\n            else \"# WARNING ⚠️ SegmentationModel not yet supported for PyTorch Hub AutoShape inference\"\n            if seg\n            else \"\"\n        )\n        LOGGER.info(\n            f\"\\nExport complete ({time.time() - t:.1f}s)\"\n            f\"\\nResults saved to {colorstr('bold', file.parent.resolve())}\"\n            f\"\\nDetect:          python {dir / ('detect.py' if det else 'predict.py')} --weights {f[-1]} {h}\"\n            f\"\\nValidate:        python {dir / 'val.py'} --weights {f[-1]} {h}\"\n            f\"\\nPyTorch Hub:     model = torch.hub.load('ultralytics/yolov5', 'custom', '{f[-1]}')  {s}\"\n            f\"\\nVisualize:       https://netron.app\"\n        )\n    return f  # return list of exported files/dirs\n\n\ndef parse_opt(known=False):\n    \"\"\"\n    Parse command-line options for YOLOv5 model export configurations.\n\n    Args:\n        known (bool): If True, uses `argparse.ArgumentParser.parse_known_args`; otherwise, uses `argparse.ArgumentParser.parse_args`.\n                      Default is False.\n\n    Returns:\n        argparse.Namespace: Object containing parsed command-line arguments.\n\n    Example:\n        ```python\n        opts = parse_opt()\n        print(opts.data)\n        print(opts.weights)\n        ```\n    \"\"\"\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--data\", type=str, default=ROOT / \"data/coco128.yaml\", help=\"dataset.yaml path\")\n    parser.add_argument(\"--weights\", nargs=\"+\", type=str, default=ROOT / \"yolov5s.pt\", help=\"model.pt path(s)\")\n    parser.add_argument(\"--imgsz\", \"--img\", \"--img-size\", nargs=\"+\", type=int, default=[640, 640], help=\"image (h, w)\")\n    parser.add_argument(\"--batch-size\", type=int, default=1, help=\"batch size\")\n    parser.add_argument(\"--device\", default=\"cpu\", help=\"cuda device, i.e. 0 or 0,1,2,3 or cpu\")\n    parser.add_argument(\"--half\", action=\"store_true\", help=\"FP16 half-precision export\")\n    parser.add_argument(\"--inplace\", action=\"store_true\", help=\"set YOLOv5 Detect() inplace=True\")\n    parser.add_argument(\"--keras\", action=\"store_true\", help=\"TF: use Keras\")\n    parser.add_argument(\"--optimize\", action=\"store_true\", help=\"TorchScript: optimize for mobile\")\n    parser.add_argument(\"--int8\", action=\"store_true\", help=\"CoreML/TF/OpenVINO INT8 quantization\")\n    parser.add_argument(\"--per-tensor\", action=\"store_true\", help=\"TF per-tensor quantization\")\n    parser.add_argument(\"--dynamic\", action=\"store_true\", help=\"ONNX/TF/TensorRT: dynamic axes\")\n    parser.add_argument(\"--cache\", type=str, default=\"\", help=\"TensorRT: timing cache file path\")\n    parser.add_argument(\"--simplify\", action=\"store_true\", help=\"ONNX: simplify model\")\n    parser.add_argument(\"--mlmodel\", action=\"store_true\", help=\"CoreML: Export in *.mlmodel format\")\n    parser.add_argument(\"--opset\", type=int, default=17, help=\"ONNX: opset version\")\n    parser.add_argument(\"--verbose\", action=\"store_true\", help=\"TensorRT: verbose log\")\n    parser.add_argument(\"--workspace\", type=int, default=4, help=\"TensorRT: workspace size (GB)\")\n    parser.add_argument(\"--nms\", action=\"store_true\", help=\"TF: add NMS to model\")\n    parser.add_argument(\"--agnostic-nms\", action=\"store_true\", help=\"TF: add agnostic NMS to model\")\n    parser.add_argument(\"--topk-per-class\", type=int, default=100, help=\"TF.js NMS: topk per class to keep\")\n    parser.add_argument(\"--topk-all\", type=int, default=100, help=\"TF.js NMS: topk for all classes to keep\")\n    parser.add_argument(\"--iou-thres\", type=float, default=0.45, help=\"TF.js NMS: IoU threshold\")\n    parser.add_argument(\"--conf-thres\", type=float, default=0.25, help=\"TF.js NMS: confidence threshold\")\n    parser.add_argument(\n        \"--include\",\n        nargs=\"+\",\n        default=[\"torchscript\"],\n        help=\"torchscript, onnx, openvino, engine, coreml, saved_model, pb, tflite, edgetpu, tfjs, paddle\",\n    )\n    opt = parser.parse_known_args()[0] if known else parser.parse_args()\n    print_args(vars(opt))\n    return opt\n\n\ndef main(opt):\n    \"\"\"Run(**vars(opt))  # Execute the run function with parsed options.\"\"\"\n    for opt.weights in opt.weights if isinstance(opt.weights, list) else [opt.weights]:\n        run(**vars(opt))\n\n\nif __name__ == \"__main__\":\n    opt = parse_opt()\n    main(opt)\n",
    "hubconf.py": "# Ultralytics 🚀 AGPL-3.0 License - https://ultralytics.com/license\n\"\"\"\nPyTorch Hub models https://pytorch.org/hub/ultralytics_yolov5.\n\nUsage:\n    import torch\n    model = torch.hub.load('ultralytics/yolov5', 'yolov5s')  # official model\n    model = torch.hub.load('ultralytics/yolov5:master', 'yolov5s')  # from branch\n    model = torch.hub.load('ultralytics/yolov5', 'custom', 'yolov5s.pt')  # custom/local model\n    model = torch.hub.load('.', 'custom', 'yolov5s.pt', source='local')  # local repo\n\"\"\"\n\nimport torch\n\n\ndef _create(name, pretrained=True, channels=3, classes=80, autoshape=True, verbose=True, device=None):\n    \"\"\"\n    Creates or loads a YOLOv5 model, with options for pretrained weights and model customization.\n\n    Args:\n        name (str): Model name (e.g., 'yolov5s') or path to the model checkpoint (e.g., 'path/to/best.pt').\n        pretrained (bool, optional): If True, loads pretrained weights into the model. Defaults to True.\n        channels (int, optional): Number of input channels the model expects. Defaults to 3.\n        classes (int, optional): Number of classes the model is expected to detect. Defaults to 80.\n        autoshape (bool, optional): If True, applies the YOLOv5 .autoshape() wrapper for various input formats. Defaults to True.\n        verbose (bool, optional): If True, prints detailed information during the model creation/loading process. Defaults to True.\n        device (str | torch.device | None, optional): Device to use for model parameters (e.g., 'cpu', 'cuda'). If None, selects\n            the best available device. Defaults to None.\n\n    Returns:\n        (DetectMultiBackend | AutoShape): The loaded YOLOv5 model, potentially wrapped with AutoShape if specified.\n\n    Examples:\n        ```python\n        import torch\n        from ultralytics import _create\n\n        # Load an official YOLOv5s model with pretrained weights\n        model = _create('yolov5s')\n\n        # Load a custom model from a local checkpoint\n        model = _create('path/to/custom_model.pt', pretrained=False)\n\n        # Load a model with specific input channels and classes\n        model = _create('yolov5s', channels=1, classes=10)\n        ```\n\n    Notes:\n        For more information on model loading and customization, visit the\n        [YOLOv5 PyTorch Hub Documentation](https://docs.ultralytics.com/yolov5/tutorials/pytorch_hub_model_loading).\n    \"\"\"\n    from pathlib import Path\n\n    from models.common import AutoShape, DetectMultiBackend\n    from models.experimental import attempt_load\n    from models.yolo import ClassificationModel, DetectionModel, SegmentationModel\n    from utils.downloads import attempt_download\n    from utils.general import LOGGER, ROOT, check_requirements, intersect_dicts, logging\n    from utils.torch_utils import select_device\n\n    if not verbose:\n        LOGGER.setLevel(logging.WARNING)\n    check_requirements(ROOT / \"requirements.txt\", exclude=(\"opencv-python\", \"tensorboard\", \"thop\"))\n    name = Path(name)\n    path = name.with_suffix(\".pt\") if name.suffix == \"\" and not name.is_dir() else name  # checkpoint path\n    try:\n        device = select_device(device)\n        if pretrained and channels == 3 and classes == 80:\n            try:\n                model = DetectMultiBackend(path, device=device, fuse=autoshape)  # detection model\n                if autoshape:\n                    if model.pt and isinstance(model.model, ClassificationModel):\n                        LOGGER.warning(\n                            \"WARNING ⚠️ YOLOv5 ClassificationModel is not yet AutoShape compatible. \"\n                            \"You must pass torch tensors in BCHW to this model, i.e. shape(1,3,224,224).\"\n                        )\n                    elif model.pt and isinstance(model.model, SegmentationModel):\n                        LOGGER.warning(\n                            \"WARNING ⚠️ YOLOv5 SegmentationModel is not yet AutoShape compatible. \"\n                            \"You will not be able to run inference with this model.\"\n                        )\n                    else:\n                        model = AutoShape(model)  # for file/URI/PIL/cv2/np inputs and NMS\n            except Exception:\n                model = attempt_load(path, device=device, fuse=False)  # arbitrary model\n        else:\n            cfg = list((Path(__file__).parent / \"models\").rglob(f\"{path.stem}.yaml\"))[0]  # model.yaml path\n            model = DetectionModel(cfg, channels, classes)  # create model\n            if pretrained:\n                ckpt = torch.load(attempt_download(path), map_location=device)  # load\n                csd = ckpt[\"model\"].float().state_dict()  # checkpoint state_dict as FP32\n                csd = intersect_dicts(csd, model.state_dict(), exclude=[\"anchors\"])  # intersect\n                model.load_state_dict(csd, strict=False)  # load\n                if len(ckpt[\"model\"].names) == classes:\n                    model.names = ckpt[\"model\"].names  # set class names attribute\n        if not verbose:\n            LOGGER.setLevel(logging.INFO)  # reset to default\n        return model.to(device)\n\n    except Exception as e:\n        help_url = \"https://docs.ultralytics.com/yolov5/tutorials/pytorch_hub_model_loading\"\n        s = f\"{e}. Cache may be out of date, try `force_reload=True` or see {help_url} for help.\"\n        raise Exception(s) from e\n\n\ndef custom(path=\"path/to/model.pt\", autoshape=True, _verbose=True, device=None):\n    \"\"\"\n    Loads a custom or local YOLOv5 model from a given path with optional autoshaping and device specification.\n\n    Args:\n        path (str): Path to the custom model file (e.g., 'path/to/model.pt').\n        autoshape (bool): Apply YOLOv5 .autoshape() wrapper to model if True, enabling compatibility with various input\n            types (default is True).\n        _verbose (bool): If True, prints all informational messages to the screen; otherwise, operates silently\n            (default is True).\n        device (str | torch.device | None): Device to load the model on, e.g., 'cpu', 'cuda', torch.device('cuda:0'), etc.\n            (default is None, which automatically selects the best available device).\n\n    Returns:\n        torch.nn.Module: A YOLOv5 model loaded with the specified parameters.\n\n    Notes:\n        For more details on loading models from PyTorch Hub:\n        https://docs.ultralytics.com/yolov5/tutorials/pytorch_hub_model_loading\n\n    Examples:\n        ```python\n        # Load model from a given path with autoshape enabled on the best available device\n        model = torch.hub.load('ultralytics/yolov5', 'custom', 'yolov5s.pt')\n\n        # Load model from a local path without autoshape on the CPU device\n        model = torch.hub.load('.', 'custom', 'yolov5s.pt', source='local', autoshape=False, device='cpu')\n        ```\n    \"\"\"\n    return _create(path, autoshape=autoshape, verbose=_verbose, device=device)\n\n\ndef yolov5n(pretrained=True, channels=3, classes=80, autoshape=True, _verbose=True, device=None):\n    \"\"\"\n    Instantiates the YOLOv5-nano model with options for pretraining, input channels, class count, autoshaping,\n    verbosity, and device.\n\n    Args:\n        pretrained (bool): If True, loads pretrained weights into the model. Defaults to True.\n        channels (int): Number of input channels for the model. Defaults to 3.\n        classes (int): Number of classes for object detection. Defaults to 80.\n        autoshape (bool): If True, applies the YOLOv5 .autoshape() wrapper to the model for various formats (file/URI/PIL/\n            cv2/np) and non-maximum suppression (NMS) during inference. Defaults to True.\n        _verbose (bool): If True, prints detailed information to the screen. Defaults to True.\n        device (str | torch.device | None): Specifies the device to use for model computation. If None, uses the best device\n            available (i.e., GPU if available, otherwise CPU). Defaults to None.\n\n    Returns:\n        DetectionModel | ClassificationModel | SegmentationModel: The instantiated YOLOv5-nano model, potentially with\n            pretrained weights and autoshaping applied.\n\n    Notes:\n        For further details on loading models from PyTorch Hub, refer to [PyTorch Hub models](https://pytorch.org/hub/\n        ultralytics_yolov5).\n\n    Examples:\n        ```python\n        import torch\n        from ultralytics import yolov5n\n\n        # Load the YOLOv5-nano model with defaults\n        model = yolov5n()\n\n        # Load the YOLOv5-nano model with a specific device\n        model = yolov5n(device='cuda')\n        ```\n    \"\"\"\n    return _create(\"yolov5n\", pretrained, channels, classes, autoshape, _verbose, device)\n\n\ndef yolov5s(pretrained=True, channels=3, classes=80, autoshape=True, _verbose=True, device=None):\n    \"\"\"\n    Create a YOLOv5-small (yolov5s) model with options for pretraining, input channels, class count, autoshaping,\n    verbosity, and device configuration.\n\n    Args:\n        pretrained (bool, optional): Flag to load pretrained weights into the model. Defaults to True.\n        channels (int, optional): Number of input channels. Defaults to 3.\n        classes (int, optional): Number of model classes. Defaults to 80.\n        autoshape (bool, optional): Whether to wrap the model with YOLOv5's .autoshape() for handling various input formats.\n            Defaults to True.\n        _verbose (bool, optional): Flag to print detailed information regarding model loading. Defaults to True.\n        device (str | torch.device | None, optional): Device to use for model computation, can be 'cpu', 'cuda', or\n            torch.device instances. If None, automatically selects the best available device. Defaults to None.\n\n    Returns:\n        torch.nn.Module: The YOLOv5-small model configured and loaded according to the specified parameters.\n\n    Example:\n        ```python\n        import torch\n\n        # Load the official YOLOv5-small model with pretrained weights\n        model = torch.hub.load('ultralytics/yolov5', 'yolov5s')\n\n        # Load the YOLOv5-small model from a specific branch\n        model = torch.hub.load('ultralytics/yolov5:master', 'yolov5s')\n\n        # Load a custom YOLOv5-small model from a local checkpoint\n        model = torch.hub.load('ultralytics/yolov5', 'custom', 'yolov5s.pt')\n\n        # Load a local YOLOv5-small model specifying source as local repository\n        model = torch.hub.load('.', 'custom', 'yolov5s.pt', source='local')\n        ```\n\n    Notes:\n        For more details on model loading and customization, visit\n        the [YOLOv5 PyTorch Hub Documentation](https://pytorch.org/hub/ultralytics_yolov5).\n    \"\"\"\n    return _create(\"yolov5s\", pretrained, channels, classes, autoshape, _verbose, device)\n\n\ndef yolov5m(pretrained=True, channels=3, classes=80, autoshape=True, _verbose=True, device=None):\n    \"\"\"\n    Instantiates the YOLOv5-medium model with customizable pretraining, channel count, class count, autoshaping,\n    verbosity, and device.\n\n    Args:\n        pretrained (bool, optional): Whether to load pretrained weights into the model. Default is True.\n        channels (int, optional): Number of input channels. Default is 3.\n        classes (int, optional): Number of model classes. Default is 80.\n        autoshape (bool, optional): Apply YOLOv5 .autoshape() wrapper to the model for handling various input formats.\n            Default is True.\n        _verbose (bool, optional): Whether to print detailed information to the screen. Default is True.\n        device (str | torch.device | None, optional): Device specification to use for model parameters (e.g., 'cpu', 'cuda').\n            Default is None.\n\n    Returns:\n        torch.nn.Module: The instantiated YOLOv5-medium model.\n\n    Usage Example:\n        ```python\n        import torch\n\n        model = torch.hub.load('ultralytics/yolov5', 'yolov5m')  # Load YOLOv5-medium from Ultralytics repository\n        model = torch.hub.load('ultralytics/yolov5:master', 'yolov5m')  # Load from the master branch\n        model = torch.hub.load('ultralytics/yolov5', 'custom', 'yolov5m.pt')  # Load a custom/local YOLOv5-medium model\n        model = torch.hub.load('.', 'custom', 'yolov5m.pt', source='local')  # Load from a local repository\n        ```\n\n    For more information, visit https://pytorch.org/hub/ultralytics_yolov5.\n    \"\"\"\n    return _create(\"yolov5m\", pretrained, channels, classes, autoshape, _verbose, device)\n\n\ndef yolov5l(pretrained=True, channels=3, classes=80, autoshape=True, _verbose=True, device=None):\n    \"\"\"\n    Creates YOLOv5-large model with options for pretraining, channels, classes, autoshaping, verbosity, and device\n    selection.\n\n    Args:\n        pretrained (bool): Load pretrained weights into the model. Default is True.\n        channels (int): Number of input channels. Default is 3.\n        classes (int): Number of model classes. Default is 80.\n        autoshape (bool): Apply YOLOv5 .autoshape() wrapper to model. Default is True.\n        _verbose (bool): Print all information to screen. Default is True.\n        device (str | torch.device | None): Device to use for model parameters, e.g., 'cpu', 'cuda', or a torch.device instance.\n            Default is None.\n\n    Returns:\n        YOLOv5 model (torch.nn.Module): The YOLOv5-large model instantiated with specified configurations and possibly\n        pretrained weights.\n\n    Examples:\n        ```python\n        import torch\n        model = torch.hub.load('ultralytics/yolov5', 'yolov5l')\n        ```\n\n    Notes:\n        For additional details, refer to the PyTorch Hub models documentation:\n        https://pytorch.org/hub/ultralytics_yolov5\n    \"\"\"\n    return _create(\"yolov5l\", pretrained, channels, classes, autoshape, _verbose, device)\n\n\ndef yolov5x(pretrained=True, channels=3, classes=80, autoshape=True, _verbose=True, device=None):\n    \"\"\"\n    Perform object detection using the YOLOv5-xlarge model with options for pretraining, input channels, class count,\n    autoshaping, verbosity, and device specification.\n\n    Args:\n        pretrained (bool): If True, loads pretrained weights into the model. Defaults to True.\n        channels (int): Number of input channels for the model. Defaults to 3.\n        classes (int): Number of model classes for object detection. Defaults to 80.\n        autoshape (bool): If True, applies the YOLOv5 .autoshape() wrapper for handling different input formats. Defaults to\n            True.\n        _verbose (bool): If True, prints detailed information during model loading. Defaults to True.\n        device (str | torch.device | None): Device specification for computing the model, e.g., 'cpu', 'cuda:0', torch.device('cuda').\n            Defaults to None.\n\n    Returns:\n        torch.nn.Module: The YOLOv5-xlarge model loaded with the specified parameters, optionally with pretrained weights and\n        autoshaping applied.\n\n    Example:\n        ```python\n        import torch\n        model = torch.hub.load('ultralytics/yolov5', 'yolov5x')\n        ```\n\n    For additional details, refer to the official YOLOv5 PyTorch Hub models documentation:\n    https://pytorch.org/hub/ultralytics_yolov5\n    \"\"\"\n    return _create(\"yolov5x\", pretrained, channels, classes, autoshape, _verbose, device)\n\n\ndef yolov5n6(pretrained=True, channels=3, classes=80, autoshape=True, _verbose=True, device=None):\n    \"\"\"\n    Creates YOLOv5-nano-P6 model with options for pretraining, channels, classes, autoshaping, verbosity, and device.\n\n    Args:\n        pretrained (bool, optional): If True, loads pretrained weights into the model. Default is True.\n        channels (int, optional): Number of input channels. Default is 3.\n        classes (int, optional): Number of model classes. Default is 80.\n        autoshape (bool, optional): If True, applies the YOLOv5 .autoshape() wrapper to the model. Default is True.\n        _verbose (bool, optional): If True, prints all information to screen. Default is True.\n        device (str | torch.device | None, optional): Device to use for model parameters. Can be 'cpu', 'cuda', or None.\n            Default is None.\n\n    Returns:\n        torch.nn.Module: YOLOv5-nano-P6 model loaded with the specified configurations.\n\n    Example:\n        ```python\n        import torch\n        model = yolov5n6(pretrained=True, channels=3, classes=80, autoshape=True, _verbose=True, device='cuda')\n        ```\n\n    Notes:\n        For more information on PyTorch Hub models, visit: https://pytorch.org/hub/ultralytics_yolov5\n    \"\"\"\n    return _create(\"yolov5n6\", pretrained, channels, classes, autoshape, _verbose, device)\n\n\ndef yolov5s6(pretrained=True, channels=3, classes=80, autoshape=True, _verbose=True, device=None):\n    \"\"\"\n    Instantiate the YOLOv5-small-P6 model with options for pretraining, input channels, number of classes, autoshaping,\n    verbosity, and device selection.\n\n    Args:\n        pretrained (bool): If True, loads pretrained weights. Default is True.\n        channels (int): Number of input channels. Default is 3.\n        classes (int): Number of object detection classes. Default is 80.\n        autoshape (bool): If True, applies YOLOv5 .autoshape() wrapper to the model, allowing for varied input formats.\n            Default is True.\n        _verbose (bool): If True, prints detailed information during model loading. Default is True.\n        device (str | torch.device | None): Device specification for model parameters (e.g., 'cpu', 'cuda', or torch.device).\n            Default is None, which selects an available device automatically.\n\n    Returns:\n        torch.nn.Module: The YOLOv5-small-P6 model instance.\n\n    Usage:\n        ```python\n        import torch\n\n        model = torch.hub.load('ultralytics/yolov5', 'yolov5s6')\n        model = torch.hub.load('ultralytics/yolov5:master', 'yolov5s6')  # load from a specific branch\n        model = torch.hub.load('ultralytics/yolov5', 'custom', 'path/to/yolov5s6.pt')  # custom/local model\n        model = torch.hub.load('.', 'custom', 'path/to/yolov5s6.pt', source='local')  # local repo model\n        ```\n\n    Notes:\n        - For more information, refer to the PyTorch Hub models documentation at https://pytorch.org/hub/ultralytics_yolov5\n\n    Raises:\n        Exception: If there is an error during model creation or loading, with a suggestion to visit the YOLOv5\n            tutorials for help.\n    \"\"\"\n    return _create(\"yolov5s6\", pretrained, channels, classes, autoshape, _verbose, device)\n\n\ndef yolov5m6(pretrained=True, channels=3, classes=80, autoshape=True, _verbose=True, device=None):\n    \"\"\"\n    Create YOLOv5-medium-P6 model with options for pretraining, channel count, class count, autoshaping, verbosity, and\n    device.\n\n    Args:\n        pretrained (bool): If True, loads pretrained weights. Default is True.\n        channels (int): Number of input channels. Default is 3.\n        classes (int): Number of model classes. Default is 80.\n        autoshape (bool): Apply YOLOv5 .autoshape() wrapper to the model for file/URI/PIL/cv2/np inputs and NMS.\n            Default is True.\n        _verbose (bool): If True, prints detailed information to the screen. Default is True.\n        device (str | torch.device | None): Device to use for model parameters. Default is None, which uses the\n            best available device.\n\n    Returns:\n        torch.nn.Module: The YOLOv5-medium-P6 model.\n\n    Refer to the PyTorch Hub models documentation: https://pytorch.org/hub/ultralytics_yolov5 for additional details.\n\n    Example:\n        ```python\n        import torch\n\n        # Load YOLOv5-medium-P6 model\n        model = torch.hub.load('ultralytics/yolov5', 'yolov5m6')\n        ```\n\n    Notes:\n        - The model can be loaded with pre-trained weights for better performance on specific tasks.\n        - The autoshape feature simplifies input handling by allowing various popular data formats.\n    \"\"\"\n    return _create(\"yolov5m6\", pretrained, channels, classes, autoshape, _verbose, device)\n\n\ndef yolov5l6(pretrained=True, channels=3, classes=80, autoshape=True, _verbose=True, device=None):\n    \"\"\"\n    Instantiate the YOLOv5-large-P6 model with options for pretraining, channel and class counts, autoshaping,\n    verbosity, and device selection.\n\n    Args:\n        pretrained (bool, optional): If True, load pretrained weights into the model. Default is True.\n        channels (int, optional): Number of input channels. Default is 3.\n        classes (int, optional): Number of model classes. Default is 80.\n        autoshape (bool, optional): If True, apply YOLOv5 .autoshape() wrapper to the model for input flexibility. Default is True.\n        _verbose (bool, optional): If True, print all information to the screen. Default is True.\n        device (str | torch.device | None, optional): Device to use for model parameters, e.g., 'cpu', 'cuda', or torch.device.\n            If None, automatically selects the best available device. Default is None.\n\n    Returns:\n        torch.nn.Module: The instantiated YOLOv5-large-P6 model.\n\n    Example:\n        ```python\n        import torch\n        model = torch.hub.load('ultralytics/yolov5', 'yolov5l6')  # official model\n        model = torch.hub.load('ultralytics/yolov5:master', 'yolov5l6')  # from specific branch\n        model = torch.hub.load('ultralytics/yolov5', 'custom', 'path/to/yolov5l6.pt')  # custom/local model\n        model = torch.hub.load('.', 'custom', 'path/to/yolov5l6.pt', source='local')  # local repository\n        ```\n\n    Note:\n        Refer to [PyTorch Hub Documentation](https://pytorch.org/hub/ultralytics_yolov5) for additional usage instructions.\n    \"\"\"\n    return _create(\"yolov5l6\", pretrained, channels, classes, autoshape, _verbose, device)\n\n\ndef yolov5x6(pretrained=True, channels=3, classes=80, autoshape=True, _verbose=True, device=None):\n    \"\"\"\n    Creates the YOLOv5-xlarge-P6 model with options for pretraining, number of input channels, class count, autoshaping,\n    verbosity, and device selection.\n\n    Args:\n        pretrained (bool): If True, loads pretrained weights into the model. Default is True.\n        channels (int): Number of input channels. Default is 3.\n        classes (int): Number of model classes. Default is 80.\n        autoshape (bool): If True, applies YOLOv5 .autoshape() wrapper to the model. Default is True.\n        _verbose (bool): If True, prints all information to the screen. Default is True.\n        device (str | torch.device | None): Device to use for model parameters, can be a string, torch.device object, or\n            None for default device selection. Default is None.\n\n    Returns:\n        torch.nn.Module: The instantiated YOLOv5-xlarge-P6 model.\n\n    Example:\n        ```python\n        import torch\n        model = torch.hub.load('ultralytics/yolov5', 'yolov5x6')  # load the YOLOv5-xlarge-P6 model\n        ```\n\n    Note:\n        For more information on YOLOv5 models, visit the official documentation:\n        https://docs.ultralytics.com/yolov5\n    \"\"\"\n    return _create(\"yolov5x6\", pretrained, channels, classes, autoshape, _verbose, device)\n\n\nif __name__ == \"__main__\":\n    import argparse\n    from pathlib import Path\n\n    import numpy as np\n    from PIL import Image\n\n    from utils.general import cv2, print_args\n\n    # Argparser\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--model\", type=str, default=\"yolov5s\", help=\"model name\")\n    opt = parser.parse_args()\n    print_args(vars(opt))\n\n    # Model\n    model = _create(name=opt.model, pretrained=True, channels=3, classes=80, autoshape=True, verbose=True)\n    # model = custom(path='path/to/model.pt')  # custom\n\n    # Images\n    imgs = [\n        \"data/images/zidane.jpg\",  # filename\n        Path(\"data/images/zidane.jpg\"),  # Path\n        \"https://ultralytics.com/images/zidane.jpg\",  # URI\n        cv2.imread(\"data/images/bus.jpg\")[:, :, ::-1],  # OpenCV\n        Image.open(\"data/images/bus.jpg\"),  # PIL\n        np.zeros((320, 640, 3)),\n    ]  # numpy\n\n    # Inference\n    results = model(imgs, size=320)  # batched inference\n\n    # Results\n    results.print()\n    results.save()\n",
    "models/__init__.py": "# Ultralytics 🚀 AGPL-3.0 License - https://ultralytics.com/license\n",
    "models/common.py": "# Ultralytics 🚀 AGPL-3.0 License - https://ultralytics.com/license\n\"\"\"Common modules.\"\"\"\n\nimport ast\nimport contextlib\nimport json\nimport math\nimport platform\nimport warnings\nimport zipfile\nfrom collections import OrderedDict, namedtuple\nfrom copy import copy\nfrom pathlib import Path\nfrom urllib.parse import urlparse\n\nimport cv2\nimport numpy as np\nimport pandas as pd\nimport requests\nimport torch\nimport torch.nn as nn\nfrom PIL import Image\nfrom torch.cuda import amp\n\n# Import 'ultralytics' package or install if missing\ntry:\n    import ultralytics\n\n    assert hasattr(ultralytics, \"__version__\")  # verify package is not directory\nexcept (ImportError, AssertionError):\n    import os\n\n    os.system(\"pip install -U ultralytics\")\n    import ultralytics\n\nfrom ultralytics.utils.plotting import Annotator, colors, save_one_box\n\nfrom utils import TryExcept\nfrom utils.dataloaders import exif_transpose, letterbox\nfrom utils.general import (\n    LOGGER,\n    ROOT,\n    Profile,\n    check_requirements,\n    check_suffix,\n    check_version,\n    colorstr,\n    increment_path,\n    is_jupyter,\n    make_divisible,\n    non_max_suppression,\n    scale_boxes,\n    xywh2xyxy,\n    xyxy2xywh,\n    yaml_load,\n)\nfrom utils.torch_utils import copy_attr, smart_inference_mode\n\n\ndef autopad(k, p=None, d=1):\n    \"\"\"\n    Pads kernel to 'same' output shape, adjusting for optional dilation; returns padding size.\n\n    `k`: kernel, `p`: padding, `d`: dilation.\n    \"\"\"\n    if d > 1:\n        k = d * (k - 1) + 1 if isinstance(k, int) else [d * (x - 1) + 1 for x in k]  # actual kernel-size\n    if p is None:\n        p = k // 2 if isinstance(k, int) else [x // 2 for x in k]  # auto-pad\n    return p\n\n\nclass Conv(nn.Module):\n    \"\"\"Applies a convolution, batch normalization, and activation function to an input tensor in a neural network.\"\"\"\n\n    default_act = nn.SiLU()  # default activation\n\n    def __init__(self, c1, c2, k=1, s=1, p=None, g=1, d=1, act=True):\n        \"\"\"Initializes a standard convolution layer with optional batch normalization and activation.\"\"\"\n        super().__init__()\n        self.conv = nn.Conv2d(c1, c2, k, s, autopad(k, p, d), groups=g, dilation=d, bias=False)\n        self.bn = nn.BatchNorm2d(c2)\n        self.act = self.default_act if act is True else act if isinstance(act, nn.Module) else nn.Identity()\n\n    def forward(self, x):\n        \"\"\"Applies a convolution followed by batch normalization and an activation function to the input tensor `x`.\"\"\"\n        return self.act(self.bn(self.conv(x)))\n\n    def forward_fuse(self, x):\n        \"\"\"Applies a fused convolution and activation function to the input tensor `x`.\"\"\"\n        return self.act(self.conv(x))\n\n\nclass DWConv(Conv):\n    \"\"\"Implements a depth-wise convolution layer with optional activation for efficient spatial filtering.\"\"\"\n\n    def __init__(self, c1, c2, k=1, s=1, d=1, act=True):\n        \"\"\"Initializes a depth-wise convolution layer with optional activation; args: input channels (c1), output\n        channels (c2), kernel size (k), stride (s), dilation (d), and activation flag (act).\n        \"\"\"\n        super().__init__(c1, c2, k, s, g=math.gcd(c1, c2), d=d, act=act)\n\n\nclass DWConvTranspose2d(nn.ConvTranspose2d):\n    \"\"\"A depth-wise transpose convolutional layer for upsampling in neural networks, particularly in YOLOv5 models.\"\"\"\n\n    def __init__(self, c1, c2, k=1, s=1, p1=0, p2=0):\n        \"\"\"Initializes a depth-wise transpose convolutional layer for YOLOv5; args: input channels (c1), output channels\n        (c2), kernel size (k), stride (s), input padding (p1), output padding (p2).\n        \"\"\"\n        super().__init__(c1, c2, k, s, p1, p2, groups=math.gcd(c1, c2))\n\n\nclass TransformerLayer(nn.Module):\n    \"\"\"Transformer layer with multihead attention and linear layers, optimized by removing LayerNorm.\"\"\"\n\n    def __init__(self, c, num_heads):\n        \"\"\"\n        Initializes a transformer layer, sans LayerNorm for performance, with multihead attention and linear layers.\n\n        See  as described in https://arxiv.org/abs/2010.11929.\n        \"\"\"\n        super().__init__()\n        self.q = nn.Linear(c, c, bias=False)\n        self.k = nn.Linear(c, c, bias=False)\n        self.v = nn.Linear(c, c, bias=False)\n        self.ma = nn.MultiheadAttention(embed_dim=c, num_heads=num_heads)\n        self.fc1 = nn.Linear(c, c, bias=False)\n        self.fc2 = nn.Linear(c, c, bias=False)\n\n    def forward(self, x):\n        \"\"\"Performs forward pass using MultiheadAttention and two linear transformations with residual connections.\"\"\"\n        x = self.ma(self.q(x), self.k(x), self.v(x))[0] + x\n        x = self.fc2(self.fc1(x)) + x\n        return x\n\n\nclass TransformerBlock(nn.Module):\n    \"\"\"A Transformer block for vision tasks with convolution, position embeddings, and Transformer layers.\"\"\"\n\n    def __init__(self, c1, c2, num_heads, num_layers):\n        \"\"\"Initializes a Transformer block for vision tasks, adapting dimensions if necessary and stacking specified\n        layers.\n        \"\"\"\n        super().__init__()\n        self.conv = None\n        if c1 != c2:\n            self.conv = Conv(c1, c2)\n        self.linear = nn.Linear(c2, c2)  # learnable position embedding\n        self.tr = nn.Sequential(*(TransformerLayer(c2, num_heads) for _ in range(num_layers)))\n        self.c2 = c2\n\n    def forward(self, x):\n        \"\"\"Processes input through an optional convolution, followed by Transformer layers and position embeddings for\n        object detection.\n        \"\"\"\n        if self.conv is not None:\n            x = self.conv(x)\n        b, _, w, h = x.shape\n        p = x.flatten(2).permute(2, 0, 1)\n        return self.tr(p + self.linear(p)).permute(1, 2, 0).reshape(b, self.c2, w, h)\n\n\nclass Bottleneck(nn.Module):\n    \"\"\"A bottleneck layer with optional shortcut and group convolution for efficient feature extraction.\"\"\"\n\n    def __init__(self, c1, c2, shortcut=True, g=1, e=0.5):\n        \"\"\"Initializes a standard bottleneck layer with optional shortcut and group convolution, supporting channel\n        expansion.\n        \"\"\"\n        super().__init__()\n        c_ = int(c2 * e)  # hidden channels\n        self.cv1 = Conv(c1, c_, 1, 1)\n        self.cv2 = Conv(c_, c2, 3, 1, g=g)\n        self.add = shortcut and c1 == c2\n\n    def forward(self, x):\n        \"\"\"Processes input through two convolutions, optionally adds shortcut if channel dimensions match; input is a\n        tensor.\n        \"\"\"\n        return x + self.cv2(self.cv1(x)) if self.add else self.cv2(self.cv1(x))\n\n\nclass BottleneckCSP(nn.Module):\n    \"\"\"CSP bottleneck layer for feature extraction with cross-stage partial connections and optional shortcuts.\"\"\"\n\n    def __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5):\n        \"\"\"Initializes CSP bottleneck with optional shortcuts; args: ch_in, ch_out, number of repeats, shortcut bool,\n        groups, expansion.\n        \"\"\"\n        super().__init__()\n        c_ = int(c2 * e)  # hidden channels\n        self.cv1 = Conv(c1, c_, 1, 1)\n        self.cv2 = nn.Conv2d(c1, c_, 1, 1, bias=False)\n        self.cv3 = nn.Conv2d(c_, c_, 1, 1, bias=False)\n        self.cv4 = Conv(2 * c_, c2, 1, 1)\n        self.bn = nn.BatchNorm2d(2 * c_)  # applied to cat(cv2, cv3)\n        self.act = nn.SiLU()\n        self.m = nn.Sequential(*(Bottleneck(c_, c_, shortcut, g, e=1.0) for _ in range(n)))\n\n    def forward(self, x):\n        \"\"\"Performs forward pass by applying layers, activation, and concatenation on input x, returning feature-\n        enhanced output.\n        \"\"\"\n        y1 = self.cv3(self.m(self.cv1(x)))\n        y2 = self.cv2(x)\n        return self.cv4(self.act(self.bn(torch.cat((y1, y2), 1))))\n\n\nclass CrossConv(nn.Module):\n    \"\"\"Implements a cross convolution layer with downsampling, expansion, and optional shortcut.\"\"\"\n\n    def __init__(self, c1, c2, k=3, s=1, g=1, e=1.0, shortcut=False):\n        \"\"\"\n        Initializes CrossConv with downsampling, expanding, and optionally shortcutting; `c1` input, `c2` output\n        channels.\n\n        Inputs are ch_in, ch_out, kernel, stride, groups, expansion, shortcut.\n        \"\"\"\n        super().__init__()\n        c_ = int(c2 * e)  # hidden channels\n        self.cv1 = Conv(c1, c_, (1, k), (1, s))\n        self.cv2 = Conv(c_, c2, (k, 1), (s, 1), g=g)\n        self.add = shortcut and c1 == c2\n\n    def forward(self, x):\n        \"\"\"Performs feature sampling, expanding, and applies shortcut if channels match; expects `x` input tensor.\"\"\"\n        return x + self.cv2(self.cv1(x)) if self.add else self.cv2(self.cv1(x))\n\n\nclass C3(nn.Module):\n    \"\"\"Implements a CSP Bottleneck module with three convolutions for enhanced feature extraction in neural networks.\"\"\"\n\n    def __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5):\n        \"\"\"Initializes C3 module with options for channel count, bottleneck repetition, shortcut usage, group\n        convolutions, and expansion.\n        \"\"\"\n        super().__init__()\n        c_ = int(c2 * e)  # hidden channels\n        self.cv1 = Conv(c1, c_, 1, 1)\n        self.cv2 = Conv(c1, c_, 1, 1)\n        self.cv3 = Conv(2 * c_, c2, 1)  # optional act=FReLU(c2)\n        self.m = nn.Sequential(*(Bottleneck(c_, c_, shortcut, g, e=1.0) for _ in range(n)))\n\n    def forward(self, x):\n        \"\"\"Performs forward propagation using concatenated outputs from two convolutions and a Bottleneck sequence.\"\"\"\n        return self.cv3(torch.cat((self.m(self.cv1(x)), self.cv2(x)), 1))\n\n\nclass C3x(C3):\n    \"\"\"Extends the C3 module with cross-convolutions for enhanced feature extraction in neural networks.\"\"\"\n\n    def __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5):\n        \"\"\"Initializes C3x module with cross-convolutions, extending C3 with customizable channel dimensions, groups,\n        and expansion.\n        \"\"\"\n        super().__init__(c1, c2, n, shortcut, g, e)\n        c_ = int(c2 * e)\n        self.m = nn.Sequential(*(CrossConv(c_, c_, 3, 1, g, 1.0, shortcut) for _ in range(n)))\n\n\nclass C3TR(C3):\n    \"\"\"C3 module with TransformerBlock for enhanced feature extraction in object detection models.\"\"\"\n\n    def __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5):\n        \"\"\"Initializes C3 module with TransformerBlock for enhanced feature extraction, accepts channel sizes, shortcut\n        config, group, and expansion.\n        \"\"\"\n        super().__init__(c1, c2, n, shortcut, g, e)\n        c_ = int(c2 * e)\n        self.m = TransformerBlock(c_, c_, 4, n)\n\n\nclass C3SPP(C3):\n    \"\"\"Extends the C3 module with an SPP layer for enhanced spatial feature extraction and customizable channels.\"\"\"\n\n    def __init__(self, c1, c2, k=(5, 9, 13), n=1, shortcut=True, g=1, e=0.5):\n        \"\"\"Initializes a C3 module with SPP layer for advanced spatial feature extraction, given channel sizes, kernel\n        sizes, shortcut, group, and expansion ratio.\n        \"\"\"\n        super().__init__(c1, c2, n, shortcut, g, e)\n        c_ = int(c2 * e)\n        self.m = SPP(c_, c_, k)\n\n\nclass C3Ghost(C3):\n    \"\"\"Implements a C3 module with Ghost Bottlenecks for efficient feature extraction in YOLOv5.\"\"\"\n\n    def __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5):\n        \"\"\"Initializes YOLOv5's C3 module with Ghost Bottlenecks for efficient feature extraction.\"\"\"\n        super().__init__(c1, c2, n, shortcut, g, e)\n        c_ = int(c2 * e)  # hidden channels\n        self.m = nn.Sequential(*(GhostBottleneck(c_, c_) for _ in range(n)))\n\n\nclass SPP(nn.Module):\n    \"\"\"Implements Spatial Pyramid Pooling (SPP) for feature extraction, ref: https://arxiv.org/abs/1406.4729.\"\"\"\n\n    def __init__(self, c1, c2, k=(5, 9, 13)):\n        \"\"\"Initializes SPP layer with Spatial Pyramid Pooling, ref: https://arxiv.org/abs/1406.4729, args: c1 (input channels), c2 (output channels), k (kernel sizes).\"\"\"\n        super().__init__()\n        c_ = c1 // 2  # hidden channels\n        self.cv1 = Conv(c1, c_, 1, 1)\n        self.cv2 = Conv(c_ * (len(k) + 1), c2, 1, 1)\n        self.m = nn.ModuleList([nn.MaxPool2d(kernel_size=x, stride=1, padding=x // 2) for x in k])\n\n    def forward(self, x):\n        \"\"\"Applies convolution and max pooling layers to the input tensor `x`, concatenates results, and returns output\n        tensor.\n        \"\"\"\n        x = self.cv1(x)\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\")  # suppress torch 1.9.0 max_pool2d() warning\n            return self.cv2(torch.cat([x] + [m(x) for m in self.m], 1))\n\n\nclass SPPF(nn.Module):\n    \"\"\"Implements a fast Spatial Pyramid Pooling (SPPF) layer for efficient feature extraction in YOLOv5 models.\"\"\"\n\n    def __init__(self, c1, c2, k=5):\n        \"\"\"\n        Initializes YOLOv5 SPPF layer with given channels and kernel size for YOLOv5 model, combining convolution and\n        max pooling.\n\n        Equivalent to SPP(k=(5, 9, 13)).\n        \"\"\"\n        super().__init__()\n        c_ = c1 // 2  # hidden channels\n        self.cv1 = Conv(c1, c_, 1, 1)\n        self.cv2 = Conv(c_ * 4, c2, 1, 1)\n        self.m = nn.MaxPool2d(kernel_size=k, stride=1, padding=k // 2)\n\n    def forward(self, x):\n        \"\"\"Processes input through a series of convolutions and max pooling operations for feature extraction.\"\"\"\n        x = self.cv1(x)\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\")  # suppress torch 1.9.0 max_pool2d() warning\n            y1 = self.m(x)\n            y2 = self.m(y1)\n            return self.cv2(torch.cat((x, y1, y2, self.m(y2)), 1))\n\n\nclass Focus(nn.Module):\n    \"\"\"Focuses spatial information into channel space using slicing and convolution for efficient feature extraction.\"\"\"\n\n    def __init__(self, c1, c2, k=1, s=1, p=None, g=1, act=True):\n        \"\"\"Initializes Focus module to concentrate width-height info into channel space with configurable convolution\n        parameters.\n        \"\"\"\n        super().__init__()\n        self.conv = Conv(c1 * 4, c2, k, s, p, g, act=act)\n        # self.contract = Contract(gain=2)\n\n    def forward(self, x):\n        \"\"\"Processes input through Focus mechanism, reshaping (b,c,w,h) to (b,4c,w/2,h/2) then applies convolution.\"\"\"\n        return self.conv(torch.cat((x[..., ::2, ::2], x[..., 1::2, ::2], x[..., ::2, 1::2], x[..., 1::2, 1::2]), 1))\n        # return self.conv(self.contract(x))\n\n\nclass GhostConv(nn.Module):\n    \"\"\"Implements Ghost Convolution for efficient feature extraction, see https://github.com/huawei-noah/ghostnet.\"\"\"\n\n    def __init__(self, c1, c2, k=1, s=1, g=1, act=True):\n        \"\"\"Initializes GhostConv with in/out channels, kernel size, stride, groups, and activation; halves out channels\n        for efficiency.\n        \"\"\"\n        super().__init__()\n        c_ = c2 // 2  # hidden channels\n        self.cv1 = Conv(c1, c_, k, s, None, g, act=act)\n        self.cv2 = Conv(c_, c_, 5, 1, None, c_, act=act)\n\n    def forward(self, x):\n        \"\"\"Performs forward pass, concatenating outputs of two convolutions on input `x`: shape (B,C,H,W).\"\"\"\n        y = self.cv1(x)\n        return torch.cat((y, self.cv2(y)), 1)\n\n\nclass GhostBottleneck(nn.Module):\n    \"\"\"Efficient bottleneck layer using Ghost Convolutions, see https://github.com/huawei-noah/ghostnet.\"\"\"\n\n    def __init__(self, c1, c2, k=3, s=1):\n        \"\"\"Initializes GhostBottleneck with ch_in `c1`, ch_out `c2`, kernel size `k`, stride `s`; see https://github.com/huawei-noah/ghostnet.\"\"\"\n        super().__init__()\n        c_ = c2 // 2\n        self.conv = nn.Sequential(\n            GhostConv(c1, c_, 1, 1),  # pw\n            DWConv(c_, c_, k, s, act=False) if s == 2 else nn.Identity(),  # dw\n            GhostConv(c_, c2, 1, 1, act=False),\n        )  # pw-linear\n        self.shortcut = (\n            nn.Sequential(DWConv(c1, c1, k, s, act=False), Conv(c1, c2, 1, 1, act=False)) if s == 2 else nn.Identity()\n        )\n\n    def forward(self, x):\n        \"\"\"Processes input through conv and shortcut layers, returning their summed output.\"\"\"\n        return self.conv(x) + self.shortcut(x)\n\n\nclass Contract(nn.Module):\n    \"\"\"Contracts spatial dimensions into channel dimensions for efficient processing in neural networks.\"\"\"\n\n    def __init__(self, gain=2):\n        \"\"\"Initializes a layer to contract spatial dimensions (width-height) into channels, e.g., input shape\n        (1,64,80,80) to (1,256,40,40).\n        \"\"\"\n        super().__init__()\n        self.gain = gain\n\n    def forward(self, x):\n        \"\"\"Processes input tensor to expand channel dimensions by contracting spatial dimensions, yielding output shape\n        `(b, c*s*s, h//s, w//s)`.\n        \"\"\"\n        b, c, h, w = x.size()  # assert (h / s == 0) and (W / s == 0), 'Indivisible gain'\n        s = self.gain\n        x = x.view(b, c, h // s, s, w // s, s)  # x(1,64,40,2,40,2)\n        x = x.permute(0, 3, 5, 1, 2, 4).contiguous()  # x(1,2,2,64,40,40)\n        return x.view(b, c * s * s, h // s, w // s)  # x(1,256,40,40)\n\n\nclass Expand(nn.Module):\n    \"\"\"Expands spatial dimensions by redistributing channels, e.g., from (1,64,80,80) to (1,16,160,160).\"\"\"\n\n    def __init__(self, gain=2):\n        \"\"\"\n        Initializes the Expand module to increase spatial dimensions by redistributing channels, with an optional gain\n        factor.\n\n        Example: x(1,64,80,80) to x(1,16,160,160).\n        \"\"\"\n        super().__init__()\n        self.gain = gain\n\n    def forward(self, x):\n        \"\"\"Processes input tensor x to expand spatial dimensions by redistributing channels, requiring C / gain^2 ==\n        0.\n        \"\"\"\n        b, c, h, w = x.size()  # assert C / s ** 2 == 0, 'Indivisible gain'\n        s = self.gain\n        x = x.view(b, s, s, c // s**2, h, w)  # x(1,2,2,16,80,80)\n        x = x.permute(0, 3, 4, 1, 5, 2).contiguous()  # x(1,16,80,2,80,2)\n        return x.view(b, c // s**2, h * s, w * s)  # x(1,16,160,160)\n\n\nclass Concat(nn.Module):\n    \"\"\"Concatenates tensors along a specified dimension for efficient tensor manipulation in neural networks.\"\"\"\n\n    def __init__(self, dimension=1):\n        \"\"\"Initializes a Concat module to concatenate tensors along a specified dimension.\"\"\"\n        super().__init__()\n        self.d = dimension\n\n    def forward(self, x):\n        \"\"\"Concatenates a list of tensors along a specified dimension; `x` is a list of tensors, `dimension` is an\n        int.\n        \"\"\"\n        return torch.cat(x, self.d)\n\n\nclass DetectMultiBackend(nn.Module):\n    \"\"\"YOLOv5 MultiBackend class for inference on various backends including PyTorch, ONNX, TensorRT, and more.\"\"\"\n\n    def __init__(self, weights=\"yolov5s.pt\", device=torch.device(\"cpu\"), dnn=False, data=None, fp16=False, fuse=True):\n        \"\"\"Initializes DetectMultiBackend with support for various inference backends, including PyTorch and ONNX.\"\"\"\n        #   PyTorch:              weights = *.pt\n        #   TorchScript:                    *.torchscript\n        #   ONNX Runtime:                   *.onnx\n        #   ONNX OpenCV DNN:                *.onnx --dnn\n        #   OpenVINO:                       *_openvino_model\n        #   CoreML:                         *.mlpackage\n        #   TensorRT:                       *.engine\n        #   TensorFlow SavedModel:          *_saved_model\n        #   TensorFlow GraphDef:            *.pb\n        #   TensorFlow Lite:                *.tflite\n        #   TensorFlow Edge TPU:            *_edgetpu.tflite\n        #   PaddlePaddle:                   *_paddle_model\n        from models.experimental import attempt_download, attempt_load  # scoped to avoid circular import\n\n        super().__init__()\n        w = str(weights[0] if isinstance(weights, list) else weights)\n        pt, jit, onnx, xml, engine, coreml, saved_model, pb, tflite, edgetpu, tfjs, paddle, triton = self._model_type(w)\n        fp16 &= pt or jit or onnx or engine or triton  # FP16\n        nhwc = coreml or saved_model or pb or tflite or edgetpu  # BHWC formats (vs torch BCWH)\n        stride = 32  # default stride\n        cuda = torch.cuda.is_available() and device.type != \"cpu\"  # use CUDA\n        if not (pt or triton):\n            w = attempt_download(w)  # download if not local\n\n        if pt:  # PyTorch\n            model = attempt_load(weights if isinstance(weights, list) else w, device=device, inplace=True, fuse=fuse)\n            stride = max(int(model.stride.max()), 32)  # model stride\n            names = model.module.names if hasattr(model, \"module\") else model.names  # get class names\n            model.half() if fp16 else model.float()\n            self.model = model  # explicitly assign for to(), cpu(), cuda(), half()\n        elif jit:  # TorchScript\n            LOGGER.info(f\"Loading {w} for TorchScript inference...\")\n            extra_files = {\"config.txt\": \"\"}  # model metadata\n            model = torch.jit.load(w, _extra_files=extra_files, map_location=device)\n            model.half() if fp16 else model.float()\n            if extra_files[\"config.txt\"]:  # load metadata dict\n                d = json.loads(\n                    extra_files[\"config.txt\"],\n                    object_hook=lambda d: {int(k) if k.isdigit() else k: v for k, v in d.items()},\n                )\n                stride, names = int(d[\"stride\"]), d[\"names\"]\n        elif dnn:  # ONNX OpenCV DNN\n            LOGGER.info(f\"Loading {w} for ONNX OpenCV DNN inference...\")\n            check_requirements(\"opencv-python>=4.5.4\")\n            net = cv2.dnn.readNetFromONNX(w)\n        elif onnx:  # ONNX Runtime\n            LOGGER.info(f\"Loading {w} for ONNX Runtime inference...\")\n            check_requirements((\"onnx\", \"onnxruntime-gpu\" if cuda else \"onnxruntime\"))\n            import onnxruntime\n\n            providers = [\"CUDAExecutionProvider\", \"CPUExecutionProvider\"] if cuda else [\"CPUExecutionProvider\"]\n            session = onnxruntime.InferenceSession(w, providers=providers)\n            output_names = [x.name for x in session.get_outputs()]\n            meta = session.get_modelmeta().custom_metadata_map  # metadata\n            if \"stride\" in meta:\n                stride, names = int(meta[\"stride\"]), eval(meta[\"names\"])\n        elif xml:  # OpenVINO\n            LOGGER.info(f\"Loading {w} for OpenVINO inference...\")\n            check_requirements(\"openvino>=2023.0\")  # requires openvino-dev: https://pypi.org/project/openvino-dev/\n            from openvino.runtime import Core, Layout, get_batch\n\n            core = Core()\n            if not Path(w).is_file():  # if not *.xml\n                w = next(Path(w).glob(\"*.xml\"))  # get *.xml file from *_openvino_model dir\n            ov_model = core.read_model(model=w, weights=Path(w).with_suffix(\".bin\"))\n            if ov_model.get_parameters()[0].get_layout().empty:\n                ov_model.get_parameters()[0].set_layout(Layout(\"NCHW\"))\n            batch_dim = get_batch(ov_model)\n            if batch_dim.is_static:\n                batch_size = batch_dim.get_length()\n            ov_compiled_model = core.compile_model(ov_model, device_name=\"AUTO\")  # AUTO selects best available device\n            stride, names = self._load_metadata(Path(w).with_suffix(\".yaml\"))  # load metadata\n        elif engine:  # TensorRT\n            LOGGER.info(f\"Loading {w} for TensorRT inference...\")\n            import tensorrt as trt  # https://developer.nvidia.com/nvidia-tensorrt-download\n\n            check_version(trt.__version__, \"7.0.0\", hard=True)  # require tensorrt>=7.0.0\n            if device.type == \"cpu\":\n                device = torch.device(\"cuda:0\")\n            Binding = namedtuple(\"Binding\", (\"name\", \"dtype\", \"shape\", \"data\", \"ptr\"))\n            logger = trt.Logger(trt.Logger.INFO)\n            with open(w, \"rb\") as f, trt.Runtime(logger) as runtime:\n                model = runtime.deserialize_cuda_engine(f.read())\n            context = model.create_execution_context()\n            bindings = OrderedDict()\n            output_names = []\n            fp16 = False  # default updated below\n            dynamic = False\n            is_trt10 = not hasattr(model, \"num_bindings\")\n            num = range(model.num_io_tensors) if is_trt10 else range(model.num_bindings)\n            for i in num:\n                if is_trt10:\n                    name = model.get_tensor_name(i)\n                    dtype = trt.nptype(model.get_tensor_dtype(name))\n                    is_input = model.get_tensor_mode(name) == trt.TensorIOMode.INPUT\n                    if is_input:\n                        if -1 in tuple(model.get_tensor_shape(name)):  # dynamic\n                            dynamic = True\n                            context.set_input_shape(name, tuple(model.get_profile_shape(name, 0)[2]))\n                        if dtype == np.float16:\n                            fp16 = True\n                    else:  # output\n                        output_names.append(name)\n                    shape = tuple(context.get_tensor_shape(name))\n                else:\n                    name = model.get_binding_name(i)\n                    dtype = trt.nptype(model.get_binding_dtype(i))\n                    if model.binding_is_input(i):\n                        if -1 in tuple(model.get_binding_shape(i)):  # dynamic\n                            dynamic = True\n                            context.set_binding_shape(i, tuple(model.get_profile_shape(0, i)[2]))\n                        if dtype == np.float16:\n                            fp16 = True\n                    else:  # output\n                        output_names.append(name)\n                    shape = tuple(context.get_binding_shape(i))\n                im = torch.from_numpy(np.empty(shape, dtype=dtype)).to(device)\n                bindings[name] = Binding(name, dtype, shape, im, int(im.data_ptr()))\n            binding_addrs = OrderedDict((n, d.ptr) for n, d in bindings.items())\n            batch_size = bindings[\"images\"].shape[0]  # if dynamic, this is instead max batch size\n        elif coreml:  # CoreML\n            LOGGER.info(f\"Loading {w} for CoreML inference...\")\n            import coremltools as ct\n\n            model = ct.models.MLModel(w)\n        elif saved_model:  # TF SavedModel\n            LOGGER.info(f\"Loading {w} for TensorFlow SavedModel inference...\")\n            import tensorflow as tf\n\n            keras = False  # assume TF1 saved_model\n            model = tf.keras.models.load_model(w) if keras else tf.saved_model.load(w)\n        elif pb:  # GraphDef https://www.tensorflow.org/guide/migrate#a_graphpb_or_graphpbtxt\n            LOGGER.info(f\"Loading {w} for TensorFlow GraphDef inference...\")\n            import tensorflow as tf\n\n            def wrap_frozen_graph(gd, inputs, outputs):\n                \"\"\"Wraps a TensorFlow GraphDef for inference, returning a pruned function.\"\"\"\n                x = tf.compat.v1.wrap_function(lambda: tf.compat.v1.import_graph_def(gd, name=\"\"), [])  # wrapped\n                ge = x.graph.as_graph_element\n                return x.prune(tf.nest.map_structure(ge, inputs), tf.nest.map_structure(ge, outputs))\n\n            def gd_outputs(gd):\n                \"\"\"Generates a sorted list of graph outputs excluding NoOp nodes and inputs, formatted as '<name>:0'.\"\"\"\n                name_list, input_list = [], []\n                for node in gd.node:  # tensorflow.core.framework.node_def_pb2.NodeDef\n                    name_list.append(node.name)\n                    input_list.extend(node.input)\n                return sorted(f\"{x}:0\" for x in list(set(name_list) - set(input_list)) if not x.startswith(\"NoOp\"))\n\n            gd = tf.Graph().as_graph_def()  # TF GraphDef\n            with open(w, \"rb\") as f:\n                gd.ParseFromString(f.read())\n            frozen_func = wrap_frozen_graph(gd, inputs=\"x:0\", outputs=gd_outputs(gd))\n        elif tflite or edgetpu:  # https://www.tensorflow.org/lite/guide/python#install_tensorflow_lite_for_python\n            try:  # https://coral.ai/docs/edgetpu/tflite-python/#update-existing-tf-lite-code-for-the-edge-tpu\n                from tflite_runtime.interpreter import Interpreter, load_delegate\n            except ImportError:\n                import tensorflow as tf\n\n                Interpreter, load_delegate = (\n                    tf.lite.Interpreter,\n                    tf.lite.experimental.load_delegate,\n                )\n            if edgetpu:  # TF Edge TPU https://coral.ai/software/#edgetpu-runtime\n                LOGGER.info(f\"Loading {w} for TensorFlow Lite Edge TPU inference...\")\n                delegate = {\"Linux\": \"libedgetpu.so.1\", \"Darwin\": \"libedgetpu.1.dylib\", \"Windows\": \"edgetpu.dll\"}[\n                    platform.system()\n                ]\n                interpreter = Interpreter(model_path=w, experimental_delegates=[load_delegate(delegate)])\n            else:  # TFLite\n                LOGGER.info(f\"Loading {w} for TensorFlow Lite inference...\")\n                interpreter = Interpreter(model_path=w)  # load TFLite model\n            interpreter.allocate_tensors()  # allocate\n            input_details = interpreter.get_input_details()  # inputs\n            output_details = interpreter.get_output_details()  # outputs\n            # load metadata\n            with contextlib.suppress(zipfile.BadZipFile):\n                with zipfile.ZipFile(w, \"r\") as model:\n                    meta_file = model.namelist()[0]\n                    meta = ast.literal_eval(model.read(meta_file).decode(\"utf-8\"))\n                    stride, names = int(meta[\"stride\"]), meta[\"names\"]\n        elif tfjs:  # TF.js\n            raise NotImplementedError(\"ERROR: YOLOv5 TF.js inference is not supported\")\n        elif paddle:  # PaddlePaddle\n            LOGGER.info(f\"Loading {w} for PaddlePaddle inference...\")\n            check_requirements(\"paddlepaddle-gpu\" if cuda else \"paddlepaddle\")\n            import paddle.inference as pdi\n\n            if not Path(w).is_file():  # if not *.pdmodel\n                w = next(Path(w).rglob(\"*.pdmodel\"))  # get *.pdmodel file from *_paddle_model dir\n            weights = Path(w).with_suffix(\".pdiparams\")\n            config = pdi.Config(str(w), str(weights))\n            if cuda:\n                config.enable_use_gpu(memory_pool_init_size_mb=2048, device_id=0)\n            predictor = pdi.create_predictor(config)\n            input_handle = predictor.get_input_handle(predictor.get_input_names()[0])\n            output_names = predictor.get_output_names()\n        elif triton:  # NVIDIA Triton Inference Server\n            LOGGER.info(f\"Using {w} as Triton Inference Server...\")\n            check_requirements(\"tritonclient[all]\")\n            from utils.triton import TritonRemoteModel\n\n            model = TritonRemoteModel(url=w)\n            nhwc = model.runtime.startswith(\"tensorflow\")\n        else:\n            raise NotImplementedError(f\"ERROR: {w} is not a supported format\")\n\n        # class names\n        if \"names\" not in locals():\n            names = yaml_load(data)[\"names\"] if data else {i: f\"class{i}\" for i in range(999)}\n        if names[0] == \"n01440764\" and len(names) == 1000:  # ImageNet\n            names = yaml_load(ROOT / \"data/ImageNet.yaml\")[\"names\"]  # human-readable names\n\n        self.__dict__.update(locals())  # assign all variables to self\n\n    def forward(self, im, augment=False, visualize=False):\n        \"\"\"Performs YOLOv5 inference on input images with options for augmentation and visualization.\"\"\"\n        b, ch, h, w = im.shape  # batch, channel, height, width\n        if self.fp16 and im.dtype != torch.float16:\n            im = im.half()  # to FP16\n        if self.nhwc:\n            im = im.permute(0, 2, 3, 1)  # torch BCHW to numpy BHWC shape(1,320,192,3)\n\n        if self.pt:  # PyTorch\n            y = self.model(im, augment=augment, visualize=visualize) if augment or visualize else self.model(im)\n        elif self.jit:  # TorchScript\n            y = self.model(im)\n        elif self.dnn:  # ONNX OpenCV DNN\n            im = im.cpu().numpy()  # torch to numpy\n            self.net.setInput(im)\n            y = self.net.forward()\n        elif self.onnx:  # ONNX Runtime\n            im = im.cpu().numpy()  # torch to numpy\n            y = self.session.run(self.output_names, {self.session.get_inputs()[0].name: im})\n        elif self.xml:  # OpenVINO\n            im = im.cpu().numpy()  # FP32\n            y = list(self.ov_compiled_model(im).values())\n        elif self.engine:  # TensorRT\n            if self.dynamic and im.shape != self.bindings[\"images\"].shape:\n                i = self.model.get_binding_index(\"images\")\n                self.context.set_binding_shape(i, im.shape)  # reshape if dynamic\n                self.bindings[\"images\"] = self.bindings[\"images\"]._replace(shape=im.shape)\n                for name in self.output_names:\n                    i = self.model.get_binding_index(name)\n                    self.bindings[name].data.resize_(tuple(self.context.get_binding_shape(i)))\n            s = self.bindings[\"images\"].shape\n            assert im.shape == s, f\"input size {im.shape} {'>' if self.dynamic else 'not equal to'} max model size {s}\"\n            self.binding_addrs[\"images\"] = int(im.data_ptr())\n            self.context.execute_v2(list(self.binding_addrs.values()))\n            y = [self.bindings[x].data for x in sorted(self.output_names)]\n        elif self.coreml:  # CoreML\n            im = im.cpu().numpy()\n            im = Image.fromarray((im[0] * 255).astype(\"uint8\"))\n            # im = im.resize((192, 320), Image.BILINEAR)\n            y = self.model.predict({\"image\": im})  # coordinates are xywh normalized\n            if \"confidence\" in y:\n                box = xywh2xyxy(y[\"coordinates\"] * [[w, h, w, h]])  # xyxy pixels\n                conf, cls = y[\"confidence\"].max(1), y[\"confidence\"].argmax(1).astype(np.float)\n                y = np.concatenate((box, conf.reshape(-1, 1), cls.reshape(-1, 1)), 1)\n            else:\n                y = list(reversed(y.values()))  # reversed for segmentation models (pred, proto)\n        elif self.paddle:  # PaddlePaddle\n            im = im.cpu().numpy().astype(np.float32)\n            self.input_handle.copy_from_cpu(im)\n            self.predictor.run()\n            y = [self.predictor.get_output_handle(x).copy_to_cpu() for x in self.output_names]\n        elif self.triton:  # NVIDIA Triton Inference Server\n            y = self.model(im)\n        else:  # TensorFlow (SavedModel, GraphDef, Lite, Edge TPU)\n            im = im.cpu().numpy()\n            if self.saved_model:  # SavedModel\n                y = self.model(im, training=False) if self.keras else self.model(im)\n            elif self.pb:  # GraphDef\n                y = self.frozen_func(x=self.tf.constant(im))\n            else:  # Lite or Edge TPU\n                input = self.input_details[0]\n                int8 = input[\"dtype\"] == np.uint8  # is TFLite quantized uint8 model\n                if int8:\n                    scale, zero_point = input[\"quantization\"]\n                    im = (im / scale + zero_point).astype(np.uint8)  # de-scale\n                self.interpreter.set_tensor(input[\"index\"], im)\n                self.interpreter.invoke()\n                y = []\n                for output in self.output_details:\n                    x = self.interpreter.get_tensor(output[\"index\"])\n                    if int8:\n                        scale, zero_point = output[\"quantization\"]\n                        x = (x.astype(np.float32) - zero_point) * scale  # re-scale\n                    y.append(x)\n            if len(y) == 2 and len(y[1].shape) != 4:\n                y = list(reversed(y))\n            y = [x if isinstance(x, np.ndarray) else x.numpy() for x in y]\n            y[0][..., :4] *= [w, h, w, h]  # xywh normalized to pixels\n\n        if isinstance(y, (list, tuple)):\n            return self.from_numpy(y[0]) if len(y) == 1 else [self.from_numpy(x) for x in y]\n        else:\n            return self.from_numpy(y)\n\n    def from_numpy(self, x):\n        \"\"\"Converts a NumPy array to a torch tensor, maintaining device compatibility.\"\"\"\n        return torch.from_numpy(x).to(self.device) if isinstance(x, np.ndarray) else x\n\n    def warmup(self, imgsz=(1, 3, 640, 640)):\n        \"\"\"Performs a single inference warmup to initialize model weights, accepting an `imgsz` tuple for image size.\"\"\"\n        warmup_types = self.pt, self.jit, self.onnx, self.engine, self.saved_model, self.pb, self.triton\n        if any(warmup_types) and (self.device.type != \"cpu\" or self.triton):\n            im = torch.empty(*imgsz, dtype=torch.half if self.fp16 else torch.float, device=self.device)  # input\n            for _ in range(2 if self.jit else 1):  #\n                self.forward(im)  # warmup\n\n    @staticmethod\n    def _model_type(p=\"path/to/model.pt\"):\n        \"\"\"\n        Determines model type from file path or URL, supporting various export formats.\n\n        Example: path='path/to/model.onnx' -> type=onnx\n        \"\"\"\n        # types = [pt, jit, onnx, xml, engine, coreml, saved_model, pb, tflite, edgetpu, tfjs, paddle]\n        from export import export_formats\n        from utils.downloads import is_url\n\n        sf = list(export_formats().Suffix)  # export suffixes\n        if not is_url(p, check=False):\n            check_suffix(p, sf)  # checks\n        url = urlparse(p)  # if url may be Triton inference server\n        types = [s in Path(p).name for s in sf]\n        types[8] &= not types[9]  # tflite &= not edgetpu\n        triton = not any(types) and all([any(s in url.scheme for s in [\"http\", \"grpc\"]), url.netloc])\n        return types + [triton]\n\n    @staticmethod\n    def _load_metadata(f=Path(\"path/to/meta.yaml\")):\n        \"\"\"Loads metadata from a YAML file, returning strides and names if the file exists, otherwise `None`.\"\"\"\n        if f.exists():\n            d = yaml_load(f)\n            return d[\"stride\"], d[\"names\"]  # assign stride, names\n        return None, None\n\n\nclass AutoShape(nn.Module):\n    \"\"\"AutoShape class for robust YOLOv5 inference with preprocessing, NMS, and support for various input formats.\"\"\"\n\n    conf = 0.25  # NMS confidence threshold\n    iou = 0.45  # NMS IoU threshold\n    agnostic = False  # NMS class-agnostic\n    multi_label = False  # NMS multiple labels per box\n    classes = None  # (optional list) filter by class, i.e. = [0, 15, 16] for COCO persons, cats and dogs\n    max_det = 1000  # maximum number of detections per image\n    amp = False  # Automatic Mixed Precision (AMP) inference\n\n    def __init__(self, model, verbose=True):\n        \"\"\"Initializes YOLOv5 model for inference, setting up attributes and preparing model for evaluation.\"\"\"\n        super().__init__()\n        if verbose:\n            LOGGER.info(\"Adding AutoShape... \")\n        copy_attr(self, model, include=(\"yaml\", \"nc\", \"hyp\", \"names\", \"stride\", \"abc\"), exclude=())  # copy attributes\n        self.dmb = isinstance(model, DetectMultiBackend)  # DetectMultiBackend() instance\n        self.pt = not self.dmb or model.pt  # PyTorch model\n        self.model = model.eval()\n        if self.pt:\n            m = self.model.model.model[-1] if self.dmb else self.model.model[-1]  # Detect()\n            m.inplace = False  # Detect.inplace=False for safe multithread inference\n            m.export = True  # do not output loss values\n\n    def _apply(self, fn):\n        \"\"\"\n        Applies to(), cpu(), cuda(), half() etc.\n\n        to model tensors excluding parameters or registered buffers.\n        \"\"\"\n        self = super()._apply(fn)\n        if self.pt:\n            m = self.model.model.model[-1] if self.dmb else self.model.model[-1]  # Detect()\n            m.stride = fn(m.stride)\n            m.grid = list(map(fn, m.grid))\n            if isinstance(m.anchor_grid, list):\n                m.anchor_grid = list(map(fn, m.anchor_grid))\n        return self\n\n    @smart_inference_mode()\n    def forward(self, ims, size=640, augment=False, profile=False):\n        \"\"\"\n        Performs inference on inputs with optional augment & profiling.\n\n        Supports various formats including file, URI, OpenCV, PIL, numpy, torch.\n        \"\"\"\n        # For size(height=640, width=1280), RGB images example inputs are:\n        #   file:        ims = 'data/images/zidane.jpg'  # str or PosixPath\n        #   URI:             = 'https://ultralytics.com/images/zidane.jpg'\n        #   OpenCV:          = cv2.imread('image.jpg')[:,:,::-1]  # HWC BGR to RGB x(640,1280,3)\n        #   PIL:             = Image.open('image.jpg') or ImageGrab.grab()  # HWC x(640,1280,3)\n        #   numpy:           = np.zeros((640,1280,3))  # HWC\n        #   torch:           = torch.zeros(16,3,320,640)  # BCHW (scaled to size=640, 0-1 values)\n        #   multiple:        = [Image.open('image1.jpg'), Image.open('image2.jpg'), ...]  # list of images\n\n        dt = (Profile(), Profile(), Profile())\n        with dt[0]:\n            if isinstance(size, int):  # expand\n                size = (size, size)\n            p = next(self.model.parameters()) if self.pt else torch.empty(1, device=self.model.device)  # param\n            autocast = self.amp and (p.device.type != \"cpu\")  # Automatic Mixed Precision (AMP) inference\n            if isinstance(ims, torch.Tensor):  # torch\n                with amp.autocast(autocast):\n                    return self.model(ims.to(p.device).type_as(p), augment=augment)  # inference\n\n            # Pre-process\n            n, ims = (len(ims), list(ims)) if isinstance(ims, (list, tuple)) else (1, [ims])  # number, list of images\n            shape0, shape1, files = [], [], []  # image and inference shapes, filenames\n            for i, im in enumerate(ims):\n                f = f\"image{i}\"  # filename\n                if isinstance(im, (str, Path)):  # filename or uri\n                    im, f = Image.open(requests.get(im, stream=True).raw if str(im).startswith(\"http\") else im), im\n                    im = np.asarray(exif_transpose(im))\n                elif isinstance(im, Image.Image):  # PIL Image\n                    im, f = np.asarray(exif_transpose(im)), getattr(im, \"filename\", f) or f\n                files.append(Path(f).with_suffix(\".jpg\").name)\n                if im.shape[0] < 5:  # image in CHW\n                    im = im.transpose((1, 2, 0))  # reverse dataloader .transpose(2, 0, 1)\n                im = im[..., :3] if im.ndim == 3 else cv2.cvtColor(im, cv2.COLOR_GRAY2BGR)  # enforce 3ch input\n                s = im.shape[:2]  # HWC\n                shape0.append(s)  # image shape\n                g = max(size) / max(s)  # gain\n                shape1.append([int(y * g) for y in s])\n                ims[i] = im if im.data.contiguous else np.ascontiguousarray(im)  # update\n            shape1 = [make_divisible(x, self.stride) for x in np.array(shape1).max(0)]  # inf shape\n            x = [letterbox(im, shape1, auto=False)[0] for im in ims]  # pad\n            x = np.ascontiguousarray(np.array(x).transpose((0, 3, 1, 2)))  # stack and BHWC to BCHW\n            x = torch.from_numpy(x).to(p.device).type_as(p) / 255  # uint8 to fp16/32\n\n        with amp.autocast(autocast):\n            # Inference\n            with dt[1]:\n                y = self.model(x, augment=augment)  # forward\n\n            # Post-process\n            with dt[2]:\n                y = non_max_suppression(\n                    y if self.dmb else y[0],\n                    self.conf,\n                    self.iou,\n                    self.classes,\n                    self.agnostic,\n                    self.multi_label,\n                    max_det=self.max_det,\n                )  # NMS\n                for i in range(n):\n                    scale_boxes(shape1, y[i][:, :4], shape0[i])\n\n            return Detections(ims, y, files, dt, self.names, x.shape)\n\n\nclass Detections:\n    \"\"\"Manages YOLOv5 detection results with methods for visualization, saving, cropping, and exporting detections.\"\"\"\n\n    def __init__(self, ims, pred, files, times=(0, 0, 0), names=None, shape=None):\n        \"\"\"Initializes the YOLOv5 Detections class with image info, predictions, filenames, timing and normalization.\"\"\"\n        super().__init__()\n        d = pred[0].device  # device\n        gn = [torch.tensor([*(im.shape[i] for i in [1, 0, 1, 0]), 1, 1], device=d) for im in ims]  # normalizations\n        self.ims = ims  # list of images as numpy arrays\n        self.pred = pred  # list of tensors pred[0] = (xyxy, conf, cls)\n        self.names = names  # class names\n        self.files = files  # image filenames\n        self.times = times  # profiling times\n        self.xyxy = pred  # xyxy pixels\n        self.xywh = [xyxy2xywh(x) for x in pred]  # xywh pixels\n        self.xyxyn = [x / g for x, g in zip(self.xyxy, gn)]  # xyxy normalized\n        self.xywhn = [x / g for x, g in zip(self.xywh, gn)]  # xywh normalized\n        self.n = len(self.pred)  # number of images (batch size)\n        self.t = tuple(x.t / self.n * 1e3 for x in times)  # timestamps (ms)\n        self.s = tuple(shape)  # inference BCHW shape\n\n    def _run(self, pprint=False, show=False, save=False, crop=False, render=False, labels=True, save_dir=Path(\"\")):\n        \"\"\"Executes model predictions, displaying and/or saving outputs with optional crops and labels.\"\"\"\n        s, crops = \"\", []\n        for i, (im, pred) in enumerate(zip(self.ims, self.pred)):\n            s += f\"\\nimage {i + 1}/{len(self.pred)}: {im.shape[0]}x{im.shape[1]} \"  # string\n            if pred.shape[0]:\n                for c in pred[:, -1].unique():\n                    n = (pred[:, -1] == c).sum()  # detections per class\n                    s += f\"{n} {self.names[int(c)]}{'s' * (n > 1)}, \"  # add to string\n                s = s.rstrip(\", \")\n                if show or save or render or crop:\n                    annotator = Annotator(im, example=str(self.names))\n                    for *box, conf, cls in reversed(pred):  # xyxy, confidence, class\n                        label = f\"{self.names[int(cls)]} {conf:.2f}\"\n                        if crop:\n                            file = save_dir / \"crops\" / self.names[int(cls)] / self.files[i] if save else None\n                            crops.append(\n                                {\n                                    \"box\": box,\n                                    \"conf\": conf,\n                                    \"cls\": cls,\n                                    \"label\": label,\n                                    \"im\": save_one_box(box, im, file=file, save=save),\n                                }\n                            )\n                        else:  # all others\n                            annotator.box_label(box, label if labels else \"\", color=colors(cls))\n                    im = annotator.im\n            else:\n                s += \"(no detections)\"\n\n            im = Image.fromarray(im.astype(np.uint8)) if isinstance(im, np.ndarray) else im  # from np\n            if show:\n                if is_jupyter():\n                    from IPython.display import display\n\n                    display(im)\n                else:\n                    im.show(self.files[i])\n            if save:\n                f = self.files[i]\n                im.save(save_dir / f)  # save\n                if i == self.n - 1:\n                    LOGGER.info(f\"Saved {self.n} image{'s' * (self.n > 1)} to {colorstr('bold', save_dir)}\")\n            if render:\n                self.ims[i] = np.asarray(im)\n        if pprint:\n            s = s.lstrip(\"\\n\")\n            return f\"{s}\\nSpeed: %.1fms pre-process, %.1fms inference, %.1fms NMS per image at shape {self.s}\" % self.t\n        if crop:\n            if save:\n                LOGGER.info(f\"Saved results to {save_dir}\\n\")\n            return crops\n\n    @TryExcept(\"Showing images is not supported in this environment\")\n    def show(self, labels=True):\n        \"\"\"\n        Displays detection results with optional labels.\n\n        Usage: show(labels=True)\n        \"\"\"\n        self._run(show=True, labels=labels)  # show results\n\n    def save(self, labels=True, save_dir=\"runs/detect/exp\", exist_ok=False):\n        \"\"\"\n        Saves detection results with optional labels to a specified directory.\n\n        Usage: save(labels=True, save_dir='runs/detect/exp', exist_ok=False)\n        \"\"\"\n        save_dir = increment_path(save_dir, exist_ok, mkdir=True)  # increment save_dir\n        self._run(save=True, labels=labels, save_dir=save_dir)  # save results\n\n    def crop(self, save=True, save_dir=\"runs/detect/exp\", exist_ok=False):\n        \"\"\"\n        Crops detection results, optionally saves them to a directory.\n\n        Args: save (bool), save_dir (str), exist_ok (bool).\n        \"\"\"\n        save_dir = increment_path(save_dir, exist_ok, mkdir=True) if save else None\n        return self._run(crop=True, save=save, save_dir=save_dir)  # crop results\n\n    def render(self, labels=True):\n        \"\"\"Renders detection results with optional labels on images; args: labels (bool) indicating label inclusion.\"\"\"\n        self._run(render=True, labels=labels)  # render results\n        return self.ims\n\n    def pandas(self):\n        \"\"\"\n        Returns detections as pandas DataFrames for various box formats (xyxy, xyxyn, xywh, xywhn).\n\n        Example: print(results.pandas().xyxy[0]).\n        \"\"\"\n        new = copy(self)  # return copy\n        ca = \"xmin\", \"ymin\", \"xmax\", \"ymax\", \"confidence\", \"class\", \"name\"  # xyxy columns\n        cb = \"xcenter\", \"ycenter\", \"width\", \"height\", \"confidence\", \"class\", \"name\"  # xywh columns\n        for k, c in zip([\"xyxy\", \"xyxyn\", \"xywh\", \"xywhn\"], [ca, ca, cb, cb]):\n            a = [[x[:5] + [int(x[5]), self.names[int(x[5])]] for x in x.tolist()] for x in getattr(self, k)]  # update\n            setattr(new, k, [pd.DataFrame(x, columns=c) for x in a])\n        return new\n\n    def tolist(self):\n        \"\"\"\n        Converts a Detections object into a list of individual detection results for iteration.\n\n        Example: for result in results.tolist():\n        \"\"\"\n        r = range(self.n)  # iterable\n        return [\n            Detections(\n                [self.ims[i]],\n                [self.pred[i]],\n                [self.files[i]],\n                self.times,\n                self.names,\n                self.s,\n            )\n            for i in r\n        ]\n\n    def print(self):\n        \"\"\"Logs the string representation of the current object's state via the LOGGER.\"\"\"\n        LOGGER.info(self.__str__())\n\n    def __len__(self):\n        \"\"\"Returns the number of results stored, overrides the default len(results).\"\"\"\n        return self.n\n\n    def __str__(self):\n        \"\"\"Returns a string representation of the model's results, suitable for printing, overrides default\n        print(results).\n        \"\"\"\n        return self._run(pprint=True)  # print results\n\n    def __repr__(self):\n        \"\"\"Returns a string representation of the YOLOv5 object, including its class and formatted results.\"\"\"\n        return f\"YOLOv5 {self.__class__} instance\\n\" + self.__str__()\n\n\nclass Proto(nn.Module):\n    \"\"\"YOLOv5 mask Proto module for segmentation models, performing convolutions and upsampling on input tensors.\"\"\"\n\n    def __init__(self, c1, c_=256, c2=32):\n        \"\"\"Initializes YOLOv5 Proto module for segmentation with input, proto, and mask channels configuration.\"\"\"\n        super().__init__()\n        self.cv1 = Conv(c1, c_, k=3)\n        self.upsample = nn.Upsample(scale_factor=2, mode=\"nearest\")\n        self.cv2 = Conv(c_, c_, k=3)\n        self.cv3 = Conv(c_, c2)\n\n    def forward(self, x):\n        \"\"\"Performs a forward pass using convolutional layers and upsampling on input tensor `x`.\"\"\"\n        return self.cv3(self.cv2(self.upsample(self.cv1(x))))\n\n\nclass Classify(nn.Module):\n    \"\"\"YOLOv5 classification head with convolution, pooling, and dropout layers for channel transformation.\"\"\"\n\n    def __init__(\n        self, c1, c2, k=1, s=1, p=None, g=1, dropout_p=0.0\n    ):  # ch_in, ch_out, kernel, stride, padding, groups, dropout probability\n        \"\"\"Initializes YOLOv5 classification head with convolution, pooling, and dropout layers for input to output\n        channel transformation.\n        \"\"\"\n        super().__init__()\n        c_ = 1280  # efficientnet_b0 size\n        self.conv = Conv(c1, c_, k, s, autopad(k, p), g)\n        self.pool = nn.AdaptiveAvgPool2d(1)  # to x(b,c_,1,1)\n        self.drop = nn.Dropout(p=dropout_p, inplace=True)\n        self.linear = nn.Linear(c_, c2)  # to x(b,c2)\n\n    def forward(self, x):\n        \"\"\"Processes input through conv, pool, drop, and linear layers; supports list concatenation input.\"\"\"\n        if isinstance(x, list):\n            x = torch.cat(x, 1)\n        return self.linear(self.drop(self.pool(self.conv(x)).flatten(1)))\n",
    "models/experimental.py": "# Ultralytics 🚀 AGPL-3.0 License - https://ultralytics.com/license\n\"\"\"Experimental modules.\"\"\"\n\nimport math\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\n\nfrom utils.downloads import attempt_download\n\n\nclass Sum(nn.Module):\n    \"\"\"Weighted sum of 2 or more layers https://arxiv.org/abs/1911.09070.\"\"\"\n\n    def __init__(self, n, weight=False):\n        \"\"\"Initializes a module to sum outputs of layers with number of inputs `n` and optional weighting, supporting 2+\n        inputs.\n        \"\"\"\n        super().__init__()\n        self.weight = weight  # apply weights boolean\n        self.iter = range(n - 1)  # iter object\n        if weight:\n            self.w = nn.Parameter(-torch.arange(1.0, n) / 2, requires_grad=True)  # layer weights\n\n    def forward(self, x):\n        \"\"\"Processes input through a customizable weighted sum of `n` inputs, optionally applying learned weights.\"\"\"\n        y = x[0]  # no weight\n        if self.weight:\n            w = torch.sigmoid(self.w) * 2\n            for i in self.iter:\n                y = y + x[i + 1] * w[i]\n        else:\n            for i in self.iter:\n                y = y + x[i + 1]\n        return y\n\n\nclass MixConv2d(nn.Module):\n    \"\"\"Mixed Depth-wise Conv https://arxiv.org/abs/1907.09595.\"\"\"\n\n    def __init__(self, c1, c2, k=(1, 3), s=1, equal_ch=True):\n        \"\"\"Initializes MixConv2d with mixed depth-wise convolutional layers, taking input and output channels (c1, c2),\n        kernel sizes (k), stride (s), and channel distribution strategy (equal_ch).\n        \"\"\"\n        super().__init__()\n        n = len(k)  # number of convolutions\n        if equal_ch:  # equal c_ per group\n            i = torch.linspace(0, n - 1e-6, c2).floor()  # c2 indices\n            c_ = [(i == g).sum() for g in range(n)]  # intermediate channels\n        else:  # equal weight.numel() per group\n            b = [c2] + [0] * n\n            a = np.eye(n + 1, n, k=-1)\n            a -= np.roll(a, 1, axis=1)\n            a *= np.array(k) ** 2\n            a[0] = 1\n            c_ = np.linalg.lstsq(a, b, rcond=None)[0].round()  # solve for equal weight indices, ax = b\n\n        self.m = nn.ModuleList(\n            [nn.Conv2d(c1, int(c_), k, s, k // 2, groups=math.gcd(c1, int(c_)), bias=False) for k, c_ in zip(k, c_)]\n        )\n        self.bn = nn.BatchNorm2d(c2)\n        self.act = nn.SiLU()\n\n    def forward(self, x):\n        \"\"\"Performs forward pass by applying SiLU activation on batch-normalized concatenated convolutional layer\n        outputs.\n        \"\"\"\n        return self.act(self.bn(torch.cat([m(x) for m in self.m], 1)))\n\n\nclass Ensemble(nn.ModuleList):\n    \"\"\"Ensemble of models.\"\"\"\n\n    def __init__(self):\n        \"\"\"Initializes an ensemble of models to be used for aggregated predictions.\"\"\"\n        super().__init__()\n\n    def forward(self, x, augment=False, profile=False, visualize=False):\n        \"\"\"Performs forward pass aggregating outputs from an ensemble of models..\"\"\"\n        y = [module(x, augment, profile, visualize)[0] for module in self]\n        # y = torch.stack(y).max(0)[0]  # max ensemble\n        # y = torch.stack(y).mean(0)  # mean ensemble\n        y = torch.cat(y, 1)  # nms ensemble\n        return y, None  # inference, train output\n\n\ndef attempt_load(weights, device=None, inplace=True, fuse=True):\n    \"\"\"\n    Loads and fuses an ensemble or single YOLOv5 model from weights, handling device placement and model adjustments.\n\n    Example inputs: weights=[a,b,c] or a single model weights=[a] or weights=a.\n    \"\"\"\n    from models.yolo import Detect, Model\n\n    model = Ensemble()\n    for w in weights if isinstance(weights, list) else [weights]:\n        ckpt = torch.load(attempt_download(w), map_location=\"cpu\")  # load\n        ckpt = (ckpt.get(\"ema\") or ckpt[\"model\"]).to(device).float()  # FP32 model\n\n        # Model compatibility updates\n        if not hasattr(ckpt, \"stride\"):\n            ckpt.stride = torch.tensor([32.0])\n        if hasattr(ckpt, \"names\") and isinstance(ckpt.names, (list, tuple)):\n            ckpt.names = dict(enumerate(ckpt.names))  # convert to dict\n\n        model.append(ckpt.fuse().eval() if fuse and hasattr(ckpt, \"fuse\") else ckpt.eval())  # model in eval mode\n\n    # Module updates\n    for m in model.modules():\n        t = type(m)\n        if t in (nn.Hardswish, nn.LeakyReLU, nn.ReLU, nn.ReLU6, nn.SiLU, Detect, Model):\n            m.inplace = inplace\n            if t is Detect and not isinstance(m.anchor_grid, list):\n                delattr(m, \"anchor_grid\")\n                setattr(m, \"anchor_grid\", [torch.zeros(1)] * m.nl)\n        elif t is nn.Upsample and not hasattr(m, \"recompute_scale_factor\"):\n            m.recompute_scale_factor = None  # torch 1.11.0 compatibility\n\n    # Return model\n    if len(model) == 1:\n        return model[-1]\n\n    # Return detection ensemble\n    print(f\"Ensemble created with {weights}\\n\")\n    for k in \"names\", \"nc\", \"yaml\":\n        setattr(model, k, getattr(model[0], k))\n    model.stride = model[torch.argmax(torch.tensor([m.stride.max() for m in model])).int()].stride  # max stride\n    assert all(model[0].nc == m.nc for m in model), f\"Models have different class counts: {[m.nc for m in model]}\"\n    return model\n"
  },
  "requirements": "# YOLOv5 requirements\n# Usage: pip install -r requirements.txt\n\n# Base ------------------------------------------------------------------------\ngitpython>=3.1.30\nmatplotlib>=3.3\nnumpy>=1.23.5\nopencv-python>=4.1.1\npillow>=10.3.0\npsutil  # system resources\nPyYAML>=5.3.1\nrequests>=2.32.2\nscipy>=1.4.1\nthop>=0.1.1  # FLOPs computation\ntorch>=1.8.0  # see https://pytorch.org/get-started/locally (recommended)\ntorchvision>=0.9.0\ntqdm>=4.66.3\nultralytics>=8.2.34  # https://ultralytics.com\n# protobuf<=3.20.1  # https://github.com/ultralytics/yolov5/issues/8012\n\n# Logging ---------------------------------------------------------------------\n# tensorboard>=2.4.1\n# clearml>=1.2.0\n# comet\n\n# Plotting --------------------------------------------------------------------\npandas>=1.1.4\nseaborn>=0.11.0\n\n# Export ----------------------------------------------------------------------\n# coremltools>=6.0  # CoreML export\n# onnx>=1.10.0  # ONNX export\n# onnx-simplifier>=0.4.1  # ONNX simplifier\n# nvidia-pyindex  # TensorRT export\n# nvidia-tensorrt  # TensorRT export\n# scikit-learn<=1.1.2  # CoreML quantization\n# tensorflow>=2.4.0,<=2.13.1  # TF exports (-cpu, -aarch64, -macos)\n# tensorflowjs>=3.9.0  # TF.js export\n# openvino-dev>=2023.0  # OpenVINO export\n\n# Deploy ----------------------------------------------------------------------\nsetuptools>=70.0.0 # Snyk vulnerability fix\n# tritonclient[all]~=2.24.0\n\n# Extras ----------------------------------------------------------------------\n# ipython  # interactive notebook\n# mss  # screenshots\n# albumentations>=1.0.3\n# pycocotools>=2.0.6  # COCO mAP\n"
}