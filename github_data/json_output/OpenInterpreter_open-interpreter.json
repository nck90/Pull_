{
  "repo_name": "OpenInterpreter/open-interpreter",
  "repo_url": "https://github.com/OpenInterpreter/open-interpreter",
  "description": "A natural language interface for computers",
  "stars": 58818,
  "language": "Python",
  "created_at": "2023-07-14T07:10:44Z",
  "updated_at": "2025-03-19T05:26:16Z",
  "files": {
    "tests/core/computer/files/test_files.py": "import unittest\nfrom unittest import mock\n\nfrom interpreter.core.computer.files.files import Files\n\n\nclass TestFiles(unittest.TestCase):\n    def setUp(self):\n        self.files = Files(mock.Mock())\n\n    @mock.patch(\"interpreter.core.computer.files.files.aifs\")\n    def test_search(self, mock_aifs):\n        # Arrange\n        mock_args = [\"foo\", \"bar\"]\n        mock_kwargs = {\"foo\": \"bar\"}\n\n        # Act\n        self.files.search(mock_args, mock_kwargs)\n\n        # Assert\n        mock_aifs.search.assert_called_once_with(mock_args, mock_kwargs)\n\n    def test_edit_original_text_in_filedata(self):\n        # Arrange\n        mock_open = mock.mock_open(read_data=\"foobar\")\n        mock_write = mock_open.return_value.write\n\n        # Act\n        with mock.patch(\"interpreter.core.computer.files.files.open\", mock_open):\n            self.files.edit(\"example/filepath/file\", \"foobar\", \"foobarbaz\")\n\n        # Assert\n        mock_open.assert_any_call(\"example/filepath/file\", \"r\")\n        mock_open.assert_any_call(\"example/filepath/file\", \"w\")\n        mock_write.assert_called_once_with(\"foobarbaz\")\n\n    def test_edit_original_text_not_in_filedata(self):\n        # Arrange\n        mock_open = mock.mock_open(read_data=\"foobar\")\n\n        # Act\n        with self.assertRaises(ValueError) as context_manager:\n            with mock.patch(\"interpreter.core.computer.files.files.open\", mock_open):\n                self.files.edit(\"example/filepath/file\", \"barbaz\", \"foobarbaz\")\n\n        # Assert\n        mock_open.assert_any_call(\"example/filepath/file\", \"r\")\n        self.assertEqual(\n            str(context_manager.exception),\n            \"Original text not found. Did you mean one of these? foobar\",\n        )\n",
    "tests/core/computer/test_computer.py": "import unittest\nfrom unittest import mock\nfrom interpreter.core.computer.computer import Computer\n\nclass TestComputer(unittest.TestCase):\n    def setUp(self):\n        self.computer = Computer(mock.Mock())\n\n    def test_get_all_computer_tools_list(self):\n        # Act\n        tools_list = self.computer._get_all_computer_tools_list()\n\n        # Assert\n        self.assertEqual(len(tools_list), 15)\n\n    def test_get_all_computer_tools_signature_and_description(self):\n        # Act\n        tools_description = self.computer._get_all_computer_tools_signature_and_description()\n\n        # Assert\n        self.assertGreater(len(tools_description), 64)\n\nif __name__ == \"__main__\":\n    testing = TestComputer()\n    testing.setUp()\n    testing.test_get_all_computer_tools_signature_and_description()",
    "tests/core/test_async_core.py": "import os\nfrom unittest import TestCase, mock\n\nfrom interpreter.core.async_core import AsyncInterpreter, Server\n\n\nclass TestServerConstruction(TestCase):\n    \"\"\"\n    Tests to make sure that the underlying server is configured correctly when constructing\n    the Server object.\n    \"\"\"\n\n    def test_host_and_port_defaults(self):\n        \"\"\"\n        Tests that a Server object takes on the default host and port when\n        a) no host and port are passed in, and\n        b) no HOST and PORT are set.\n        \"\"\"\n        with mock.patch.dict(os.environ, {}):\n            s = Server(AsyncInterpreter())\n            self.assertEqual(s.host, Server.DEFAULT_HOST)\n            self.assertEqual(s.port, Server.DEFAULT_PORT)\n\n    def test_host_and_port_passed_in(self):\n        \"\"\"\n        Tests that a Server object takes on the passed-in host and port when they are passed-in,\n        ignoring the surrounding HOST and PORT env vars.\n        \"\"\"\n        host = \"the-really-real-host\"\n        port = 2222\n\n        with mock.patch.dict(\n            os.environ,\n            {\"INTERPRETER_HOST\": \"this-is-supes-fake\", \"INTERPRETER_PORT\": \"9876\"},\n        ):\n            sboth = Server(AsyncInterpreter(), host, port)\n            self.assertEqual(sboth.host, host)\n            self.assertEqual(sboth.port, port)\n\n    def test_host_and_port_from_env_1(self):\n        \"\"\"\n        Tests that the Server object takes on the HOST and PORT env vars as host and port when\n        nothing has been passed in.\n        \"\"\"\n        fake_host = \"fake_host\"\n        fake_port = 1234\n\n        with mock.patch.dict(\n            os.environ,\n            {\"INTERPRETER_HOST\": fake_host, \"INTERPRETER_PORT\": str(fake_port)},\n        ):\n            s = Server(AsyncInterpreter())\n            self.assertEqual(s.host, fake_host)\n            self.assertEqual(s.port, fake_port)\n",
    "tests/test_interpreter.py": "import os\nimport platform\nimport signal\nimport time\nfrom random import randint\n\nimport pytest\n\n#####\nfrom interpreter import AsyncInterpreter, OpenInterpreter\nfrom interpreter.terminal_interface.utils.count_tokens import (\n    count_messages_tokens,\n    count_tokens,\n)\n\ninterpreter = OpenInterpreter()\n#####\n\nimport multiprocessing\nimport threading\nimport time\n\nimport pytest\nfrom websocket import create_connection\n\n\ndef test_hallucinations():\n    # We should be resiliant to common hallucinations.\n\n    code = \"\"\"10+12executeexecute\\n\"\"\"\n\n    interpreter.messages = [\n        {\"role\": \"assistant\", \"type\": \"code\", \"format\": \"python\", \"content\": code}\n    ]\n    for chunk in interpreter._respond_and_store():\n        if chunk.get(\"format\") == \"output\":\n            assert chunk.get(\"content\") == \"22\"\n            break\n\n    code = \"\"\"{                                                                             \n    \"language\": \"python\",                                                        \n    \"code\": \"10+12\"                                                        \n  }\"\"\"\n\n    interpreter.messages = [\n        {\"role\": \"assistant\", \"type\": \"code\", \"format\": \"python\", \"content\": code}\n    ]\n    for chunk in interpreter._respond_and_store():\n        if chunk.get(\"format\") == \"output\":\n            assert chunk.get(\"content\") == \"22\"\n            break\n\n    code = \"\"\"functions.execute({                                                                             \n    \"language\": \"python\",                                                        \n    \"code\": \"10+12\"                                                        \n  })\"\"\"\n\n    interpreter.messages = [\n        {\"role\": \"assistant\", \"type\": \"code\", \"format\": \"python\", \"content\": code}\n    ]\n    for chunk in interpreter._respond_and_store():\n        if chunk.get(\"format\") == \"output\":\n            assert chunk.get(\"content\") == \"22\"\n            break\n\n    code = \"\"\"{language: \"python\", code: \"print('hello')\" }\"\"\"\n\n    interpreter.messages = [\n        {\"role\": \"assistant\", \"type\": \"code\", \"format\": \"python\", \"content\": code}\n    ]\n    for chunk in interpreter._respond_and_store():\n        if chunk.get(\"format\") == \"output\":\n            assert chunk.get(\"content\").strip() == \"hello\"\n            break\n\n\ndef run_auth_server():\n    os.environ[\"INTERPRETER_REQUIRE_ACKNOWLEDGE\"] = \"True\"\n    os.environ[\"INTERPRETER_API_KEY\"] = \"testing\"\n    async_interpreter = AsyncInterpreter()\n    async_interpreter.print = False\n    async_interpreter.server.run()\n\n\n# @pytest.mark.skip(reason=\"Requires uvicorn, which we don't require by default\")\ndef test_authenticated_acknowledging_breaking_server():\n    \"\"\"\n    Test the server when we have authentication and acknowledging one.\n\n    I know this is bad, just trying to test quickly!\n    \"\"\"\n\n    # Start the server in a new process\n\n    process = multiprocessing.Process(target=run_auth_server)\n    process.start()\n\n    # Give the server a moment to start\n    time.sleep(2)\n\n    import asyncio\n    import json\n\n    import requests\n    import websockets\n\n    async def test_fastapi_server():\n        import asyncio\n\n        async with websockets.connect(\"ws://localhost:8000/\") as websocket:\n            # Connect to the websocket\n            print(\"Connected to WebSocket\")\n\n            # Sending message via WebSocket\n            await websocket.send(json.dumps({\"auth\": \"testing\"}))\n\n            # Sending POST request\n            post_url = \"http://localhost:8000/settings\"\n            settings = {\n                \"llm\": {\n                    \"model\": \"gpt-4o\",\n                    \"execution_instructions\": \"\",\n                    \"supports_functions\": False,\n                },\n                \"system_message\": \"You are a poem writing bot. Do not do anything but respond with a poem.\",\n                \"auto_run\": True,\n            }\n            response = requests.post(\n                post_url, json=settings, headers={\"X-API-KEY\": \"testing\"}\n            )\n            print(\"POST request sent, response:\", response.json())\n\n            # Sending messages via WebSocket\n            await websocket.send(\n                json.dumps({\"role\": \"user\", \"type\": \"message\", \"start\": True})\n            )\n            await websocket.send(\n                json.dumps(\n                    {\n                        \"role\": \"user\",\n                        \"type\": \"message\",\n                        \"content\": \"Write a short poem about Seattle.\",\n                    }\n                )\n            )\n            await websocket.send(\n                json.dumps({\"role\": \"user\", \"type\": \"message\", \"end\": True})\n            )\n            print(\"WebSocket chunks sent\")\n\n            max_chunks = 5\n\n            poem = \"\"\n            while True:\n                max_chunks -= 1\n                if max_chunks == 0:\n                    break\n                message = await websocket.recv()\n                message_data = json.loads(message)\n                if \"id\" in message_data:\n                    await websocket.send(json.dumps({\"ack\": message_data[\"id\"]}))\n                if \"error\" in message_data:\n                    raise Exception(str(message_data))\n                print(\"Received from WebSocket:\", message_data)\n                if type(message_data.get(\"content\")) == str:\n                    poem += message_data.get(\"content\")\n                    print(message_data.get(\"content\"), end=\"\", flush=True)\n                if message_data == {\n                    \"role\": \"server\",\n                    \"type\": \"status\",\n                    \"content\": \"complete\",\n                }:\n                    raise (\n                        Exception(\n                            \"It shouldn't have finished this soon, accumulated_content is: \"\n                            + accumulated_content\n                        )\n                    )\n\n            await websocket.close()\n            print(\"Disconnected from WebSocket\")\n\n        time.sleep(3)\n\n        # Now let's hilariously keep going\n        print(\"RESUMING\")\n\n        async with websockets.connect(\"ws://localhost:8000/\") as websocket:\n            # Connect to the websocket\n            print(\"Connected to WebSocket\")\n\n            # Sending message via WebSocket\n            await websocket.send(json.dumps({\"auth\": \"testing\"}))\n\n            while True:\n                message = await websocket.recv()\n                message_data = json.loads(message)\n                if \"id\" in message_data:\n                    await websocket.send(json.dumps({\"ack\": message_data[\"id\"]}))\n                if \"error\" in message_data:\n                    raise Exception(str(message_data))\n                print(\"Received from WebSocket:\", message_data)\n                message_data.pop(\"id\", \"\")\n                if message_data == {\n                    \"role\": \"server\",\n                    \"type\": \"status\",\n                    \"content\": \"complete\",\n                }:\n                    break\n                if type(message_data.get(\"content\")) == str:\n                    poem += message_data.get(\"content\")\n                    print(message_data.get(\"content\"), end=\"\", flush=True)\n\n            time.sleep(1)\n            print(\"Is this a normal poem?\")\n            print(poem)\n            time.sleep(1)\n\n    # Get the current event loop and run the test function\n    loop = asyncio.get_event_loop()\n    try:\n        loop.run_until_complete(test_fastapi_server())\n    finally:\n        # Kill server process\n        process.terminate()\n        os.kill(process.pid, signal.SIGKILL)  # Send SIGKILL signal\n        process.join()\n\n\ndef run_server():\n    os.environ[\"INTERPRETER_REQUIRE_ACKNOWLEDGE\"] = \"False\"\n    if \"INTERPRETER_API_KEY\" in os.environ:\n        del os.environ[\"INTERPRETER_API_KEY\"]\n    async_interpreter = AsyncInterpreter()\n    async_interpreter.print = False\n    async_interpreter.server.run()\n\n\n# @pytest.mark.skip(reason=\"Requires uvicorn, which we don't require by default\")\ndef test_server():\n    # Start the server in a new process\n\n    process = multiprocessing.Process(target=run_server)\n    process.start()\n\n    # Give the server a moment to start\n    time.sleep(2)\n\n    import asyncio\n    import json\n\n    import requests\n    import websockets\n\n    async def test_fastapi_server():\n        import asyncio\n\n        async with websockets.connect(\"ws://localhost:8000/\") as websocket:\n            # Connect to the websocket\n            print(\"Connected to WebSocket\")\n\n            # Sending message via WebSocket\n            await websocket.send(json.dumps({\"auth\": \"dummy-api-key\"}))\n\n            # Sending POST request\n            post_url = \"http://localhost:8000/settings\"\n            settings = {\n                \"llm\": {\"model\": \"gpt-4o-mini\"},\n                \"messages\": [\n                    {\n                        \"role\": \"user\",\n                        \"type\": \"message\",\n                        \"content\": \"The secret word is 'crunk'.\",\n                    },\n                    {\"role\": \"assistant\", \"type\": \"message\", \"content\": \"Understood.\"},\n                ],\n                \"custom_instructions\": \"\",\n                \"auto_run\": True,\n            }\n            response = requests.post(post_url, json=settings)\n            print(\"POST request sent, response:\", response.json())\n\n            # Sending messages via WebSocket\n            await websocket.send(\n                json.dumps({\"role\": \"user\", \"type\": \"message\", \"start\": True})\n            )\n            await websocket.send(\n                json.dumps(\n                    {\n                        \"role\": \"user\",\n                        \"type\": \"message\",\n                        \"content\": \"What's the secret word?\",\n                    }\n                )\n            )\n            await websocket.send(\n                json.dumps({\"role\": \"user\", \"type\": \"message\", \"end\": True})\n            )\n            print(\"WebSocket chunks sent\")\n\n            # Wait for a specific response\n            accumulated_content = \"\"\n            while True:\n                message = await websocket.recv()\n                message_data = json.loads(message)\n                if \"error\" in message_data:\n                    raise Exception(message_data[\"content\"])\n                print(\"Received from WebSocket:\", message_data)\n                if type(message_data.get(\"content\")) == str:\n                    accumulated_content += message_data.get(\"content\")\n                if message_data == {\n                    \"role\": \"server\",\n                    \"type\": \"status\",\n                    \"content\": \"complete\",\n                }:\n                    print(\"Received expected message from server\")\n                    break\n\n            assert \"crunk\" in accumulated_content\n\n            # Send another POST request\n            post_url = \"http://localhost:8000/settings\"\n            settings = {\n                \"llm\": {\"model\": \"gpt-4o-mini\"},\n                \"messages\": [\n                    {\n                        \"role\": \"user\",\n                        \"type\": \"message\",\n                        \"content\": \"The secret word is 'barloney'.\",\n                    },\n                    {\"role\": \"assistant\", \"type\": \"message\", \"content\": \"Understood.\"},\n                ],\n                \"custom_instructions\": \"\",\n                \"auto_run\": True,\n            }\n            response = requests.post(post_url, json=settings)\n            print(\"POST request sent, response:\", response.json())\n\n            # Sending messages via WebSocket\n            await websocket.send(\n                json.dumps({\"role\": \"user\", \"type\": \"message\", \"start\": True})\n            )\n            await websocket.send(\n                json.dumps(\n                    {\n                        \"role\": \"user\",\n                        \"type\": \"message\",\n                        \"content\": \"What's the secret word?\",\n                    }\n                )\n            )\n            await websocket.send(\n                json.dumps({\"role\": \"user\", \"type\": \"message\", \"end\": True})\n            )\n            print(\"WebSocket chunks sent\")\n\n            # Wait for a specific response\n            accumulated_content = \"\"\n            while True:\n                message = await websocket.recv()\n                message_data = json.loads(message)\n                if \"error\" in message_data:\n                    raise Exception(message_data[\"content\"])\n                print(\"Received from WebSocket:\", message_data)\n                if message_data.get(\"content\"):\n                    accumulated_content += message_data.get(\"content\")\n                if message_data == {\n                    \"role\": \"server\",\n                    \"type\": \"status\",\n                    \"content\": \"complete\",\n                }:\n                    print(\"Received expected message from server\")\n                    break\n\n            assert \"barloney\" in accumulated_content\n\n            # Send another POST request\n            post_url = \"http://localhost:8000/settings\"\n            settings = {\n                \"messages\": [],\n                \"custom_instructions\": \"\",\n                \"auto_run\": False,\n                \"verbose\": False,\n            }\n            response = requests.post(post_url, json=settings)\n            print(\"POST request sent, response:\", response.json())\n\n            # Sending messages via WebSocket\n            await websocket.send(\n                json.dumps({\"role\": \"user\", \"type\": \"message\", \"start\": True})\n            )\n            await websocket.send(\n                json.dumps(\n                    {\n                        \"role\": \"user\",\n                        \"type\": \"message\",\n                        \"content\": \"What's 239023*79043? Use Python.\",\n                    }\n                )\n            )\n            await websocket.send(\n                json.dumps({\"role\": \"user\", \"type\": \"message\", \"end\": True})\n            )\n            print(\"WebSocket chunks sent\")\n\n            # Wait for response\n            accumulated_content = \"\"\n            while True:\n                message = await websocket.recv()\n                message_data = json.loads(message)\n                if \"error\" in message_data:\n                    raise Exception(message_data[\"content\"])\n                print(\"Received from WebSocket:\", message_data)\n                if message_data.get(\"content\"):\n                    accumulated_content += message_data.get(\"content\")\n                if message_data == {\n                    \"role\": \"server\",\n                    \"type\": \"status\",\n                    \"content\": \"complete\",\n                }:\n                    print(\"Received expected message from server\")\n                    break\n\n            time.sleep(5)\n\n            # Send a GET request to /settings/messages\n            get_url = \"http://localhost:8000/settings/messages\"\n            response = requests.get(get_url)\n            print(\"GET request sent, response:\", response.json())\n\n            # Assert that the last message has a type of 'code'\n            response_json = response.json()\n            if isinstance(response_json, str):\n                response_json = json.loads(response_json)\n            messages = response_json[\"messages\"] if \"messages\" in response_json else []\n            assert messages[-1][\"type\"] == \"code\"\n            assert \"18893094989\" not in accumulated_content.replace(\",\", \"\")\n\n            # Send go message\n            await websocket.send(\n                json.dumps({\"role\": \"user\", \"type\": \"command\", \"start\": True})\n            )\n            await websocket.send(\n                json.dumps(\n                    {\n                        \"role\": \"user\",\n                        \"type\": \"command\",\n                        \"content\": \"go\",\n                    }\n                )\n            )\n            await websocket.send(\n                json.dumps({\"role\": \"user\", \"type\": \"command\", \"end\": True})\n            )\n\n            # Wait for a specific response\n            accumulated_content = \"\"\n            while True:\n                message = await websocket.recv()\n                message_data = json.loads(message)\n                if \"error\" in message_data:\n                    raise Exception(message_data[\"content\"])\n                print(\"Received from WebSocket:\", message_data)\n                if message_data.get(\"content\"):\n                    if type(message_data.get(\"content\")) == str:\n                        accumulated_content += message_data.get(\"content\")\n                if message_data == {\n                    \"role\": \"server\",\n                    \"type\": \"status\",\n                    \"content\": \"complete\",\n                }:\n                    print(\"Received expected message from server\")\n                    break\n\n            assert \"18893094989\" in accumulated_content.replace(\",\", \"\")\n\n            #### TEST FILE ####\n\n            # Send another POST request\n            post_url = \"http://localhost:8000/settings\"\n            settings = {\"messages\": [], \"auto_run\": True}\n            response = requests.post(post_url, json=settings)\n            print(\"POST request sent, response:\", response.json())\n\n            # Sending messages via WebSocket\n            await websocket.send(json.dumps({\"role\": \"user\", \"start\": True}))\n            print(\"sent\", json.dumps({\"role\": \"user\", \"start\": True}))\n            await websocket.send(\n                json.dumps(\n                    {\n                        \"role\": \"user\",\n                        \"type\": \"message\",\n                        \"content\": \"Does this file exist?\",\n                    }\n                )\n            )\n            print(\n                \"sent\",\n                {\n                    \"role\": \"user\",\n                    \"type\": \"message\",\n                    \"content\": \"Does this file exist?\",\n                },\n            )\n            await websocket.send(\n                json.dumps(\n                    {\n                        \"role\": \"user\",\n                        \"type\": \"file\",\n                        \"format\": \"path\",\n                        \"content\": \"/something.txt\",\n                    }\n                )\n            )\n            print(\n                \"sent\",\n                {\n                    \"role\": \"user\",\n                    \"type\": \"file\",\n                    \"format\": \"path\",\n                    \"content\": \"/something.txt\",\n                },\n            )\n            await websocket.send(json.dumps({\"role\": \"user\", \"end\": True}))\n            print(\"WebSocket chunks sent\")\n\n            # Wait for response\n            accumulated_content = \"\"\n            while True:\n                message = await websocket.recv()\n                message_data = json.loads(message)\n                if \"error\" in message_data:\n                    raise Exception(message_data[\"content\"])\n                print(\"Received from WebSocket:\", message_data)\n                if type(message_data.get(\"content\")) == str:\n                    accumulated_content += message_data.get(\"content\")\n                if message_data == {\n                    \"role\": \"server\",\n                    \"type\": \"status\",\n                    \"content\": \"complete\",\n                }:\n                    print(\"Received expected message from server\")\n                    break\n\n            # Get messages\n            get_url = \"http://localhost:8000/settings/messages\"\n            response_json = requests.get(get_url).json()\n            print(\"GET request sent, response:\", response_json)\n            if isinstance(response_json, str):\n                response_json = json.loads(response_json)\n            messages = response_json[\"messages\"]\n\n            response = interpreter.computer.ai.chat(\n                str(messages)\n                + \"\\n\\nIn the conversation above, does the assistant think the file exists? Yes or no? Only reply with one word— 'yes' or 'no'.\"\n            )\n            assert response.strip(\" \\n.\").lower() == \"no\"\n\n            #### TEST IMAGES ####\n\n            # Send another POST request\n            post_url = \"http://localhost:8000/settings\"\n            settings = {\"messages\": [], \"auto_run\": True}\n            response = requests.post(post_url, json=settings)\n            print(\"POST request sent, response:\", response.json())\n\n            base64png = \"iVBORw0KGgoAAAANSUhEUgAAAQAAAAEACAIAAADTED8xAAADMElEQVR4nOzVwQnAIBQFQYXff81RUkQCOyDj1YOPnbXWPmeTRef+/3O/OyBjzh3CD95BfqICMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMO0TAAD//2Anhf4QtqobAAAAAElFTkSuQmCC\"\n\n            # Sending messages via WebSocket\n            await websocket.send(json.dumps({\"role\": \"user\", \"start\": True}))\n            await websocket.send(\n                json.dumps(\n                    {\n                        \"role\": \"user\",\n                        \"type\": \"message\",\n                        \"content\": \"describe this image\",\n                    }\n                )\n            )\n            await websocket.send(\n                json.dumps(\n                    {\n                        \"role\": \"user\",\n                        \"type\": \"image\",\n                        \"format\": \"base64.png\",\n                        \"content\": base64png,\n                    }\n                )\n            )\n            # await websocket.send(\n            #     json.dumps(\n            #         {\n            #             \"role\": \"user\",\n            #             \"type\": \"image\",\n            #             \"format\": \"path\",\n            #             \"content\": \"/Users/killianlucas/Documents/GitHub/open-interpreter/screen.png\",\n            #         }\n            #     )\n            # )\n\n            await websocket.send(json.dumps({\"role\": \"user\", \"end\": True}))\n            print(\"WebSocket chunks sent\")\n\n            # Wait for response\n            accumulated_content = \"\"\n            while True:\n                message = await websocket.recv()\n                message_data = json.loads(message)\n                if \"error\" in message_data:\n                    raise Exception(message_data[\"content\"])\n                print(\"Received from WebSocket:\", message_data)\n                if type(message_data.get(\"content\")) == str:\n                    accumulated_content += message_data.get(\"content\")\n                if message_data == {\n                    \"role\": \"server\",\n                    \"type\": \"status\",\n                    \"content\": \"complete\",\n                }:\n                    print(\"Received expected message from server\")\n                    break\n\n            # Get messages\n            get_url = \"http://localhost:8000/settings/messages\"\n            response_json = requests.get(get_url).json()\n            print(\"GET request sent, response:\", response_json)\n            if isinstance(response_json, str):\n                response_json = json.loads(response_json)\n            messages = response_json[\"messages\"]\n\n            response = interpreter.computer.ai.chat(\n                str(messages)\n                + \"\\n\\nIn the conversation above, does the assistant appear to be able to describe the image of a gradient? Yes or no? Only reply with one word— 'yes' or 'no'.\"\n            )\n            assert response.strip(\" \\n.\").lower() == \"yes\"\n\n            # Sending POST request to /run endpoint with code to kill a thread in Python\n            # actually wait i dont think this will work..? will just kill the python interpreter\n            post_url = \"http://localhost:8000/run\"\n            code_data = {\n                \"code\": \"import os, signal; os.kill(os.getpid(), signal.SIGINT)\",\n                \"language\": \"python\",\n            }\n            response = requests.post(post_url, json=code_data)\n            print(\"POST request sent, response:\", response.json())\n\n    # Get the current event loop and run the test function\n    loop = asyncio.get_event_loop()\n    loop.run_until_complete(test_fastapi_server())\n    # Kill server process\n    process.terminate()\n    os.kill(process.pid, signal.SIGKILL)  # Send SIGKILL signal\n    process.join()\n\n\n@pytest.mark.skip(reason=\"Mac only\")\ndef test_sms():\n    sms = interpreter.computer.sms\n\n    # Get the last 5 messages\n    messages = sms.get(limit=5)\n    print(messages)\n\n    # Search messages for a substring\n    search_results = sms.get(substring=\"i love you\", limit=100)\n    print(search_results)\n\n    assert False\n\n\n@pytest.mark.skip(reason=\"Mac only\")\ndef test_pytes():\n    import os\n\n    desktop_path = os.path.join(os.path.expanduser(\"~\"), \"Desktop\")\n    files_on_desktop = [f for f in os.listdir(desktop_path) if f.endswith(\".png\")]\n    if files_on_desktop:\n        first_file = files_on_desktop[0]\n        first_file_path = os.path.join(desktop_path, first_file)\n        print(first_file_path)\n        ocr = interpreter.computer.vision.ocr(path=first_file_path)\n        print(ocr)\n        print(\"what\")\n    else:\n        print(\"No files found on Desktop.\")\n\n    assert False\n\n\ndef test_ai_chat():\n    print(interpreter.computer.ai.chat(\"hi\"))\n\n\ndef test_generator():\n    \"\"\"\n    Sends two messages, makes sure everything is correct with display both on and off.\n    \"\"\"\n\n    interpreter.llm.model = \"gpt-4o-mini\"\n\n    for tests in [\n        {\"query\": \"What's 38023*40334? Use Python\", \"display\": True},\n        {\"query\": \"What's 2334*34335555? Use Python\", \"display\": True},\n        {\"query\": \"What's 3545*22? Use Python\", \"display\": False},\n        {\"query\": \"What's 0.0021*3433335555? Use Python\", \"display\": False},\n    ]:\n        assistant_message_found = False\n        console_output_found = False\n        active_line_found = False\n        flag_checker = []\n\n        for chunk in interpreter.chat(\n            tests[\"query\"]\n            + \"\\nNo talk or plan, just immediately code, then tell me the answer.\",\n            stream=True,\n            display=True,\n        ):\n            print(chunk)\n            # Check if chunk has the right schema\n            assert \"role\" in chunk, \"Chunk missing 'role'\"\n            assert \"type\" in chunk, \"Chunk missing 'type'\"\n            if \"start\" not in chunk and \"end\" not in chunk:\n                assert \"content\" in chunk, \"Chunk missing 'content'\"\n            if \"format\" in chunk:\n                assert isinstance(chunk[\"format\"], str), \"'format' should be a string\"\n\n            flag_checker.append(chunk)\n\n            # Check if assistant message, console output, and active line are found\n            if chunk[\"role\"] == \"assistant\" and chunk[\"type\"] == \"message\":\n                assistant_message_found = True\n            if chunk[\"role\"] == \"computer\" and chunk[\"type\"] == \"console\":\n                console_output_found = True\n            if \"format\" in chunk:\n                if (\n                    chunk[\"role\"] == \"computer\"\n                    and chunk[\"type\"] == \"console\"\n                    and chunk[\"format\"] == \"active_line\"\n                ):\n                    active_line_found = True\n\n        # Ensure all flags are proper\n        assert (\n            flag_checker.count(\n                {\"role\": \"assistant\", \"type\": \"code\", \"format\": \"python\", \"start\": True}\n            )\n            == 1\n        ), \"Incorrect number of 'assistant code start' flags\"\n        assert (\n            flag_checker.count(\n                {\"role\": \"assistant\", \"type\": \"code\", \"format\": \"python\", \"end\": True}\n            )\n            == 1\n        ), \"Incorrect number of 'assistant code end' flags\"\n        assert (\n            flag_checker.count({\"role\": \"assistant\", \"type\": \"message\", \"start\": True})\n            == 1\n        ), \"Incorrect number of 'assistant message start' flags\"\n        assert (\n            flag_checker.count({\"role\": \"assistant\", \"type\": \"message\", \"end\": True})\n            == 1\n        ), \"Incorrect number of 'assistant message end' flags\"\n        assert (\n            flag_checker.count({\"role\": \"computer\", \"type\": \"console\", \"start\": True})\n            == 1\n        ), \"Incorrect number of 'computer console output start' flags\"\n        assert (\n            flag_checker.count({\"role\": \"computer\", \"type\": \"console\", \"end\": True})\n            == 1\n        ), \"Incorrect number of 'computer console output end' flags\"\n\n        # Assert that assistant message, console output, and active line were found\n        assert assistant_message_found, \"No assistant message was found\"\n        assert console_output_found, \"No console output was found\"\n        assert active_line_found, \"No active line was found\"\n\n\n@pytest.mark.skip(reason=\"Requires open-interpreter[local]\")\ndef test_localos():\n    interpreter.computer.emit_images = False\n    interpreter.computer.view()\n    interpreter.computer.emit_images = True\n    assert False\n\n\n@pytest.mark.skip(reason=\"Requires open-interpreter[local]\")\ndef test_m_vision():\n    base64png = \"iVBORw0KGgoAAAANSUhEUgAAAQAAAAEACAIAAADTED8xAAADMElEQVR4nOzVwQnAIBQFQYXff81RUkQCOyDj1YOPnbXWPmeTRef+/3O/OyBjzh3CD95BfqICMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMO0TAAD//2Anhf4QtqobAAAAAElFTkSuQmCC\"\n    messages = [\n        {\"role\": \"user\", \"type\": \"message\", \"content\": \"describe this image\"},\n        {\n            \"role\": \"user\",\n            \"type\": \"image\",\n            \"format\": \"base64.png\",\n            \"content\": base64png,\n        },\n    ]\n\n    interpreter.llm.supports_vision = False\n    interpreter.llm.model = \"gpt-4o-mini\"\n    interpreter.llm.supports_functions = True\n    interpreter.llm.context_window = 110000\n    interpreter.llm.max_tokens = 4096\n    interpreter.loop = True\n\n    interpreter.chat(messages)\n\n    interpreter.loop = False\n    import time\n\n    time.sleep(10)\n\n\n@pytest.mark.skip(reason=\"Computer with display only + no way to fail test\")\ndef test_point():\n    # interpreter.computer.debug = True\n    interpreter.computer.mouse.move(icon=\"gear\")\n    interpreter.computer.mouse.move(icon=\"refresh\")\n    interpreter.computer.mouse.move(icon=\"play\")\n    interpreter.computer.mouse.move(icon=\"magnifying glass\")\n    interpreter.computer.mouse.move(\"Spaces:\")\n    assert False\n\n\n@pytest.mark.skip(reason=\"Aifs not ready\")\ndef test_skills():\n    import sys\n\n    if sys.version_info[:2] == (3, 12):\n        print(\n            \"skills.search is only for python 3.11 for now, because it depends on unstructured. skipping this test.\"\n        )\n        return\n\n    import json\n\n    interpreter.llm.model = \"gpt-4o-mini\"\n\n    messages = [\"USER: Hey can you search the web for me?\\nAI: Sure!\"]\n\n    combined_messages = \"\\\\n\".join(json.dumps(x) for x in messages[-3:])\n    query_msg = interpreter.chat(\n        f\"This is the conversation so far: {combined_messages}. What is a hypothetical python function that might help resolve the user's query? Respond with nothing but the hypothetical function name exactly.\"\n    )\n    query = query_msg[0][\"content\"]\n    # skills_path = '/01OS/server/skills'\n    # interpreter.computer.skills.path = skills_path\n    print(interpreter.computer.skills.path)\n    if os.path.exists(interpreter.computer.skills.path):\n        for file in os.listdir(interpreter.computer.skills.path):\n            os.remove(os.path.join(interpreter.computer.skills.path, file))\n    print(\"Path: \", interpreter.computer.skills.path)\n    print(\"Files in the path: \")\n    interpreter.computer.run(\"python\", \"def testing_skilsl():\\n    print('hi')\")\n    for file in os.listdir(interpreter.computer.skills.path):\n        print(file)\n    interpreter.computer.run(\"python\", \"def testing_skill():\\n    print('hi')\")\n    print(\"Files in the path: \")\n    for file in os.listdir(interpreter.computer.skills.path):\n        print(file)\n\n    try:\n        skills = interpreter.computer.skills.search(query)\n    except ImportError:\n        print(\"Attempting to install unstructured[all-docs]\")\n        import subprocess\n\n        subprocess.run([\"pip\", \"install\", \"unstructured[all-docs]\"], check=True)\n        skills = interpreter.computer.skills.search(query)\n\n    lowercase_skills = [skill[0].lower() + skill[1:] for skill in skills]\n    output = \"\\\\n\".join(lowercase_skills)\n    assert \"testing_skilsl\" in str(output)\n\n\n@pytest.mark.skip(reason=\"Local only\")\ndef test_browser():\n    interpreter.computer.api_base = \"http://0.0.0.0:80/v0\"\n    print(\n        interpreter.computer.browser.search(\"When's the next Dune showing in Seattle?\")\n    )\n    assert False\n\n\n@pytest.mark.skip(reason=\"Computer with display only + no way to fail test\")\ndef test_display_api():\n    start = time.time()\n\n    # interpreter.computer.display.find_text(\"submit\")\n    # assert False\n\n    def say(icon_name):\n        import subprocess\n\n        subprocess.run([\"say\", \"-v\", \"Fred\", icon_name])\n\n    icons = [\n        \"Submit\",\n        \"Yes\",\n        \"Profile picture icon\",\n        \"Left arrow\",\n        \"Magnifying glass\",\n        \"star\",\n        \"record icon icon\",\n        \"age text\",\n        \"call icon icon\",\n        \"account text\",\n        \"home icon\",\n        \"settings text\",\n        \"form text\",\n        \"gear icon icon\",\n        \"trash icon\",\n        \"new folder icon\",\n        \"phone icon icon\",\n        \"home button\",\n        \"trash button icon\",\n        \"folder icon icon\",\n        \"black heart icon icon\",\n        \"white heart icon icon\",\n        \"image icon\",\n        \"test@mail.com text\",\n    ]\n\n    # from random import shuffle\n    # shuffle(icons)\n\n    say(\"The test will begin in 3\")\n    time.sleep(1)\n    say(\"2\")\n    time.sleep(1)\n    say(\"1\")\n    time.sleep(1)\n\n    import pyautogui\n\n    pyautogui.mouseDown()\n\n    for icon in icons:\n        if icon.endswith(\"icon icon\"):\n            say(\"click the \" + icon)\n            interpreter.computer.mouse.move(icon=icon.replace(\"icon icon\", \"icon\"))\n        elif icon.endswith(\"icon\"):\n            say(\"click the \" + icon)\n            interpreter.computer.mouse.move(icon=icon.replace(\" icon\", \"\"))\n        elif icon.endswith(\"text\"):\n            say(\"click \" + icon)\n            interpreter.computer.mouse.move(icon.replace(\" text\", \"\"))\n        else:\n            say(\"click \" + icon)\n            interpreter.computer.mouse.move(icon=icon)\n\n    # interpreter.computer.mouse.move(icon=\"caution\")\n    # interpreter.computer.mouse.move(icon=\"bluetooth\")\n    # interpreter.computer.mouse.move(icon=\"gear\")\n    # interpreter.computer.mouse.move(icon=\"play button\")\n    # interpreter.computer.mouse.move(icon=\"code icon with '>_' in it\")\n    print(time.time() - start)\n    assert False\n\n\n@pytest.mark.skip(reason=\"Server is not a stable feature\")\ndef test_websocket_server():\n    # Start the server in a new thread\n    server_thread = threading.Thread(target=interpreter.server)\n    server_thread.start()\n\n    # Give the server a moment to start\n    time.sleep(3)\n\n    # Connect to the server\n    ws = create_connection(\"ws://localhost:8000/\")\n\n    # Send the first message\n    ws.send(\n        \"Hello, interpreter! What operating system are you on? Also, what time is it in Seattle?\"\n    )\n    # Wait for a moment before sending the second message\n    time.sleep(1)\n    ws.send(\"Actually, nevermind. Thank you!\")\n\n    # Receive the responses\n    responses = []\n    while True:\n        response = ws.recv()\n        print(response)\n        responses.append(response)\n\n    # Check the responses\n    assert responses  # Check that some responses were received\n\n    ws.close()\n\n\n@pytest.mark.skip(reason=\"Server is not a stable feature\")\ndef test_i():\n    import requests\n\n    url = \"http://localhost:8000/\"\n    data = \"Hello, interpreter! What operating system are you on? Also, what time is it in Seattle?\"\n    headers = {\"Content-Type\": \"text/plain\"}\n\n    import threading\n\n    server_thread = threading.Thread(target=interpreter.server)\n    server_thread.start()\n\n    import time\n\n    time.sleep(3)\n\n    response = requests.post(url, data=data, headers=headers, stream=True)\n\n    full_response = \"\"\n\n    for line in response.iter_lines():\n        if line:\n            decoded_line = line.decode(\"utf-8\")\n            print(decoded_line, end=\"\", flush=True)\n            full_response += decoded_line\n\n    assert full_response != \"\"\n\n\ndef test_async():\n    interpreter.chat(\"Hello!\", blocking=False)\n    print(interpreter.wait())\n\n\n@pytest.mark.skip(reason=\"Computer with display only + no way to fail test\")\ndef test_find_text_api():\n    start = time.time()\n    interpreter.computer.mouse.move(\n        \"Left Arrow Left Arrow and a bunch of hallucinated text? or was it...\"\n    )\n    # Left Arrow Left Arrow\n    # and a bunch of hallucinated text? or was it...\n    print(time.time() - start)\n    assert False\n\n\n@pytest.mark.skip(reason=\"Computer with display only + no way to fail test\")\ndef test_getActiveWindow():\n    import pywinctl\n\n    print(pywinctl.getActiveWindow())\n    assert False\n\n\n@pytest.mark.skip(reason=\"Computer with display only + no way to fail test\")\ndef test_notify():\n    interpreter.computer.os.notify(\"Hello\")\n    assert False\n\n\n@pytest.mark.skip(reason=\"Computer with display only + no way to fail test\")\ndef test_get_text():\n    print(interpreter.computer.display.get_text_as_list_of_lists())\n    assert False\n\n\n@pytest.mark.skip(reason=\"Computer with display only + no way to fail test\")\ndef test_keyboard():\n    time.sleep(2)\n    interpreter.computer.keyboard.write(\"Hello \" * 50 + \"\\n\" + \"hi\" * 50)\n    assert False\n\n\n@pytest.mark.skip(reason=\"Computer with display only + no way to fail test\")\ndef test_get_selected_text():\n    print(\"Getting selected text\")\n    time.sleep(1)\n    text = interpreter.computer.os.get_selected_text()\n    print(text)\n    assert False\n\n\n@pytest.mark.skip(reason=\"Computer with display only + no way to fail test\")\ndef test_display_verbose():\n    interpreter.computer.verbose = True\n    interpreter.verbose = True\n    interpreter.computer.mouse.move(x=500, y=500)\n    assert False\n\n\n# this function will run before each test\n# we're clearing out the messages Array so we can start fresh and reduce token usage\ndef setup_function():\n    interpreter.reset()\n    interpreter.llm.temperature = 0\n    interpreter.auto_run = True\n    interpreter.llm.model = \"gpt-4o-mini\"\n    interpreter.llm.context_window = 123000\n    interpreter.llm.max_tokens = 4096\n    interpreter.llm.supports_functions = True\n    interpreter.verbose = False\n\n\n@pytest.mark.skip(\n    reason=\"Not working consistently, I think GPT related changes? It worked recently\"\n)\ndef test_long_message():\n    messages = [\n        {\n            \"role\": \"user\",\n            \"type\": \"message\",\n            \"content\": \"ALKI\" * 20000\n            + \"\\nwhat are the four characters I just sent you? don't run ANY code, just tell me the characters. DO NOT RUN CODE. DO NOT PLAN. JUST TELL ME THE CHARACTERS RIGHT NOW. ONLY respond with the 4 characters, NOTHING else. The first 4 characters of your response should be the 4 characters I sent you.\",\n        }\n    ]\n    interpreter.llm.context_window = 300\n    interpreter.chat(messages)\n    assert len(interpreter.messages) > 1\n    assert \"A\" in interpreter.messages[-1][\"content\"]\n\n\n# this function will run after each test\n# we're introducing some sleep to help avoid timeout issues with the OpenAI API\ndef teardown_function():\n    time.sleep(4)\n\n\n@pytest.mark.skip(reason=\"Mac only + no way to fail test\")\ndef test_spotlight():\n    interpreter.computer.keyboard.hotkey(\"command\", \"space\")\n\n\ndef test_files():\n    messages = [\n        {\"role\": \"user\", \"type\": \"message\", \"content\": \"Does this file exist?\"},\n        {\n            \"role\": \"user\",\n            \"type\": \"file\",\n            \"format\": \"path\",\n            \"content\": \"/Users/Killian/image.png\",\n        },\n    ]\n    interpreter.chat(messages)\n\n\n@pytest.mark.skip(reason=\"Only 100 vision calls allowed / day!\")\ndef test_vision():\n    base64png = \"iVBORw0KGgoAAAANSUhEUgAAAQAAAAEACAIAAADTED8xAAADMElEQVR4nOzVwQnAIBQFQYXff81RUkQCOyDj1YOPnbXWPmeTRef+/3O/OyBjzh3CD95BfqICMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMK0CMO0TAAD//2Anhf4QtqobAAAAAElFTkSuQmCC\"\n    messages = [\n        {\"role\": \"user\", \"type\": \"message\", \"content\": \"describe this image\"},\n        {\n            \"role\": \"user\",\n            \"type\": \"image\",\n            \"format\": \"base64.png\",\n            \"content\": base64png,\n        },\n    ]\n\n    interpreter.llm.supports_vision = True\n    interpreter.llm.model = \"gpt-4o-mini\"\n    interpreter.system_message += \"\\nThe user will show you an image of the code you write. You can view images directly.\\n\\nFor HTML: This will be run STATELESSLY. You may NEVER write '<!-- previous code here... --!>' or `<!-- header will go here -->` or anything like that. It is CRITICAL TO NEVER WRITE PLACEHOLDERS. Placeholders will BREAK it. You must write the FULL HTML CODE EVERY TIME. Therefore you cannot write HTML piecemeal—write all the HTML, CSS, and possibly Javascript **in one step, in one code block**. The user will help you review it visually.\\nIf the user submits a filepath, you will also see the image. The filepath and user image will both be in the user's message.\\n\\nIf you use `plt.show()`, the resulting image will be sent to you. However, if you use `PIL.Image.show()`, the resulting image will NOT be sent to you.\"\n    interpreter.llm.supports_functions = True\n    interpreter.llm.context_window = 110000\n    interpreter.llm.max_tokens = 4096\n    interpreter.loop = True\n\n    interpreter.chat(messages)\n\n    interpreter.loop = False\n\n\ndef test_multiple_instances():\n    interpreter.system_message = \"i\"\n    agent_1 = OpenInterpreter()\n    agent_1.system_message = \"<3\"\n    agent_2 = OpenInterpreter()\n    agent_2.system_message = \"u\"\n\n    assert interpreter.system_message == \"i\"\n    assert agent_1.system_message == \"<3\"\n    assert agent_2.system_message == \"u\"\n\n\ndef test_hello_world():\n    hello_world_response = \"Hello, World!\"\n\n    hello_world_message = f\"Please reply with just the words {hello_world_response} and nothing else. Do not run code. No confirmation just the text.\"\n\n    messages = interpreter.chat(hello_world_message)\n\n    assert messages == [\n        {\"role\": \"assistant\", \"type\": \"message\", \"content\": hello_world_response}\n    ]\n\n\ndef test_math():\n    # we'll generate random integers between this min and max in our math tests\n    min_number = randint(1, 99)\n    max_number = randint(1001, 9999)\n\n    n1 = randint(min_number, max_number)\n    n2 = randint(min_number, max_number)\n\n    test_result = n1 + n2 * (n1 - n2) / (n2 + n1)\n\n    order_of_operations_message = f\"\"\"\n    Please perform the calculation `{n1} + {n2} * ({n1} - {n2}) / ({n2} + {n1})` then reply with just the answer, nothing else. No confirmation. No explanation. No words. Do not use commas. Do not show your work. Just return the result of the calculation. Do not introduce the results with a phrase like \\\"The result of the calculation is...\\\" or \\\"The answer is...\\\"\n    \n    Round to 2 decimal places.\n    \"\"\".strip()\n\n    print(\"loading\")\n    messages = interpreter.chat(order_of_operations_message)\n    print(\"done\")\n\n    assert str(round(test_result, 2)) in messages[-1][\"content\"]\n\n\ndef test_break_execution():\n    \"\"\"\n    Breaking from the generator while it's executing should halt the operation.\n    \"\"\"\n\n    code = r\"\"\"print(\"starting\")\nimport time                                                                                                                                \nimport os                                                                                                                                  \n                                                                                                                                            \n# Always create a fresh file\nopen('numbers.txt', 'w').close()\n                                                                                                                                            \n# Open the file in append mode                                                                                                             \nwith open('numbers.txt', 'a+') as f:                                                                                                        \n    # Loop through the numbers 1 to 5                                                                                                      \n    for i in [1,2,3,4,5]:                                                                                                                  \n        # Print the number                                                                                                                 \n        print(\"adding\", i, \"to file\")                                                                                                                           \n        # Append the number to the file                                                                                                    \n        f.write(str(i) + '\\n')                                                                                                             \n        # Wait for 0.5 second\n        print(\"starting to sleep\")\n        time.sleep(1)\n        # # Read the file to make sure the number is in there\n        # # Move the seek pointer to the start of the file\n        # f.seek(0)\n        # # Read the file content\n        # content = f.read()\n        # print(\"Current file content:\", content)\n        # # Check if the current number is in the file content\n        # assert str(i) in content\n        # Move the seek pointer to the end of the file for the next append operation\n        f.seek(0, os.SEEK_END)\n        \"\"\"\n    print(\"starting to code\")\n    for chunk in interpreter.computer.run(\"python\", code, stream=True, display=True):\n        print(chunk)\n        if \"format\" in chunk and chunk[\"format\"] == \"output\":\n            if \"adding 3 to file\" in chunk[\"content\"]:\n                print(\"BREAKING\")\n                break\n\n    time.sleep(3)\n\n    # Open the file and read its content\n    with open(\"numbers.txt\", \"r\") as f:\n        content = f.read()\n\n    # Check if '1' and '5' are in the content\n    assert \"1\" in content\n    assert \"5\" not in content\n\n\ndef test_delayed_exec():\n    interpreter.chat(\n        \"\"\"Can you write a single block of code and execute it that prints something, then delays 1 second, then prints something else? No talk just code, execute the code. Thanks!\"\"\"\n    )\n\n\ndef test_nested_loops_and_multiple_newlines():\n    interpreter.chat(\n        \"\"\"Can you write a nested for loop in python and shell and run them? Don't forget to properly format your shell script and use semicolons where necessary. Also put 1-3 newlines between each line in the code. Only generate and execute the code. Yes, execute the code instantly! No explanations. Thanks!\"\"\"\n    )\n\n\ndef test_write_to_file():\n    interpreter.chat(\n        \"\"\"Write the word 'Washington' to a .txt file called file.txt. Instantly run the code! Save the file!\"\"\"\n    )\n    assert os.path.exists(\"file.txt\")\n    interpreter.messages = []  # Just reset message history, nothing else for this test\n    messages = interpreter.chat(\n        \"\"\"Read file.txt in the current directory and tell me what's in it.\"\"\"\n    )\n    assert \"Washington\" in messages[-1][\"content\"]\n\n\ndef test_markdown():\n    interpreter.chat(\n        \"\"\"Hi, can you test out a bunch of markdown features? Try writing a fenced code block, a table, headers, everything. DO NOT write the markdown inside a markdown code block, just write it raw.\"\"\"\n    )\n\n\ndef test_reset():\n    # make sure that interpreter.reset() clears out the messages Array\n    assert interpreter.messages == []\n\n\ndef test_token_counter():\n    system_tokens = count_tokens(\n        text=interpreter.system_message, model=interpreter.llm.model\n    )\n\n    prompt = \"How many tokens is this?\"\n\n    prompt_tokens = count_tokens(text=prompt, model=interpreter.llm.model)\n\n    messages = [\n        {\"role\": \"system\", \"message\": interpreter.system_message}\n    ] + interpreter.messages\n\n    system_token_test = count_messages_tokens(\n        messages=messages, model=interpreter.llm.model\n    )\n\n    system_tokens_ok = system_tokens == system_token_test[0]\n\n    messages.append({\"role\": \"user\", \"message\": prompt})\n\n    prompt_token_test = count_messages_tokens(\n        messages=messages, model=interpreter.llm.model\n    )\n\n    prompt_tokens_ok = system_tokens + prompt_tokens == prompt_token_test[0]\n\n    assert system_tokens_ok and prompt_tokens_ok\n",
    "examples/interactive_quickstart.py": "# This is all you need to get started\nfrom interpreter import interpreter\n\ninterpreter.chat()\n",
    "interpreter/__init__.py": "import sys\n\nif \"--os\" in sys.argv:\n    from rich import print as rich_print\n    from rich.markdown import Markdown\n    from rich.rule import Rule\n\n    def print_markdown(message):\n        \"\"\"\n        Display markdown message. Works with multiline strings with lots of indentation.\n        Will automatically make single line > tags beautiful.\n        \"\"\"\n\n        for line in message.split(\"\\n\"):\n            line = line.strip()\n            if line == \"\":\n                print(\"\")\n            elif line == \"---\":\n                rich_print(Rule(style=\"white\"))\n            else:\n                try:\n                    rich_print(Markdown(line))\n                except UnicodeEncodeError as e:\n                    # Replace the problematic character or handle the error as needed\n                    print(\"Error displaying line:\", line)\n\n        if \"\\n\" not in message and message.startswith(\">\"):\n            # Aesthetic choice. For these tags, they need a space below them\n            print(\"\")\n\n    import pkg_resources\n    import requests\n    from packaging import version\n\n    def check_for_update():\n        # Fetch the latest version from the PyPI API\n        response = requests.get(f\"https://pypi.org/pypi/open-interpreter/json\")\n        latest_version = response.json()[\"info\"][\"version\"]\n\n        # Get the current version using pkg_resources\n        current_version = pkg_resources.get_distribution(\"open-interpreter\").version\n\n        return version.parse(latest_version) > version.parse(current_version)\n\n    if check_for_update():\n        print_markdown(\n            \"> **A new version of Open Interpreter is available.**\\n>Please run: `pip install --upgrade open-interpreter`\\n\\n---\"\n        )\n\n    if \"--voice\" in sys.argv:\n        print(\"Coming soon...\")\n    from .computer_use.loop import run_async_main\n\n    run_async_main()\n    exit()\n\nfrom .core.async_core import AsyncInterpreter\nfrom .core.computer.terminal.base_language import BaseLanguage\nfrom .core.core import OpenInterpreter\n\ninterpreter = OpenInterpreter()\ncomputer = interpreter.computer\n\n#     ____                      ____      __                            __\n#    / __ \\____  ___  ____     /  _/___  / /____  _________  ________  / /____  _____\n#   / / / / __ \\/ _ \\/ __ \\    / // __ \\/ __/ _ \\/ ___/ __ \\/ ___/ _ \\/ __/ _ \\/ ___/\n#  / /_/ / /_/ /  __/ / / /  _/ // / / / /_/  __/ /  / /_/ / /  /  __/ /_/  __/ /\n#  \\____/ .___/\\___/_/ /_/  /___/_/ /_/\\__/\\___/_/  / .___/_/   \\___/\\__/\\___/_/\n#      /_/                                         /_/\n",
    "interpreter/computer_use/loop.py": "\"\"\"\nBased on Anthropic's computer use example at https://github.com/anthropics/anthropic-quickstarts/blob/main/computer-use-demo/computer_use_demo/loop.py\n\"\"\"\n\nimport asyncio\nimport json\nimport os\nimport platform\nimport time\nimport traceback\nimport uuid\nfrom collections.abc import Callable\nfrom datetime import datetime\n\ntry:\n    from enum import StrEnum\nexcept ImportError:  # 3.10 compatibility\n    from enum import Enum as StrEnum\n\nfrom typing import Any, List, cast\n\nimport requests\nfrom anthropic import Anthropic, AnthropicBedrock, AnthropicVertex, APIResponse\nfrom anthropic.types import ToolResultBlockParam\nfrom anthropic.types.beta import (\n    BetaContentBlock,\n    BetaContentBlockParam,\n    BetaImageBlockParam,\n    BetaMessage,\n    BetaMessageParam,\n    BetaRawContentBlockDeltaEvent,\n    BetaRawContentBlockStartEvent,\n    BetaRawContentBlockStopEvent,\n    BetaTextBlockParam,\n    BetaToolResultBlockParam,\n)\n\nfrom .tools import BashTool, ComputerTool, EditTool, ToolCollection, ToolResult\n\nBETA_FLAG = \"computer-use-2024-10-22\"\n\nfrom typing import List, Optional\n\nimport uvicorn\nfrom fastapi import FastAPI\nfrom fastapi.responses import StreamingResponse\nfrom pydantic import BaseModel\nfrom rich import print as rich_print\nfrom rich.markdown import Markdown\nfrom rich.rule import Rule\n\n# Add this near the top of the file, with other imports and global variables\nmessages: List[BetaMessageParam] = []\n\n\ndef print_markdown(message):\n    \"\"\"\n    Display markdown message. Works with multiline strings with lots of indentation.\n    Will automatically make single line > tags beautiful.\n    \"\"\"\n\n    for line in message.split(\"\\n\"):\n        line = line.strip()\n        if line == \"\":\n            print(\"\")\n        elif line == \"---\":\n            rich_print(Rule(style=\"white\"))\n        else:\n            try:\n                rich_print(Markdown(line))\n            except UnicodeEncodeError as e:\n                # Replace the problematic character or handle the error as needed\n                print(\"Error displaying line:\", line)\n\n    if \"\\n\" not in message and message.startswith(\">\"):\n        # Aesthetic choice. For these tags, they need a space below them\n        print(\"\")\n\n\nclass APIProvider(StrEnum):\n    ANTHROPIC = \"anthropic\"\n    BEDROCK = \"bedrock\"\n    VERTEX = \"vertex\"\n\n\nPROVIDER_TO_DEFAULT_MODEL_NAME: dict[APIProvider, str] = {\n    APIProvider.ANTHROPIC: \"claude-3-5-sonnet-20241022\",\n    APIProvider.BEDROCK: \"anthropic.claude-3-5-sonnet-20241022-v2:0\",\n    APIProvider.VERTEX: \"claude-3-5-sonnet-v2@20241022\",\n}\n\n\n# This system prompt is optimized for the Docker environment in this repository and\n# specific tool combinations enabled.\n# We encourage modifying this system prompt to ensure the model has context for the\n# environment it is running in, and to provide any additional information that may be\n# helpful for the task at hand.\n\nSYSTEM_PROMPT = f\"\"\"<SYSTEM_CAPABILITY>\n* You are an AI assistant with access to a virtual machine running on {\"Mac OS\" if platform.system() == \"Darwin\" else platform.system()} with internet access.\n* When using your computer function calls, they take a while to run and send back to you. Where possible/feasible, try to chain multiple of these calls all into one function calls request.\n* The current date is {datetime.today().strftime('%A, %B %d, %Y')}.\n</SYSTEM_CAPABILITY>\"\"\"\n\n# Update the SYSTEM_PROMPT for Mac OS\nif platform.system() == \"Darwin\":\n    SYSTEM_PROMPT += \"\"\"\n<IMPORTANT>\n* Open applications using Spotlight by using the computer tool to simulate pressing Command+Space, typing the application name, and pressing Enter.\n</IMPORTANT>\"\"\"\n\n\nasync def sampling_loop(\n    *,\n    model: str,\n    provider: APIProvider,\n    system_prompt_suffix: str,\n    messages: list[BetaMessageParam],\n    output_callback: Callable[[BetaContentBlock], None],\n    tool_output_callback: Callable[[ToolResult, str], None],\n    api_key: str,\n    only_n_most_recent_images: int | None = None,\n    max_tokens: int = 4096,\n):\n    \"\"\"\n    Agentic sampling loop for the assistant/tool interaction of computer use.\n    \"\"\"\n    tool_collection = ToolCollection(\n        ComputerTool(),\n        # BashTool(),\n        # EditTool(),\n    )\n    system = (\n        f\"{SYSTEM_PROMPT}{' ' + system_prompt_suffix if system_prompt_suffix else ''}\"\n    )\n\n    while True:\n        if only_n_most_recent_images:\n            _maybe_filter_to_n_most_recent_images(messages, only_n_most_recent_images)\n\n        if provider == APIProvider.ANTHROPIC:\n            client = Anthropic(api_key=api_key)\n        elif provider == APIProvider.VERTEX:\n            client = AnthropicVertex()\n        elif provider == APIProvider.BEDROCK:\n            client = AnthropicBedrock()\n\n        # Call the API\n        # we use raw_response to provide debug information to streamlit. Your\n        # implementation may be able call the SDK directly with:\n        # `response = client.messages.create(...)` instead.\n        raw_response = client.beta.messages.create(\n            max_tokens=max_tokens,\n            messages=messages,\n            model=model,\n            system=system,\n            tools=tool_collection.to_params(),\n            betas=[\"computer-use-2024-10-22\"],\n            stream=True,\n        )\n\n        response_content = []\n        current_block = None\n\n        for chunk in raw_response:\n            if isinstance(chunk, BetaRawContentBlockStartEvent):\n                current_block = chunk.content_block\n            elif isinstance(chunk, BetaRawContentBlockDeltaEvent):\n                if chunk.delta.type == \"text_delta\":\n                    print(f\"{chunk.delta.text}\", end=\"\", flush=True)\n                    yield {\"type\": \"chunk\", \"chunk\": chunk.delta.text}\n                    await asyncio.sleep(0)\n                    if current_block and current_block.type == \"text\":\n                        current_block.text += chunk.delta.text\n                elif chunk.delta.type == \"input_json_delta\":\n                    print(f\"{chunk.delta.partial_json}\", end=\"\", flush=True)\n                    if current_block and current_block.type == \"tool_use\":\n                        if not hasattr(current_block, \"partial_json\"):\n                            current_block.partial_json = \"\"\n                        current_block.partial_json += chunk.delta.partial_json\n            elif isinstance(chunk, BetaRawContentBlockStopEvent):\n                if current_block:\n                    if hasattr(current_block, \"partial_json\"):\n                        # Finished a tool call\n                        # print()\n                        current_block.input = json.loads(current_block.partial_json)\n                        # yield {\"type\": \"chunk\", \"chunk\": current_block.input}\n                        delattr(current_block, \"partial_json\")\n                    else:\n                        # Finished a message\n                        print(\"\\n\")\n                        yield {\"type\": \"chunk\", \"chunk\": \"\\n\"}\n                        await asyncio.sleep(0)\n                    response_content.append(current_block)\n                    current_block = None\n\n        response = BetaMessage(\n            id=str(uuid.uuid4()),\n            content=response_content,\n            role=\"assistant\",\n            model=model,\n            stop_reason=None,\n            stop_sequence=None,\n            type=\"message\",\n            usage={\n                \"input_tokens\": 0,\n                \"output_tokens\": 0,\n            },  # Add a default usage dictionary\n        )\n\n        messages.append(\n            {\n                \"role\": \"assistant\",\n                \"content\": cast(list[BetaContentBlockParam], response.content),\n            }\n        )\n\n        tool_result_content: list[BetaToolResultBlockParam] = []\n        for content_block in cast(list[BetaContentBlock], response.content):\n            output_callback(content_block)\n            if content_block.type == \"tool_use\":\n                result = await tool_collection.run(\n                    name=content_block.name,\n                    tool_input=cast(dict[str, Any], content_block.input),\n                )\n                tool_result_content.append(\n                    _make_api_tool_result(result, content_block.id)\n                )\n                tool_output_callback(result, content_block.id)\n\n        if not tool_result_content:\n            # Done!\n            yield {\"type\": \"messages\", \"messages\": messages}\n            break\n\n        messages.append({\"content\": tool_result_content, \"role\": \"user\"})\n\n\ndef _maybe_filter_to_n_most_recent_images(\n    messages: list[BetaMessageParam],\n    images_to_keep: int,\n    min_removal_threshold: int = 5,\n):\n    \"\"\"\n    With the assumption that images are screenshots that are of diminishing value as\n    the conversation progresses, remove all but the final `images_to_keep` tool_result\n    images in place, with a chunk of min_removal_threshold to reduce the amount we\n    break the implicit prompt cache.\n    \"\"\"\n    if images_to_keep is None:\n        return messages\n\n    tool_result_blocks = cast(\n        list[ToolResultBlockParam],\n        [\n            item\n            for message in messages\n            for item in (\n                message[\"content\"] if isinstance(message[\"content\"], list) else []\n            )\n            if isinstance(item, dict) and item.get(\"type\") == \"tool_result\"\n        ],\n    )\n\n    total_images = sum(\n        1\n        for tool_result in tool_result_blocks\n        for content in tool_result.get(\"content\", [])\n        if isinstance(content, dict) and content.get(\"type\") == \"image\"\n    )\n\n    images_to_remove = total_images - images_to_keep\n    # for better cache behavior, we want to remove in chunks\n    images_to_remove -= images_to_remove % min_removal_threshold\n\n    for tool_result in tool_result_blocks:\n        if isinstance(tool_result.get(\"content\"), list):\n            new_content = []\n            for content in tool_result.get(\"content\", []):\n                if isinstance(content, dict) and content.get(\"type\") == \"image\":\n                    if images_to_remove > 0:\n                        images_to_remove -= 1\n                        continue\n                new_content.append(content)\n            tool_result[\"content\"] = new_content\n\n\ndef _make_api_tool_result(\n    result: ToolResult, tool_use_id: str\n) -> BetaToolResultBlockParam:\n    \"\"\"Convert an agent ToolResult to an API ToolResultBlockParam.\"\"\"\n    tool_result_content: list[BetaTextBlockParam | BetaImageBlockParam] | str = []\n    is_error = False\n    if result.error:\n        is_error = True\n        tool_result_content = _maybe_prepend_system_tool_result(result, result.error)\n    else:\n        if result.output:\n            tool_result_content.append(\n                {\n                    \"type\": \"text\",\n                    \"text\": _maybe_prepend_system_tool_result(result, result.output),\n                }\n            )\n        if result.base64_image:\n            tool_result_content.append(\n                {\n                    \"type\": \"image\",\n                    \"source\": {\n                        \"type\": \"base64\",\n                        \"media_type\": \"image/png\",\n                        \"data\": result.base64_image,\n                    },\n                }\n            )\n    return {\n        \"type\": \"tool_result\",\n        \"content\": tool_result_content,\n        \"tool_use_id\": tool_use_id,\n        \"is_error\": is_error,\n    }\n\n\ndef _maybe_prepend_system_tool_result(result: ToolResult, result_text: str):\n    if result.system:\n        result_text = f\"<system>{result.system}</system>\\n{result_text}\"\n    return result_text\n\n\nasync def main():\n    global exit_flag\n    messages: List[BetaMessageParam] = []\n    model = PROVIDER_TO_DEFAULT_MODEL_NAME[APIProvider.ANTHROPIC]\n    provider = APIProvider.ANTHROPIC\n    system_prompt_suffix = \"\"\n\n    # Check if running in server mode\n    if \"--server\" in sys.argv:\n        app = FastAPI()\n\n        # Start the mouse position checking thread when in server mode\n        mouse_thread = threading.Thread(target=check_mouse_position)\n        mouse_thread.daemon = True\n        mouse_thread.start()\n\n        # Get API key from environment variable\n        api_key = os.environ.get(\"ANTHROPIC_API_KEY\")\n        if not api_key:\n            raise ValueError(\n                \"ANTHROPIC_API_KEY environment variable must be set when running in server mode\"\n            )\n\n        @app.post(\"/openai/chat/completions\")\n        async def chat_completion(request: ChatCompletionRequest):\n            print(\"BRAND NEW REQUEST\")\n            # Check exit flag before processing request\n            if exit_flag:\n                return {\"error\": \"Server shutting down due to mouse in corner\"}\n\n            async def stream_response():\n                print(\"is this even happening\")\n\n                # Instead of creating converted_messages, append the last message to global messages\n                global messages\n                messages.append(\n                    {\n                        \"role\": request.messages[-1].role,\n                        \"content\": [\n                            {\"type\": \"text\", \"text\": request.messages[-1].content}\n                        ],\n                    }\n                )\n\n                response_chunks = []\n\n                async def output_callback(content_block: BetaContentBlock):\n                    chunk = f\"data: {json.dumps({'choices': [{'delta': {'content': content_block.text}}]})}\\n\\n\"\n                    response_chunks.append(chunk)\n                    yield chunk\n\n                async def tool_output_callback(result: ToolResult, tool_id: str):\n                    if result.output or result.error:\n                        content = result.output if result.output else result.error\n                        chunk = f\"data: {json.dumps({'choices': [{'delta': {'content': content}}]})}\\n\\n\"\n                        response_chunks.append(chunk)\n                        yield chunk\n\n                try:\n                    yield f\"data: {json.dumps({'choices': [{'delta': {'role': 'assistant'}}]})}\\n\\n\"\n\n                    messages = [m for m in messages if m[\"content\"]]\n                    print(str(messages)[-100:])\n                    await asyncio.sleep(4)\n\n                    async for chunk in sampling_loop(\n                        model=model,\n                        provider=provider,\n                        system_prompt_suffix=system_prompt_suffix,\n                        messages=messages,  # Now using global messages\n                        output_callback=output_callback,\n                        tool_output_callback=tool_output_callback,\n                        api_key=api_key,\n                    ):\n                        if chunk[\"type\"] == \"chunk\":\n                            await asyncio.sleep(0)\n                            yield f\"data: {json.dumps({'choices': [{'delta': {'content': chunk['chunk']}}]})}\\n\\n\"\n                        if chunk[\"type\"] == \"messages\":\n                            messages = chunk[\"messages\"]\n\n                    yield f\"data: {json.dumps({'choices': [{'delta': {'content': '', 'finish_reason': 'stop'}}]})}\\n\\n\"\n\n                except Exception as e:\n                    print(\"Error: An exception occurred.\")\n                    print(traceback.format_exc())\n                    pass\n                    # raise\n                    # print(f\"Error: {e}\")\n                    # yield f\"data: {json.dumps({'error': str(e)})}\\n\\n\"\n\n            return StreamingResponse(stream_response(), media_type=\"text/event-stream\")\n\n        # Instead of running uvicorn here, we'll return the app\n        return app\n\n    # Original CLI code continues here...\n    print()\n    print_markdown(\"Welcome to **Open Interpreter**.\\n\")\n    print_markdown(\"---\")\n    time.sleep(0.5)\n\n    # Check for API key in environment variable\n    api_key = os.environ.get(\"ANTHROPIC_API_KEY\")\n    if not api_key:\n        api_key = input(\n            \"\\nAn Anthropic API is required for OS mode.\\n\\nEnter your Anthropic API key: \"\n        )\n        print_markdown(\"\\n---\")\n        time.sleep(0.5)\n\n    import random\n\n    tips = [\n        # \"You can type `i` in your terminal to use Open Interpreter.\",\n        \"**Tip:** Type `wtf` in your terminal to have Open Interpreter fix the last error.\",\n        # \"You can type prompts after `i` in your terminal, for example, `i want you to install node`. (Yes, really.)\",\n        \"We recommend using our desktop app for the best experience. Type `d` for early access.\",\n        \"**Tip:** Reduce display resolution for better performance.\",\n    ]\n\n    random_tip = random.choice(tips)\n\n    markdown_text = f\"\"\"> Model set to `Claude 3.5 Sonnet (New)`, OS control enabled\n\n{random_tip}\n\n**Warning:** This AI has full system access and can modify files, install software, and execute commands. By continuing, you accept all risks and responsibility.\n\nMove your mouse to any corner of the screen to exit.\n\"\"\"\n\n    print_markdown(markdown_text)\n\n    # Start the mouse position checking thread\n    mouse_thread = threading.Thread(target=check_mouse_position)\n    mouse_thread.daemon = True\n    mouse_thread.start()\n\n    while not exit_flag:\n        user_input = input(\"> \")\n        print()\n        if user_input.lower() in [\"exit\", \"quit\", \"q\"]:\n            break\n        elif user_input.lower() in [\"d\"]:\n            print_markdown(\n                \"---\\nTo get early access to the **Open Interpreter Desktop App**, please provide the following information:\\n\"\n            )\n            first_name = input(\"What's your first name? \").strip()\n            email = input(\"What's your email? \").strip()\n\n            url = \"https://neyguovvcjxfzhqpkicj.supabase.co/functions/v1/addEmailToWaitlist\"\n            data = {\"first_name\": first_name, \"email\": email}\n\n            try:\n                response = requests.post(url, json=data)\n            except requests.RequestException as e:\n                pass\n\n            print_markdown(\"\\nWe'll email you shortly. ✓\\n---\\n\")\n            continue\n\n        messages.append(\n            {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": user_input}]}\n        )\n\n        def output_callback(content_block: BetaContentBlock):\n            pass\n\n        def tool_output_callback(result: ToolResult, tool_id: str):\n            if result.output:\n                print(f\"---\\n{result.output}\\n---\")\n            if result.error:\n                print(f\"---\\n{result.error}\\n---\")\n\n        try:\n            async for chunk in sampling_loop(\n                model=model,\n                provider=provider,\n                system_prompt_suffix=system_prompt_suffix,\n                messages=messages,\n                output_callback=output_callback,\n                tool_output_callback=tool_output_callback,\n                api_key=api_key,\n            ):\n                if chunk[\"type\"] == \"messages\":\n                    messages = chunk[\"messages\"]\n        except Exception as e:\n            raise\n\n    # The thread will automatically terminate when the main program exits\n\n\ndef run_async_main():\n    if \"--server\" in sys.argv:\n        # Start uvicorn server directly without asyncio.run()\n        app = asyncio.run(main())\n        uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n    else:\n        asyncio.run(main())\n\n\nif __name__ == \"__main__\":\n    run_async_main()\n\nimport sys\nimport threading\n\n# Replace the pynput and screeninfo imports with pyautogui\nimport pyautogui\n\n# Replace the global variables and functions related to mouse tracking\nexit_flag = False\n\n\ndef check_mouse_position():\n    global exit_flag\n    corner_threshold = 10\n    screen_width, screen_height = pyautogui.size()\n\n    while not exit_flag:\n        x, y = pyautogui.position()\n        if (\n            (x <= corner_threshold and y <= corner_threshold)\n            or (x <= corner_threshold and y >= screen_height - corner_threshold)\n            or (x >= screen_width - corner_threshold and y <= corner_threshold)\n            or (\n                x >= screen_width - corner_threshold\n                and y >= screen_height - corner_threshold\n            )\n        ):\n            exit_flag = True\n            print(\"\\nMouse moved to corner. Exiting...\")\n            os._exit(0)\n        threading.Event().wait(0.1)  # Check every 100ms\n\n\nclass ChatMessage(BaseModel):\n    role: str\n    content: str\n\n\nclass ChatCompletionRequest(BaseModel):\n    messages: List[ChatMessage]\n    stream: Optional[bool] = False\n",
    "interpreter/computer_use/tools/__init__.py": "from .base import CLIResult, ToolResult\nfrom .bash import BashTool\nfrom .collection import ToolCollection\nfrom .computer import ComputerTool\nfrom .edit import EditTool\n\n__ALL__ = [\n    BashTool,\n    CLIResult,\n    ComputerTool,\n    EditTool,\n    ToolCollection,\n    ToolResult,\n]\n",
    "interpreter/computer_use/tools/base.py": "from abc import ABCMeta, abstractmethod\nfrom dataclasses import dataclass, fields, replace\nfrom typing import Any\n\nfrom anthropic.types.beta import BetaToolUnionParam\n\n\nclass BaseAnthropicTool(metaclass=ABCMeta):\n    \"\"\"Abstract base class for Anthropic-defined tools.\"\"\"\n\n    @abstractmethod\n    def __call__(self, **kwargs) -> Any:\n        \"\"\"Executes the tool with the given arguments.\"\"\"\n        ...\n\n    @abstractmethod\n    def to_params(\n        self,\n    ) -> BetaToolUnionParam:\n        raise NotImplementedError\n\n\n@dataclass(kw_only=True, frozen=True)\nclass ToolResult:\n    \"\"\"Represents the result of a tool execution.\"\"\"\n\n    output: str | None = None\n    error: str | None = None\n    base64_image: str | None = None\n    system: str | None = None\n\n    def __bool__(self):\n        return any(getattr(self, field.name) for field in fields(self))\n\n    def __add__(self, other: \"ToolResult\"):\n        def combine_fields(\n            field: str | None, other_field: str | None, concatenate: bool = True\n        ):\n            if field and other_field:\n                if concatenate:\n                    return field + other_field\n                raise ValueError(\"Cannot combine tool results\")\n            return field or other_field\n\n        return ToolResult(\n            output=combine_fields(self.output, other.output),\n            error=combine_fields(self.error, other.error),\n            base64_image=combine_fields(self.base64_image, other.base64_image, False),\n            system=combine_fields(self.system, other.system),\n        )\n\n    def replace(self, **kwargs):\n        \"\"\"Returns a new ToolResult with the given fields replaced.\"\"\"\n        return replace(self, **kwargs)\n\n\nclass CLIResult(ToolResult):\n    \"\"\"A ToolResult that can be rendered as a CLI output.\"\"\"\n\n\nclass ToolFailure(ToolResult):\n    \"\"\"A ToolResult that represents a failure.\"\"\"\n\n\nclass ToolError(Exception):\n    \"\"\"Raised when a tool encounters an error.\"\"\"\n\n    def __init__(self, message):\n        self.message = message\n"
  },
  "requirements": null
}