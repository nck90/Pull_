{
  "repo_name": "xtekky/gpt4free",
  "repo_url": "https://github.com/xtekky/gpt4free",
  "description": "The official gpt4free repository | various collection of powerful language models | o3 and deepseek r1, gpt-4.5",
  "stars": 63839,
  "language": "Python",
  "created_at": "2023-03-29T17:00:43Z",
  "updated_at": "2025-03-19T04:54:15Z",
  "files": {
    "etc/testing/_providers.py": "import sys\nfrom pathlib import Path\nfrom colorama import Fore, Style\n\nsys.path.append(str(Path(__file__).parent.parent))\n\nfrom g4f import Provider, ProviderType, models\nfrom g4f.Provider import __providers__\n\n\ndef main():\n    providers = get_providers()\n    failed_providers = []\n\n    for provider in providers:\n        if provider.needs_auth:\n            continue\n        print(\"Provider:\", provider.__name__)\n        result = test(provider)\n        print(\"Result:\", result)\n        if provider.working and not result:\n            failed_providers.append(provider)\n    print()\n\n    if failed_providers:\n        print(f\"{Fore.RED + Style.BRIGHT}Failed providers:{Style.RESET_ALL}\")\n        for _provider in failed_providers:\n            print(f\"{Fore.RED}{_provider.__name__}\")\n    else:\n        print(f\"{Fore.GREEN + Style.BRIGHT}All providers are working\")\n\n\ndef get_providers() -> list[ProviderType]:\n    return [\n        provider\n        for provider in __providers__\n        if provider.__name__ not in dir(Provider.deprecated)\n        and provider.url is not None\n    ]\n\ndef create_response(provider: ProviderType) -> str:\n    response = provider.create_completion(\n        model=models.default.name,\n        messages=[{\"role\": \"user\", \"content\": \"Hello, who are you? Answer in detail much as possible.\"}],\n        stream=False,\n    )\n    return \"\".join(response)\n\ndef test(provider: ProviderType) -> bool:\n    try:\n        response = create_response(provider)\n        assert type(response) is str\n        assert len(response) > 0\n        return response\n    except Exception:\n        return False\n\n\nif __name__ == \"__main__\":\n    main()\n    \n",
    "etc/testing/log_time.py": "from time import time\n\n\nasync def log_time_async(method: callable, **kwargs):\n    start = time()\n    result = await method(**kwargs)\n    secs = f\"{round(time() - start, 2)} secs\"\n    return \" \".join([result, secs]) if result else secs\n\n\ndef log_time_yield(method: callable, **kwargs):\n    start = time()\n    result = yield from method(**kwargs)\n    yield f\" {round(time() - start, 2)} secs\"\n\n\ndef log_time(method: callable, **kwargs):\n    start = time()\n    result = method(**kwargs)\n    secs = f\"{round(time() - start, 2)} secs\"\n    return \" \".join([result, secs]) if result else secs",
    "etc/testing/test_all.py": "import asyncio\nimport sys\nfrom pathlib import Path\nsys.path.append(str(Path(__file__).parent.parent.parent))\n\nimport g4f\n\n\nasync def test(model: g4f.Model):\n    try:\n        try:\n            for response in g4f.ChatCompletion.create(\n                    model=model,\n                    messages=[{\"role\": \"user\", \"content\": \"write a poem about a tree\"}],\n                    temperature=0.1,\n                    stream=True\n            ):\n                print(response, end=\"\")\n\n            print()\n        except:\n            for response in await g4f.ChatCompletion.create_async(\n                    model=model,\n                    messages=[{\"role\": \"user\", \"content\": \"write a poem about a tree\"}],\n                    temperature=0.1,\n                    stream=True\n            ):\n                print(response, end=\"\")\n\n            print()\n\n        return True\n    except Exception as e:\n        print(model.name, \"not working:\", e)\n        print(e.__traceback__.tb_next)\n        return False\n\n\nasync def start_test():\n    models_to_test = [\n        # GPT-3.5\n        g4f.models.gpt_35_turbo,\n\n        # GPT-4\n        g4f.models.gpt_4,\n    ]\n\n    models_working = []\n\n    for model in models_to_test:\n        if await test(model):\n            models_working.append(model.name)\n\n    print(\"working models:\", models_working)\n\n\nasyncio.run(start_test())\n",
    "etc/testing/test_api.py": "import openai\n\n# Set your Hugging Face token as the API key if you use embeddings\n# If you don't use embeddings, leave it empty\nopenai.api_key = \"YOUR_HUGGING_FACE_TOKEN\"  # Replace with your actual token\n\n# Set the API base URL if needed, e.g., for a local development environment\nopenai.api_base = \"http://localhost:1337/v1\"\n\ndef main():\n    response = openai.ChatCompletion.create(\n        model=\"gpt-3.5-turbo\",\n        messages=[{\"role\": \"user\", \"content\": \"write a poem about a tree\"}],\n        stream=True,\n    )\n    if isinstance(response, dict):\n        # Not streaming\n        print(response.choices[0].message.content)\n    else:\n        # Streaming\n        for token in response:\n            content = token[\"choices\"][0][\"delta\"].get(\"content\")\n            if content is not None:\n                print(content, end=\"\", flush=True)\n\nif __name__ == \"__main__\":\n    main()",
    "etc/testing/test_async.py": "import sys\nfrom pathlib import Path\nimport asyncio\n\nsys.path.append(str(Path(__file__).parent.parent))\nsys.path.append(str(Path(__file__).parent.parent.parent))\n\nimport g4f\nfrom testing._providers import get_providers\nfrom testing.log_time import log_time_async\n\nasync def create_async(provider):\n    try:\n        response = await log_time_async(\n            provider.create_async,\n            model=g4f.models.default.name,\n            messages=[{\"role\": \"user\", \"content\": \"Hello, are you GPT 3.5?\"}]\n        )\n        print(f\"{provider.__name__}:\", response)\n    except Exception as e:\n        print(f\"{provider.__name__}: {e.__class__.__name__}: {e}\")\n\nasync def run_async():\n  responses: list = [\n      create_async(provider)\n      for provider in get_providers()\n      if provider.working\n  ]\n  await asyncio.gather(*responses)\n\nprint(\"Total:\", asyncio.run(log_time_async(run_async)))",
    "etc/testing/test_chat_completion.py": "import sys\nfrom pathlib import Path\n\nsys.path.append(str(Path(__file__).parent.parent.parent))\n\nimport g4f, asyncio\n\nprint(\"create:\", end=\" \", flush=True)\nfor response in g4f.ChatCompletion.create(\n    model=g4f.models.default,\n    #provider=g4f.Provider.Bing,\n    messages=[{\"role\": \"user\", \"content\": \"write a poem about a tree\"}],\n    stream=True\n):\n    print(response, end=\"\", flush=True)\nprint()\n\nasync def run_async():\n    response = await g4f.ChatCompletion.create_async(\n        model=g4f.models.default,\n        #provider=g4f.Provider.Bing,\n        messages=[{\"role\": \"user\", \"content\": \"hello!\"}],\n    )\n    print(\"create_async:\", response)\n\nasyncio.run(run_async())\n",
    "etc/testing/test_gui.py": "import sys\nfrom pathlib import Path\nsys.path.append(str(Path(__file__).parent.parent.parent))\n\nfrom g4f.gui import run_gui\nrun_gui()\n",
    "etc/testing/test_interference.py": "# type: ignore\nimport openai\n\nopenai.api_key = \"\"\nopenai.api_base = \"http://localhost:1337\"\n\n\ndef main():\n    chat_completion = openai.ChatCompletion.create(\n        model=\"gpt-3.5-turbo\",\n        messages=[{\"role\": \"user\", \"content\": \"write a poem about a tree\"}],\n        stream=True,\n    )\n\n    if isinstance(chat_completion, dict):\n        # not stream\n        print(chat_completion.choices[0].message.content)\n    else:\n        # stream\n        for token in chat_completion:\n            content = token[\"choices\"][0][\"delta\"].get(\"content\")\n            if content != None:\n                print(content, end=\"\", flush=True)\n\n\nif __name__ == \"__main__\":\n    main()",
    "etc/testing/test_needs_auth.py": "import sys\nfrom pathlib import Path\nimport asyncio\n\nsys.path.append(str(Path(__file__).parent.parent))\n\nimport g4f\nfrom  testing.log_time import log_time, log_time_async, log_time_yield\n\n\n_providers = [\n    g4f.Provider.H2o,\n    g4f.Provider.You,\n    g4f.Provider.HuggingChat,\n    g4f.Provider.OpenAssistant,\n    g4f.Provider.Bing,\n    g4f.Provider.Bard\n]\n\n_instruct = \"Hello, are you GPT 4?.\"\n\n_example = \"\"\"\nOpenaiChat: Hello! How can I assist you today? 2.0 secs\nBard: Hello! How can I help you today? 3.44 secs\nBing: Hello, this is Bing. How can I help? ðŸ˜Š 4.14 secs\nAsync Total: 4.25 secs\n\nOpenaiChat: Hello! How can I assist you today? 1.85 secs\nBard: Hello! How can I help you today? 3.38 secs\nBing: Hello, this is Bing. How can I help? ðŸ˜Š 6.14 secs\nStream Total: 11.37 secs\n\nOpenaiChat: Hello! How can I help you today? 3.28 secs\nBard: Hello there! How can I help you today? 3.58 secs\nBing: Hello! How can I help you today? 3.28 secs\nNo Stream Total: 10.14 secs\n\"\"\"\n\nprint(\"Bing: \", end=\"\")\nfor response in log_time_yield(\n    g4f.ChatCompletion.create,\n    model=g4f.models.default,\n    messages=[{\"role\": \"user\", \"content\": _instruct}],\n    provider=g4f.Provider.Bing,\n    #cookies=g4f.get_cookies(\".huggingface.co\"),\n    stream=True,\n    auth=True\n):\n    print(response, end=\"\", flush=True)\nprint()\nprint()\n\n\nasync def run_async():\n    responses = [\n        log_time_async(\n            provider.create_async, \n            model=None,\n            messages=[{\"role\": \"user\", \"content\": _instruct}],\n        )\n        for provider in _providers\n    ]\n    responses = await asyncio.gather(*responses)\n    for idx, provider in enumerate(_providers):\n        print(f\"{provider.__name__}:\", responses[idx])\nprint(\"Async Total:\", asyncio.run(log_time_async(run_async)))\nprint()\n\n\ndef run_stream():\n    for provider in _providers:\n        print(f\"{provider.__name__}: \", end=\"\")\n        for response in log_time_yield(\n            provider.create_completion,\n            model=None,\n            messages=[{\"role\": \"user\", \"content\": _instruct}],\n        ):\n            print(response, end=\"\", flush=True)\n        print()\nprint(\"Stream Total:\", log_time(run_stream))\nprint()\n\n\ndef create_no_stream():\n    for provider in _providers:\n        print(f\"{provider.__name__}:\", end=\" \")\n        for response in log_time_yield(\n            provider.create_completion,\n            model=None,\n            messages=[{\"role\": \"user\", \"content\": _instruct}],\n            stream=False\n        ):\n            print(response, end=\"\")\n        print()\nprint(\"No Stream Total:\", log_time(create_no_stream))\nprint()",
    "etc/testing/test_providers.py": "from g4f.Provider import __all__, ProviderUtils\nfrom g4f import ChatCompletion\nimport concurrent.futures\n\n_ = [\n    'BaseProvider',\n    'AsyncProvider',\n    'AsyncGeneratorProvider',\n    'RetryProvider'\n]\n\ndef test_provider(provider):\n    try:\n        provider = (ProviderUtils.convert[provider])\n        if provider.working and not provider.needs_auth:\n            print('testing', provider.__name__)\n            completion = ChatCompletion.create(model='gpt-3.5-turbo', \n                                            messages=[{\"role\": \"user\", \"content\": \"hello\"}], provider=provider)\n            return completion, provider.__name__\n    except Exception as e:\n        #print(f'Failed to test provider: {provider} | {e}')\n        return None\n\nwith concurrent.futures.ThreadPoolExecutor() as executor:\n    futures = [\n        executor.submit(test_provider, provider)\n        for provider in __all__\n        if provider not in _\n    ]\n    for future in concurrent.futures.as_completed(futures):\n        if result := future.result():\n            print(f'{result[1]} | {result[0]}')"
  },
  "requirements": "requests\npycryptodome\ncurl_cffi>=0.6.2\naiohttp\ncertifi\nbrowser_cookie3\nduckduckgo-search>=5.0\nnest_asyncio\nwerkzeug\npillow\nplatformdirs\nfastapi\nuvicorn\nflask\nbrotli\nbeautifulsoup4\nsetuptools\ncryptography\nnodriver\npython-multipart\npypdf2\npython-docx"
}