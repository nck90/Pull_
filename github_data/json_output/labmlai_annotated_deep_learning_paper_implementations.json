{
  "repo_name": "labmlai/annotated_deep_learning_paper_implementations",
  "repo_url": "https://github.com/labmlai/annotated_deep_learning_paper_implementations",
  "description": "ðŸ§‘â€ðŸ« 60+ Implementations/tutorials of deep learning papers with side-by-side notes ðŸ“; including transformers (original, xl, switch, feedback, vit, ...), optimizers (adam, adabelief, sophia, ...), gans(cyclegan, stylegan2, ...), ðŸŽ® reinforcement learning (ppo, dqn), capsnet, distillation, ... ðŸ§ ",
  "stars": 59287,
  "language": "Python",
  "created_at": "2020-08-25T02:29:34Z",
  "updated_at": "2025-03-19T06:49:16Z",
  "files": {
    "labml_nn/optimizers/performance_test.py": "\"\"\"\n---\ntitle: Test performance of Adam implementations\nsummary: This experiment compares performance of Adam implementations.\n---\n\n# Performance testing Adam\n\n```text\nTorchAdam warmup...[DONE]\t222.59ms\nTorchAdam...[DONE]\t1,356.01ms\nMyAdam warmup...[DONE]\t119.15ms\nMyAdam...[DONE]\t1,192.89ms\n```\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1ngowaAsADj8VdZfBifu_6L6rtjGoEeoR?usp=sharing)\n\"\"\"\n\nimport torch\nimport torch.nn as nn\nfrom labml_helpers.device import DeviceInfo\nfrom torch.optim import Adam as TorchAdam\n\nfrom labml import monit\nfrom labml_nn.optimizers.adam import Adam as MyAdam\nfrom labml_nn.optimizers.mnist_experiment import Model\n\n\ndef test():\n    device_info = DeviceInfo(use_cuda=True, cuda_device=0)\n    print(device_info)\n    inp = torch.randn((64, 1, 28, 28), device=device_info.device)\n    target = torch.ones(64, dtype=torch.long, device=device_info.device)\n    loss_func = nn.CrossEntropyLoss()\n    model = Model().to(device_info.device)\n    my_adam = MyAdam(model.parameters())\n    torch_adam = TorchAdam(model.parameters())\n    loss = loss_func(model(inp), target)\n    loss.backward()\n    with monit.section('MyAdam warmup'):\n        for i in range(100):\n            my_adam.step()\n    with monit.section('MyAdam'):\n        for i in range(1000):\n            my_adam.step()\n    with monit.section('TorchAdam warmup'):\n        for i in range(100):\n            torch_adam.step()\n    with monit.section('TorchAdam'):\n        for i in range(1000):\n            torch_adam.step()\n\n\nif __name__ == '__main__':\n    test()\n",
    "labml_nn/__init__.py": "\"\"\"\n# [Annotated Research Paper Implementations: Transformers, StyleGAN, Stable Diffusion, DDPM/DDIM, LayerNorm, Nucleus Sampling and more](index.html)\n\nThis is a collection of simple PyTorch implementations of\nneural networks and related algorithms.\n[These implementations](https://github.com/labmlai/annotated_deep_learning_paper_implementations) are documented with explanations,\nand the [website](index.html)\nrenders these as side-by-side formatted notes.\nWe believe these would help you understand these algorithms better.\n\n![Screenshot](dqn-light.png)\n\nWe are actively maintaining this repo and adding new\nimplementations.\n[![Twitter](https://img.shields.io/twitter/follow/labmlai?style=social)](https://twitter.com/labmlai) for updates.\n\n## Translations\n\n### **[English (original)](https://nn.labml.ai)**\n### **[Chinese (translated)](https://nn.labml.ai/zh/)**\n### **[Japanese (translated)](https://nn.labml.ai/ja/)**\n\n## Paper Implementations\n\n#### âœ¨ [Transformers](transformers/index.html)\n\n* [Multi-headed attention](transformers/mha.html)\n* [Transformer building blocks](transformers/models.html)\n* [Transformer XL](transformers/xl/index.html)\n    * [Relative multi-headed attention](transformers/xl/relative_mha.html)\n* [Rotary Positional Embeddings (RoPE)](transformers/rope/index.html)\n* [Attention with Linear Biases (ALiBi)](transformers/alibi/index.html)\n* [RETRO](transformers/retro/index.html)\n* [Compressive Transformer](transformers/compressive/index.html)\n* [GPT Architecture](transformers/gpt/index.html)\n* [GLU Variants](transformers/glu_variants/simple.html)\n* [kNN-LM: Generalization through Memorization](transformers/knn/index.html)\n* [Feedback Transformer](transformers/feedback/index.html)\n* [Switch Transformer](transformers/switch/index.html)\n* [Fast Weights Transformer](transformers/fast_weights/index.html)\n* [FNet](transformers/fnet/index.html)\n* [Attention Free Transformer](transformers/aft/index.html)\n* [Masked Language Model](transformers/mlm/index.html)\n* [MLP-Mixer: An all-MLP Architecture for Vision](transformers/mlp_mixer/index.html)\n* [Pay Attention to MLPs (gMLP)](transformers/gmlp/index.html)\n* [Vision Transformer (ViT)](transformers/vit/index.html)\n* [Primer EZ](transformers/primer_ez/index.html)\n* [Hourglass](transformers/hour_glass/index.html)\n\n#### âœ¨ [Low-Rank Adaptation (LoRA)](lora/index.html)\n\n#### âœ¨ [Eleuther GPT-NeoX](neox/index.html)\n* [Generate on a 48GB GPU](neox/samples/generate.html)\n* [Finetune on two 48GB GPUs](neox/samples/finetune.html)\n* [LLM.int8()](neox/utils/llm_int8.html)\n\n#### âœ¨ [Diffusion models](diffusion/index.html)\n\n* [Denoising Diffusion Probabilistic Models (DDPM)](diffusion/ddpm/index.html)\n* [Denoising Diffusion Implicit Models (DDIM)](diffusion/stable_diffusion/sampler/ddim.html)\n* [Latent Diffusion Models](diffusion/stable_diffusion/latent_diffusion.html)\n* [Stable Diffusion](diffusion/stable_diffusion/index.html)\n\n#### âœ¨ [Generative Adversarial Networks](gan/index.html)\n* [Original GAN](gan/original/index.html)\n* [GAN with deep convolutional network](gan/dcgan/index.html)\n* [Cycle GAN](gan/cycle_gan/index.html)\n* [Wasserstein GAN](gan/wasserstein/index.html)\n* [Wasserstein GAN with Gradient Penalty](gan/wasserstein/gradient_penalty/index.html)\n* [StyleGAN 2](gan/stylegan/index.html)\n\n#### âœ¨ [Recurrent Highway Networks](recurrent_highway_networks/index.html)\n\n#### âœ¨ [LSTM](lstm/index.html)\n\n#### âœ¨ [HyperNetworks - HyperLSTM](hypernetworks/hyper_lstm.html)\n\n#### âœ¨ [ResNet](resnet/index.html)\n\n#### âœ¨ [ConvMixer](conv_mixer/index.html)\n\n#### âœ¨ [Capsule Networks](capsule_networks/index.html)\n\n#### âœ¨ [U-Net](unet/index.html)\n\n#### âœ¨ [Sketch RNN](sketch_rnn/index.html)\n\n#### âœ¨ Graph Neural Networks\n\n* [Graph Attention Networks (GAT)](graphs/gat/index.html)\n* [Graph Attention Networks v2 (GATv2)](graphs/gatv2/index.html)\n\n#### âœ¨ [Reinforcement Learning](rl/index.html)\n* [Proximal Policy Optimization](rl/ppo/index.html) with\n [Generalized Advantage Estimation](rl/ppo/gae.html)\n* [Deep Q Networks](rl/dqn/index.html) with\n with [Dueling Network](rl/dqn/model.html),\n [Prioritized Replay](rl/dqn/replay_buffer.html)\n and Double Q Network.\n\n#### âœ¨ [Counterfactual Regret Minimization (CFR)](cfr/index.html)\n\nSolving games with incomplete information such as poker with CFR.\n\n* [Kuhn Poker](cfr/kuhn/index.html)\n\n#### âœ¨ [Optimizers](optimizers/index.html)\n* [Adam](optimizers/adam.html)\n* [AMSGrad](optimizers/amsgrad.html)\n* [Adam Optimizer with warmup](optimizers/adam_warmup.html)\n* [Noam Optimizer](optimizers/noam.html)\n* [Rectified Adam Optimizer](optimizers/radam.html)\n* [AdaBelief Optimizer](optimizers/ada_belief.html)\n* [Sophia-G Optimizer](optimizers/sophia.html)\n\n#### âœ¨ [Normalization Layers](normalization/index.html)\n* [Batch Normalization](normalization/batch_norm/index.html)\n* [Layer Normalization](normalization/layer_norm/index.html)\n* [Instance Normalization](normalization/instance_norm/index.html)\n* [Group Normalization](normalization/group_norm/index.html)\n* [Weight Standardization](normalization/weight_standardization/index.html)\n* [Batch-Channel Normalization](normalization/batch_channel_norm/index.html)\n* [DeepNorm](normalization/deep_norm/index.html)\n\n#### âœ¨ [Distillation](distillation/index.html)\n\n#### âœ¨ [Adaptive Computation](adaptive_computation/index.html)\n\n* [PonderNet](adaptive_computation/ponder_net/index.html)\n\n#### âœ¨ [Uncertainty](uncertainty/index.html)\n\n* [Evidential Deep Learning to Quantify Classification Uncertainty](uncertainty/evidence/index.html)\n\n#### âœ¨ [Activations](activations/index.html)\n\n* [Fuzzy Tiling Activations](activations/fta/index.html)\n\n#### âœ¨ [Language Model Sampling Techniques](sampling/index.html)\n* [Greedy Sampling](sampling/greedy.html)\n* [Temperature Sampling](sampling/temperature.html)\n* [Top-k Sampling](sampling/top_k.html)\n* [Nucleus Sampling](sampling/nucleus.html)\n\n#### âœ¨ [Scalable Training/Inference](scaling/index.html)\n* [Zero3 memory optimizations](scaling/zero3/index.html)\n\n### Installation\n\n```bash\npip install labml-nn\n```\n\"\"\"\n",
    "labml_nn/activations/__init__.py": "\"\"\"\n---\ntitle: Neural Network Activation Functions\nsummary: >\n A set of PyTorch implementations/tutorials related to neural network activations\n---\n\n# Neural Networks Activations\n\n* [Fuzzy Tiling Activations](fta/index.html)\n* ðŸš§ [Swish](swish/index.html)\n\"\"\"\n\nfrom .swish import Swish\n",
    "labml_nn/activations/fta/__init__.py": "\"\"\"\n---\ntitle: Fuzzy Tiling Activations\nsummary: >\n  PyTorch implementation and tutorial of Fuzzy Tiling Activations from the\n  paper Fuzzy Tiling Activations: A Simple Approach to Learning Sparse Representations Online.\n---\n\n# Fuzzy Tiling Activations (FTA)\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/activations/fta/experiment.ipynb)\n\nThis is a [PyTorch](https://pytorch.org) implementation/tutorial of\n[Fuzzy Tiling Activations: A Simple Approach to Learning Sparse Representations Online](https://arxiv.org/abs/1911.08068).\n\nFuzzy tiling activations are a form of sparse activations based on binning.\n\nBinning is classification of a scalar value into a bin based on intervals.\nOne problem with binning is that it gives zero gradients for most values (except at the boundary of bins).\nThe other is that binning loses precision if the bin intervals are large.\n\nFTA overcomes these disadvantages.\nInstead of hard boundaries like in Tiling Activations, FTA uses soft boundaries\nbetween bins.\nThis gives non-zero gradients for all or a wide range of values.\nAnd also doesn't lose precision since it's captured in partial values.\n\n#### Tiling Activations\n\n$\\mathbf{c}$ is the tiling vector,\n\n$$\\mathbf{c} = (l, l + \\delta, l + 2 \\delta, \\dots, u - 2 \\delta, u - \\delta)$$\n\nwhere $[l, u]$ is the input range, $\\delta$ is the bin size, and $u - l$ is divisible by $\\delta$.\n\nTiling activation is,\n\n$$\\phi(z) = 1 - I_+ \\big( \\max(\\mathbf{c} - z, 0) + \\max(z - \\delta - \\mathbf{c}) \\big)$$\n\nwhere $I_+(\\cdot)$ is the indicator function which gives $1$ if the input is positive and $0$ otherwise.\n\nNote that tiling activation gives zero gradients because it has hard boundaries.\n\n#### Fuzzy Tiling Activations\n\nThe fuzzy indicator function,\n\n$$I_{\\eta,+}(x) = I_+(\\eta - x) x + I_+ (x - \\eta)$$\n\nwhich increases linearly from $0$ to $1$ when $0 \\le x \\lt \\eta$\nand is equal to $1$ for $\\eta \\le x$.\n$\\eta$ is a hyper-parameter.\n\nFTA uses this to create soft boundaries between bins.\n\n$$\\phi_\\eta(z) = 1 - I_{\\eta,+} \\big( \\max(\\mathbf{c} - z, 0) + \\max(z - \\delta - \\mathbf{c}, 0) \\big)$$\n\n[Here's a simple experiment](experiment.html) that uses FTA in a transformer.\n\"\"\"\n\nimport torch\nfrom torch import nn\n\n\nclass FTA(nn.Module):\n    \"\"\"\n    ### Fuzzy Tiling Activations (FTA)\n    \"\"\"\n\n    def __init__(self, lower_limit: float, upper_limit: float, delta: float, eta: float):\n        \"\"\"\n        :param lower_limit: is the lower limit $l$\n        :param upper_limit: is the upper limit $u$\n        :param delta: is the bin size $\\delta$\n        :param eta: is the parameter $\\eta$ that detemines the softness of the boundaries.\n        \"\"\"\n        super().__init__()\n        # Initialize tiling vector\n        # $$\\mathbf{c} = (l, l + \\delta, l + 2 \\delta, \\dots, u - 2 \\delta, u - \\delta)$$\n        self.c = nn.Parameter(torch.arange(lower_limit, upper_limit, delta), requires_grad=False)\n        # The input vector expands by a factor equal to the number of bins $\\frac{u - l}{\\delta}$\n        self.expansion_factor = len(self.c)\n        # $\\delta$\n        self.delta = delta\n        # $\\eta$\n        self.eta = eta\n\n    def fuzzy_i_plus(self, x: torch.Tensor):\n        \"\"\"\n        #### Fuzzy indicator function\n\n        $$I_{\\eta,+}(x) = I_+(\\eta - x) x + I_+ (x - \\eta)$$\n        \"\"\"\n        return (x <= self.eta) * x + (x > self.eta)\n\n    def forward(self, z: torch.Tensor):\n        # Add another dimension of size $1$.\n        # We will expand this into bins.\n        z = z.view(*z.shape, 1)\n\n        # $$\\phi_\\eta(z) = 1 - I_{\\eta,+} \\big( \\max(\\mathbf{c} - z, 0) + \\max(z - \\delta - \\mathbf{c}, 0) \\big)$$\n        z = 1. - self.fuzzy_i_plus(torch.clip(self.c - z, min=0.) + torch.clip(z - self.delta - self.c, min=0.))\n\n        # Reshape back to original number of dimensions.\n        # The last dimension size gets expanded by the number of bins, $\\frac{u - l}{\\delta}$.\n        return z.view(*z.shape[:-2], -1)\n\n\ndef _test():\n    \"\"\"\n    #### Code to test the FTA module\n    \"\"\"\n    from labml.logger import inspect\n\n    # Initialize\n    a = FTA(-10, 10, 2., 0.5)\n    # Print $\\mathbf{c}$\n    inspect(a.c)\n    # Print number of bins $\\frac{u - l}{\\delta}$\n    inspect(a.expansion_factor)\n\n    # Input $z$\n    z = torch.tensor([1.1, 2.2, 3.3, 4.4, 5.5, 6.6, 7.7, 8.8, 9., 10., 11.])\n    # Print $z$\n    inspect(z)\n    # Print $\\phi_\\eta(z)$\n    inspect(a(z))\n\n\nif __name__ == '__main__':\n    _test()\n",
    "labml_nn/activations/fta/experiment.py": "\"\"\"\n---\ntitle: Fuzzy Tiling Activation Experiment\nsummary: >\n Training a transformer with FTA in FFN on Tiny Shakespeare.\n---\n\n# [Fuzzy Tiling Activation](index.html) Experiment\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/activations/fta/experiment.ipynb)\n\nHere we train a transformer that uses [Fuzzy Tiling Activation](index.html) in the\n[Feed-Forward Network](../../transformers/feed_forward.html).\nWe use it for a language model and train it on Tiny Shakespeare dataset\nfor demonstration.\n\nHowever, this is probably not the ideal task for FTA, and we\nbelieve FTA is more suitable for modeling data with continuous variables.\n\"\"\"\n\nimport copy\n\nimport torch\nimport torch.nn as nn\n\nfrom labml import experiment\nfrom labml.configs import option\nfrom labml_helpers.module import Module\nfrom labml_nn.activations.fta import FTA\nfrom labml_nn.experiments.nlp_autoregression import NLPAutoRegressionConfigs\nfrom labml_nn.transformers import MultiHeadAttention, TransformerLayer\nfrom labml_nn.transformers.utils import subsequent_mask\n\n\nclass FeedForwardFTA(nn.Module):\n    \"\"\"\n    ## FFN module with [FTA](index.html) activation\n    \"\"\"\n\n    def __init__(self, d_model: int, d_ff: int,\n                 activation: FTA,\n                 dropout: float = 0.1):\n        \"\"\"\n        * `d_model` is the number of features in a token embedding\n        * `d_ff` is the number of features in the hidden layer of the FFN\n        * `activation` is FTA activation module\n        * `dropout` is dropout probability for the hidden layer\n        \"\"\"\n        super().__init__()\n        # Layer one parameterized by weight $W_1$ and bias $b_1$\n        self.layer1 = nn.Linear(d_model, d_ff)\n        # Layer two parameterized by weight $W_1$ and bias $b_1$\n        self.layer2 = nn.Linear(d_ff * activation.expansion_factor, d_model)\n        # Hidden layer dropout\n        self.dropout = nn.Dropout(dropout)\n        # Activation function $f$\n        self.activation = activation\n\n    def forward(self, x: torch.Tensor):\n        # $f(x W_1 + b_1)$\n        x = self.activation(self.layer1(x))\n        # Apply dropout\n        x = self.dropout(x)\n        #\n        return self.layer2(x)\n\n\nclass AutoregressiveTransformer(Module):\n    \"\"\"\n    ## Auto-Regressive model\n\n    This is an autoregressive transformer model that uses Feed-Forward Networks with\n     (Fuzzy Tiling Activations)(index.html).\n    \"\"\"\n\n    def __init__(self, n_tokens: int, d_model: int, n_layers: int, layer: TransformerLayer):\n        \"\"\"\n        :param n_tokens: is the number of tokens in the vocabulary\n        :param d_model: is the embedding size\n        :param n_layers: is the number of transformer layers\n        :param layer: is the layer. We use `n_layers` copies of this for the transformer.\n        \"\"\"\n        super().__init__()\n        # Transformer with `n_layers` layers\n        self.transformer_layers = nn.ModuleList([copy.deepcopy(layer) for _ in range(n_layers)])\n\n        # Token embedding layer\n        self.emb = nn.Embedding(n_tokens, d_model)\n        # Readout layer\n        self.readout = nn.Linear(d_model, n_tokens)\n\n        # The mask will be initialized on the first call\n        self.mask = None\n\n    def forward(self, x: torch.Tensor):\n        \"\"\"\n        :param x: are the input tokens of shape `[seq_len, batch_size]`\n        \"\"\"\n        # Create auto-regressive mask\n        if self.mask is None or self.mask.size(0) != len(x):\n            # Subsequent mask, will mask out tokens from seeing future tokens\n            self.mask = subsequent_mask(len(x)).to(x.device)\n\n        # Get the token embeddings\n        x = self.emb(x)\n        # Transformer encoder\n        for layer in self.transformer_layers:\n            x = layer(x=x, mask=self.mask)\n        # Get logits\n        x = self.readout(x)\n\n        # Return results\n        return x, None\n\n\nclass Configs(NLPAutoRegressionConfigs):\n    \"\"\"\n    ## Configurations\n\n    This inherits from\n    [`NLPAutoRegressionConfigs`](../../experiments/nlp_autoregression.html#NLPAutoRegressionConfigs)\n    \"\"\"\n\n    # Model\n    model: AutoregressiveTransformer\n\n    # Number of layers\n    n_layers: int = 4\n\n    # $\\alpha$ and $\\beta$ for DeepNorm\n    deep_norm_alpha: float\n    deep_norm_beta: float\n\n    # Number of heads in the attention\n    n_heads: int = 4\n    # Embedding size\n    d_model: int = 256\n    # Size of each attention head\n    d_k: int = 16\n    # Feed forward layer size\n    d_ff: int = 256\n\n    # FTA\n    fta_lower_limit: float = -1.\n    fta_upper_limit: float = +1.\n    fta_delta: float = 0.2\n    fta_eta: float = 0.05\n\n\n@option(Configs.model)\ndef _model(c: Configs):\n    \"\"\"\n    #### Initialize the model\n    \"\"\"\n\n    # Create FTA activation module\n    fta = FTA(c.fta_lower_limit, c.fta_upper_limit, c.fta_delta, c.fta_eta)\n    # Create the transformer.\n    # We re-use [`TransformerLayer`](../../transformers/models.html#TransformerLayer) and\n    # [`MultiHeadAttention`](../../transformers/mha.html) implementations.\n    m = AutoregressiveTransformer(c.n_tokens, c.d_model, c.n_layers,\n                                  TransformerLayer(d_model=c.d_model,\n                                                   feed_forward=FeedForwardFTA(d_model=c.d_model,\n                                                                               d_ff=c.d_ff,\n                                                                               activation=fta,\n                                                                               dropout=0.1),\n                                                   self_attn=MultiHeadAttention(c.n_heads, c.d_model,\n                                                                                dropout_prob=0.0),\n                                                   dropout_prob=0.0))\n\n    # Move to the device\n    return m.to(c.device)\n\n\ndef main():\n    \"\"\"\n    #### Create and run the experiment\n    \"\"\"\n    # Create experiment\n    experiment.create(name=\"fta\", writers={'screen', 'labml'})\n    # Create configs\n    conf = Configs()\n    # Override configurations\n    experiment.configs(conf, {\n        # Use character level tokenizer\n        'tokenizer': 'character',\n        # Prompt separator is blank\n        'prompt_separator': '',\n        # Starting prompt for sampling\n        'prompt': 'It is ',\n        # Use Tiny Shakespeare dataset\n        'text': 'tiny_shakespeare',\n\n        # Use a context size of $256$\n        'seq_len': 256,\n        # Train for 32 epochs\n        'epochs': 32,\n        # Batch size $16$\n        'batch_size': 16,\n        # Switch between training and validation for $10$ times per epoch\n        'inner_iterations': 10,\n\n        # Adam optimizer with no warmup\n        'optimizer.optimizer': 'Adam',\n        'optimizer.learning_rate': 3e-4,\n    })\n\n    # Set model(s) for saving and loading\n    experiment.add_pytorch_models({'model': conf.model})\n\n    # Start the experiment\n    with experiment.start():\n        # Run training\n        conf.run()\n\n\n#\nif __name__ == '__main__':\n    main()\n",
    "labml_nn/activations/swish.py": "import torch\nfrom torch import nn\n\nfrom labml_helpers.module import Module\n\n\nclass Swish(Module):\n    def __init__(self):\n        super().__init__()\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        return x * self.sigmoid(x)\n",
    "labml_nn/adaptive_computation/__init__.py": "\"\"\"\n---\ntitle: Neural Networks with Adaptive Computation\nsummary: >\n A set of PyTorch implementations/tutorials related to adaptive computation\n---\n\n# Neural Networks with Adaptive Computation\n\nThese are neural network architectures that change the computation complexity based on the\ncomplexity of the input sample.\n\n* ðŸš§ TODO: Adaptive Computation Time for Recurrent Neural Networks\n* [PonderNet: Learning to Ponder](ponder_net/index.html)\n\"\"\"\n",
    "labml_nn/adaptive_computation/parity.py": "\"\"\"\n---\ntitle: \"Parity Task\"\nsummary: >\n  This creates data for Parity Task from the paper Adaptive Computation Time\n  for Recurrent Neural Networks\n---\n\n# Parity Task\n\nThis creates data for Parity Task from the paper\n[Adaptive Computation Time for Recurrent Neural Networks](https://arxiv.org/abs/1603.08983).\n\nThe input of the parity task is a vector with $0$'s $1$'s and $-1$'s.\nThe output is the parity of $1$'s - one if there is an odd number of $1$'s and zero otherwise.\nThe input is generated by making a random number of elements in the vector either $1$ or $-1$'s.\n\"\"\"\n\nfrom typing import Tuple\n\nimport torch\nfrom torch.utils.data import Dataset\n\n\nclass ParityDataset(Dataset):\n    \"\"\"\n    ### Parity dataset\n    \"\"\"\n\n    def __init__(self, n_samples: int, n_elems: int = 64):\n        \"\"\"\n        * `n_samples` is the number of samples\n        * `n_elems` is the number of elements in the input vector\n        \"\"\"\n        self.n_samples = n_samples\n        self.n_elems = n_elems\n\n    def __len__(self):\n        \"\"\"\n        Size of the dataset\n        \"\"\"\n        return self.n_samples\n\n    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Generate a sample\n        \"\"\"\n\n        # Empty vector\n        x = torch.zeros((self.n_elems,))\n        # Number of non-zero elements - a random number between $1$ and total number of elements\n        n_non_zero = torch.randint(1, self.n_elems + 1, (1,)).item()\n        # Fill non-zero elements with $1$'s and $-1$'s\n        x[:n_non_zero] = torch.randint(0, 2, (n_non_zero,)) * 2 - 1\n        # Randomly permute the elements\n        x = x[torch.randperm(self.n_elems)]\n\n        # The parity\n        y = (x == 1.).sum() % 2\n\n        #\n        return x, y\n",
    "labml_nn/adaptive_computation/ponder_net/__init__.py": "\"\"\"\n---\ntitle: \"PonderNet: Learning to Ponder\"\nsummary: >\n A PyTorch implementation/tutorial of PonderNet: Learning to Ponder.\n---\n\n# PonderNet: Learning to Ponder\n\nThis is a [PyTorch](https://pytorch.org) implementation of the paper\n[PonderNet: Learning to Ponder](https://arxiv.org/abs/2107.05407).\n\nPonderNet adapts the computation based on the input.\nIt changes the number of steps to take on a recurrent network based on the input.\nPonderNet learns this with end-to-end gradient descent.\n\nPonderNet has a step function of the form\n\n$$\\hat{y}_n, h_{n+1}, \\lambda_n = s(x, h_n)$$\n\nwhere $x$ is the input, $h_n$ is the state, $\\hat{y}_n$ is the prediction at step $n$,\nand $\\lambda_n$ is the probability of halting (stopping) at current step.\n\n$s$ can be any neural network (e.g. LSTM, MLP, GRU, Attention layer).\n\nThe unconditioned probability of halting at step $n$ is then,\n\n$$p_n = \\lambda_n \\prod_{j=1}^{n-1} (1 - \\lambda_j)$$\n\nThat is the probability of not being halted at any of the previous steps and halting at step $n$.\n\nDuring inference, we halt by sampling based on the halting probability $\\lambda_n$\n and get the prediction at the halting layer $\\hat{y}_n$ as the final output.\n\nDuring training, we get the predictions from all the layers and calculate the losses for each of them.\nAnd then take the weighted average of the losses based on the probabilities of getting halted at each layer\n$p_n$.\n\nThe step function is applied to a maximum number of steps donated by $N$.\n\nThe overall loss of PonderNet is\n\n\\begin{align}\nL &= L_{Rec} + \\beta L_{Reg} \\\\\nL_{Rec} &= \\sum_{n=1}^N p_n \\mathcal{L}(y, \\hat{y}_n) \\\\\nL_{Reg} &= \\mathop{KL} \\Big(p_n \\Vert p_G(\\lambda_p) \\Big)\n\\end{align}\n\n$\\mathcal{L}$ is the normal loss function between target $y$ and prediction $\\hat{y}_n$.\n\n$\\mathop{KL}$ is the [Kullbackâ€“Leibler divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence).\n\n$p_G$ is the [Geometric distribution](https://en.wikipedia.org/wiki/Geometric_distribution) parameterized by\n$\\lambda_p$. *$\\lambda_p$ has nothing to do with $\\lambda_n$; we are just sticking to same notation as the paper*.\n$$Pr_{p_G(\\lambda_p)}(X = k) = (1 - \\lambda_p)^k \\lambda_p$$.\n\nThe regularization loss biases the network towards taking $\\frac{1}{\\lambda_p}$ steps and incentivizes\n non-zero probabilities for all steps; i.e. promotes exploration.\n\nHere is the [training code `experiment.py`](experiment.html) to train a PonderNet on [Parity Task](../parity.html).\n\"\"\"\n\nfrom typing import Tuple\n\nimport torch\nfrom torch import nn\n\nfrom labml_helpers.module import Module\n\n\nclass ParityPonderGRU(Module):\n    \"\"\"\n    ## PonderNet with GRU for Parity Task\n\n    This is a simple model that uses a [GRU Cell](https://pytorch.org/docs/stable/generated/torch.nn.GRUCell.html)\n    as the step function.\n\n    This model is for the [Parity Task](../parity.html) where the input is a vector of `n_elems`.\n    Each element of the vector is either `0`, `1` or `-1` and the output is the parity\n    - a binary value that is true if the number of `1`s is odd and false otherwise.\n\n    The prediction of the model is the log probability of the parity being $1$.\n    \"\"\"\n\n    def __init__(self, n_elems: int, n_hidden: int, max_steps: int):\n        \"\"\"\n        * `n_elems` is the number of elements in the input vector\n        * `n_hidden` is the state vector size of the GRU\n        * `max_steps` is the maximum number of steps $N$\n        \"\"\"\n        super().__init__()\n\n        self.max_steps = max_steps\n        self.n_hidden = n_hidden\n\n        # GRU\n        # $$h_{n+1} = s_h(x, h_n)$$\n        self.gru = nn.GRUCell(n_elems, n_hidden)\n        # $$\\hat{y}_n = s_y(h_n)$$\n        # We could use a layer that takes the concatenation of $h$ and $x$ as input\n        # but we went with this for simplicity.\n        self.output_layer = nn.Linear(n_hidden, 1)\n        # $$\\lambda_n = s_\\lambda(h_n)$$\n        self.lambda_layer = nn.Linear(n_hidden, 1)\n        self.lambda_prob = nn.Sigmoid()\n        # An option to set during inference so that computation is actually halted at inference time\n        self.is_halt = False\n\n    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n        \"\"\"\n        * `x` is the input of shape `[batch_size, n_elems]`\n\n        This outputs a tuple of four tensors:\n\n        1. $p_1 \\dots p_N$ in a tensor of shape `[N, batch_size]`\n        2. $\\hat{y}_1 \\dots \\hat{y}_N$ in a tensor of shape `[N, batch_size]` - the log probabilities of the parity being $1$\n        3. $p_m$ of shape `[batch_size]`\n        4. $\\hat{y}_m$ of shape `[batch_size]` where the computation was halted at step $m$\n        \"\"\"\n\n        #\n        batch_size = x.shape[0]\n\n        # We get initial state $h_1 = s_h(x)$\n        h = x.new_zeros((x.shape[0], self.n_hidden))\n        h = self.gru(x, h)\n\n        # Lists to store $p_1 \\dots p_N$ and $\\hat{y}_1 \\dots \\hat{y}_N$\n        p = []\n        y = []\n        # $\\prod_{j=1}^{n-1} (1 - \\lambda_j)$\n        un_halted_prob = h.new_ones((batch_size,))\n\n        # A vector to maintain which samples has halted computation\n        halted = h.new_zeros((batch_size,))\n        # $p_m$ and $\\hat{y}_m$ where the computation was halted at step $m$\n        p_m = h.new_zeros((batch_size,))\n        y_m = h.new_zeros((batch_size,))\n\n        # Iterate for $N$ steps\n        for n in range(1, self.max_steps + 1):\n            # The halting probability $\\lambda_N = 1$ for the last step\n            if n == self.max_steps:\n                lambda_n = h.new_ones(h.shape[0])\n            # $\\lambda_n = s_\\lambda(h_n)$\n            else:\n                lambda_n = self.lambda_prob(self.lambda_layer(h))[:, 0]\n            # $\\hat{y}_n = s_y(h_n)$\n            y_n = self.output_layer(h)[:, 0]\n\n            # $$p_n = \\lambda_n \\prod_{j=1}^{n-1} (1 - \\lambda_j)$$\n            p_n = un_halted_prob * lambda_n\n            # Update $\\prod_{j=1}^{n-1} (1 - \\lambda_j)$\n            un_halted_prob = un_halted_prob * (1 - lambda_n)\n\n            # Halt based on halting probability $\\lambda_n$\n            halt = torch.bernoulli(lambda_n) * (1 - halted)\n\n            # Collect $p_n$ and $\\hat{y}_n$\n            p.append(p_n)\n            y.append(y_n)\n\n            # Update $p_m$ and $\\hat{y}_m$ based on what was halted at current step $n$\n            p_m = p_m * (1 - halt) + p_n * halt\n            y_m = y_m * (1 - halt) + y_n * halt\n\n            # Update halted samples\n            halted = halted + halt\n            # Get next state $h_{n+1} = s_h(x, h_n)$\n            h = self.gru(x, h)\n\n            # Stop the computation if all samples have halted\n            if self.is_halt and halted.sum() == batch_size:\n                break\n\n        #\n        return torch.stack(p), torch.stack(y), p_m, y_m\n\n\nclass ReconstructionLoss(Module):\n    \"\"\"\n    ## Reconstruction loss\n\n    $$L_{Rec} = \\sum_{n=1}^N p_n \\mathcal{L}(y, \\hat{y}_n)$$\n\n    $\\mathcal{L}$ is the normal loss function between target $y$ and prediction $\\hat{y}_n$.\n    \"\"\"\n\n    def __init__(self, loss_func: nn.Module):\n        \"\"\"\n        * `loss_func` is the loss function $\\mathcal{L}$\n        \"\"\"\n        super().__init__()\n        self.loss_func = loss_func\n\n    def forward(self, p: torch.Tensor, y_hat: torch.Tensor, y: torch.Tensor):\n        \"\"\"\n        * `p` is $p_1 \\dots p_N$ in a tensor of shape `[N, batch_size]`\n        * `y_hat` is $\\hat{y}_1 \\dots \\hat{y}_N$ in a tensor of shape `[N, batch_size, ...]`\n        * `y` is the target of shape `[batch_size, ...]`\n        \"\"\"\n\n        # The total $\\sum_{n=1}^N p_n \\mathcal{L}(y, \\hat{y}_n)$\n        total_loss = p.new_tensor(0.)\n        # Iterate upto $N$\n        for n in range(p.shape[0]):\n            # $p_n \\mathcal{L}(y, \\hat{y}_n)$ for each sample and the mean of them\n            loss = (p[n] * self.loss_func(y_hat[n], y)).mean()\n            # Add to total loss\n            total_loss = total_loss + loss\n\n        #\n        return total_loss\n\n\nclass RegularizationLoss(Module):\n    \"\"\"\n    ## Regularization loss\n\n    $$L_{Reg} = \\mathop{KL} \\Big(p_n \\Vert p_G(\\lambda_p) \\Big)$$\n\n    $\\mathop{KL}$ is the [Kullbackâ€“Leibler divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence).\n\n    $p_G$ is the [Geometric distribution](https://en.wikipedia.org/wiki/Geometric_distribution) parameterized by\n    $\\lambda_p$. *$\\lambda_p$ has nothing to do with $\\lambda_n$; we are just sticking to same notation as the paper*.\n    $$Pr_{p_G(\\lambda_p)}(X = k) = (1 - \\lambda_p)^k \\lambda_p$$.\n\n    The regularization loss biases the network towards taking $\\frac{1}{\\lambda_p}$ steps and incentivies non-zero probabilities\n    for all steps; i.e. promotes exploration.\n    \"\"\"\n\n    def __init__(self, lambda_p: float, max_steps: int = 1_000):\n        \"\"\"\n        * `lambda_p` is $\\lambda_p$ - the success probability of geometric distribution\n        * `max_steps` is the highest $N$; we use this to pre-compute $p_G(\\lambda_p)$\n        \"\"\"\n        super().__init__()\n\n        # Empty vector to calculate $p_G(\\lambda_p)$\n        p_g = torch.zeros((max_steps,))\n        # $(1 - \\lambda_p)^k$\n        not_halted = 1.\n        # Iterate upto `max_steps`\n        for k in range(max_steps):\n            # $$Pr_{p_G(\\lambda_p)}(X = k) = (1 - \\lambda_p)^k \\lambda_p$$\n            p_g[k] = not_halted * lambda_p\n            # Update $(1 - \\lambda_p)^k$\n            not_halted = not_halted * (1 - lambda_p)\n\n        # Save $Pr_{p_G(\\lambda_p)}$\n        self.p_g = nn.Parameter(p_g, requires_grad=False)\n\n        # KL-divergence loss\n        self.kl_div = nn.KLDivLoss(reduction='batchmean')\n\n    def forward(self, p: torch.Tensor):\n        \"\"\"\n        * `p` is $p_1 \\dots p_N$ in a tensor of shape `[N, batch_size]`\n        \"\"\"\n        # Transpose `p` to `[batch_size, N]`\n        p = p.transpose(0, 1)\n        # Get $Pr_{p_G(\\lambda_p)}$ upto $N$ and expand it across the batch dimension\n        p_g = self.p_g[None, :p.shape[1]].expand_as(p)\n\n        # Calculate the KL-divergence.\n        # *The [PyTorch KL-divergence](https://pytorch.org/docs/stable/generated/torch.nn.KLDivLoss.html)\n        # implementation accepts log probabilities.*\n        return self.kl_div(p.log(), p_g)\n",
    "labml_nn/adaptive_computation/ponder_net/experiment.py": "\"\"\"\n---\ntitle: \"PonderNet Parity Task Experiment\"\nsummary: >\n  This trains is a PonderNet on Parity Task\n---\n\n# [PonderNet](index.html) [Parity Task](../parity.html) Experiment\n\nThis trains a [PonderNet](index.html) on [Parity Task](../parity.html).\n\"\"\"\n\nfrom typing import Any\n\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader\n\nfrom labml import tracker, experiment\nfrom labml_helpers.metrics.accuracy import AccuracyDirect\nfrom labml_helpers.train_valid import SimpleTrainValidConfigs, BatchIndex\nfrom labml_nn.adaptive_computation.parity import ParityDataset\nfrom labml_nn.adaptive_computation.ponder_net import ParityPonderGRU, ReconstructionLoss, RegularizationLoss\n\n\nclass Configs(SimpleTrainValidConfigs):\n    \"\"\"\n    Configurations with a\n     [simple training loop](https://docs.labml.ai/api/helpers.html#labml_helpers.train_valid.SimpleTrainValidConfigs)\n    \"\"\"\n\n    # Number of epochs\n    epochs: int = 100\n    # Number of batches per epoch\n    n_batches: int = 500\n    # Batch size\n    batch_size: int = 128\n\n    # Model\n    model: ParityPonderGRU\n\n    # $L_{Rec}$\n    loss_rec: ReconstructionLoss\n    # $L_{Reg}$\n    loss_reg: RegularizationLoss\n\n    # The number of elements in the input vector.\n    # *We keep it low for demonstration; otherwise, training takes a lot of time.\n    # Although the parity task seems simple, figuring out the pattern by looking at samples\n    # is quite hard.*\n    n_elems: int = 8\n    # Number of units in the hidden layer (state)\n    n_hidden: int = 64\n    # Maximum number of steps $N$\n    max_steps: int = 20\n\n    # $\\lambda_p$ for the geometric distribution $p_G(\\lambda_p)$\n    lambda_p: float = 0.2\n    # Regularization loss $L_{Reg}$ coefficient $\\beta$\n    beta: float = 0.01\n\n    # Gradient clipping by norm\n    grad_norm_clip: float = 1.0\n\n    # Training and validation loaders\n    train_loader: DataLoader\n    valid_loader: DataLoader\n\n    # Accuracy calculator\n    accuracy = AccuracyDirect()\n\n    def init(self):\n        # Print indicators to screen\n        tracker.set_scalar('loss.*', True)\n        tracker.set_scalar('loss_reg.*', True)\n        tracker.set_scalar('accuracy.*', True)\n        tracker.set_scalar('steps.*', True)\n\n        # We need to set the metrics to calculate them for the epoch for training and validation\n        self.state_modules = [self.accuracy]\n\n        # Initialize the model\n        self.model = ParityPonderGRU(self.n_elems, self.n_hidden, self.max_steps).to(self.device)\n        # $L_{Rec}$\n        self.loss_rec = ReconstructionLoss(nn.BCEWithLogitsLoss(reduction='none')).to(self.device)\n        # $L_{Reg}$\n        self.loss_reg = RegularizationLoss(self.lambda_p, self.max_steps).to(self.device)\n\n        # Training and validation loaders\n        self.train_loader = DataLoader(ParityDataset(self.batch_size * self.n_batches, self.n_elems),\n                                       batch_size=self.batch_size)\n        self.valid_loader = DataLoader(ParityDataset(self.batch_size * 32, self.n_elems),\n                                       batch_size=self.batch_size)\n\n    def step(self, batch: Any, batch_idx: BatchIndex):\n        \"\"\"\n        This method gets called by the trainer for each batch\n        \"\"\"\n        # Set the model mode\n        self.model.train(self.mode.is_train)\n\n        # Get the input and labels and move them to the model's device\n        data, target = batch[0].to(self.device), batch[1].to(self.device)\n\n        # Increment step in training mode\n        if self.mode.is_train:\n            tracker.add_global_step(len(data))\n\n        # Run the model\n        p, y_hat, p_sampled, y_hat_sampled = self.model(data)\n\n        # Calculate the reconstruction loss\n        loss_rec = self.loss_rec(p, y_hat, target.to(torch.float))\n        tracker.add(\"loss.\", loss_rec)\n\n        # Calculate the regularization loss\n        loss_reg = self.loss_reg(p)\n        tracker.add(\"loss_reg.\", loss_reg)\n\n        # $L = L_{Rec} + \\beta L_{Reg}$\n        loss = loss_rec + self.beta * loss_reg\n\n        # Calculate the expected number of steps taken\n        steps = torch.arange(1, p.shape[0] + 1, device=p.device)\n        expected_steps = (p * steps[:, None]).sum(dim=0)\n        tracker.add(\"steps.\", expected_steps)\n\n        # Call accuracy metric\n        self.accuracy(y_hat_sampled > 0, target)\n\n        if self.mode.is_train:\n            # Compute gradients\n            loss.backward()\n            # Clip gradients\n            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=self.grad_norm_clip)\n            # Optimizer\n            self.optimizer.step()\n            # Clear gradients\n            self.optimizer.zero_grad()\n            #\n            tracker.save()\n\n\ndef main():\n    \"\"\"\n    Run the experiment\n    \"\"\"\n    experiment.create(name='ponder_net')\n\n    conf = Configs()\n    experiment.configs(conf, {\n        'optimizer.optimizer': 'Adam',\n        'optimizer.learning_rate': 0.0003,\n    })\n\n    with experiment.start():\n        conf.run()\n\n#\nif __name__ == '__main__':\n    main()\n"
  },
  "requirements": "torch>=1.10\ntorchvision>=0.11\ntorchtext>=0.11\nlabml>=0.4.147\nlabml-helpers>=0.4.84\nnumpy>=1.19\nmatplotlib>=3.0.3\neinops>=0.3.0\ngym[atari]\nopencv-python\nPillow>=6.2.1\nfaiss\n"
}