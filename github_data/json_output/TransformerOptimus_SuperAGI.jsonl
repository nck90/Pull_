{"repo_info": {"repo_name": "SuperAGI", "repo_owner": "TransformerOptimus", "repo_url": "https://github.com/TransformerOptimus/SuperAGI"}}
{"type": "test_file", "path": "tests/integration_tests/vector_store/test_weaviate.py", "content": "import unittest\nfrom unittest.mock import Mock, patch, call, MagicMock\nfrom superagi.vector_store.weaviate import create_weaviate_client, Weaviate, Document\n\nclass TestWeaviateClient(unittest.TestCase):\n    @patch('weaviate.Client')\n    @patch('weaviate.AuthApiKey')\n    def test_create_weaviate_client(self, MockAuth, MockClient):\n        # Test when url and api_key are provided\n        auth_instance = MockAuth.return_value\n        MockClient.return_value = 'client'\n        self.assertEqual(create_weaviate_client('url', 'api_key'), 'client')\n        MockAuth.assert_called_once_with(api_key='api_key')\n        MockClient.assert_called_once_with(url='url', auth_client_secret=auth_instance)\n\n        with self.assertRaises(ValueError):\n            create_weaviate_client()  # Raises an error if no url is provided\n\nclass TestWeaviate(unittest.TestCase):\n\n    def setUp(self):\n        # create a new mock object for the client.batch attribute with the required methods for a context manager.\n        mock_batch = MagicMock()\n        mock_batch.__enter__.return_value = mock_batch\n        mock_batch.__exit__.return_value = None\n\n        self.client = Mock()\n        self.client.batch = mock_batch\n\n        self.embedding_model = Mock()\n        self.weaviateVectorStore = Weaviate(self.client, self.embedding_model, 'class_name', 'text_field')\n\n    def test_get_matching_text(self):\n        self.client.query.get.return_value.with_near_vector.return_value.with_where.return_value.with_limit.return_value.do.return_value = {'data': {'Get': {'class_name': []}}}\n        self.embedding_model.get_embedding.return_value = 'vector'\n        self.weaviateVectorStore._get_metadata_fields = Mock(return_value=['field1', 'field2'])\n        self.weaviateVectorStore._get_search_res = Mock(return_value='search_res')\n        self.weaviateVectorStore._build_documents = Mock(return_value=['document1', 'document2'])\n        self.assertEqual(self.weaviateVectorStore.get_matching_text('query', metadata={'field1': 'value'})\n                         , {'search_res': 'search_res', 'documents': ['document1', 'document2']})\n        self.embedding_model.get_embedding.assert_called_once_with('query')\n\n    def test_add_texts(self):\n        self.embedding_model.get_embedding.return_value = 'vector'\n        self.weaviateVectorStore.add_embeddings_to_vector_db = Mock()\n        texts = ['text1', 'text2']\n        result = self.weaviateVectorStore.add_texts(texts)\n        self.assertEqual(len(result), 2)    # We expect to get 2 IDs.\n        self.assertTrue(isinstance(result[0], str))    # The IDs should be strings.\n        self.embedding_model.get_embedding.assert_has_calls([call(texts[0]), call(texts[1])])\n        self.assertEqual(self.weaviateVectorStore.add_embeddings_to_vector_db.call_count, 2)\n\n    def test_add_embeddings_to_vector_db(self):\n        embeddings = {'ids': ['id1', 'id2'], 'data_object': [{'field': 'value1'}, {'field': 'value2'}], 'vectors': ['v1', 'v2']}\n        self.weaviateVectorStore.add_embeddings_to_vector_db(embeddings)\n        calls = [call.add_data_object({'field': 'value1'}, class_name='class_name', uuid='id1', vector='v1'),\n                call.add_data_object({'field': 'value2'}, class_name='class_name', uuid='id2', vector='v2')]\n\n        self.client.batch.assert_has_calls(calls)\n\n    def test_delete_embeddings_from_vector_db(self):\n        # You need to setup appropriate return values from the Weaviate client\n        self.weaviateVectorStore.delete_embeddings_from_vector_db(['id1', 'id2'])\n        self.client.data_object.delete.assert_called()\n\nif __name__ == '__main__':\n    unittest.main()"}
{"type": "test_file", "path": "tests/integration_tests/vector_embeddings/test_qdrant.py", "content": "import unittest\n\nfrom superagi.vector_embeddings.qdrant import Qdrant\n\nclass TestQdrant(unittest.TestCase):\n\n    def setUp(self):\n        self.uuid = ['1234', '5678']\n        self.embeds = [[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]]\n        self.metadata = [{'key1': 'value1'}, {'key2': 'value2'}]\n\n        self.qdrant_obj = Qdrant(self.uuid, self.embeds, self.metadata)\n\n    def test_init(self):\n        self.assertEqual(self.qdrant_obj.uuid, self.uuid)\n        self.assertEqual(self.qdrant_obj.embeds, self.embeds)\n        self.assertEqual(self.qdrant_obj.metadata, self.metadata)\n\n    def test_get_vector_embeddings_from_chunks(self):\n        expected = {\n            'ids': self.uuid,\n            'payload': self.metadata,\n            'vectors': self.embeds,\n        }\n        result = self.qdrant_obj.get_vector_embeddings_from_chunks()\n        \n        self.assertEqual(result, expected)\n\nif __name__ == '__main__':\n    unittest.main()"}
{"type": "test_file", "path": "tests/unit_tests/controllers/test_tool_config.py", "content": "from unittest.mock import MagicMock, patch\n\nimport pytest\nfrom fastapi.testclient import TestClient\n\nfrom main import app\nfrom superagi.controllers.tool_config import update_tool_config\nfrom superagi.models.organisation import Organisation\nfrom superagi.models.tool_config import ToolConfig\nfrom superagi.models.toolkit import Toolkit\n\nclient = TestClient(app)\n\n\n@pytest.fixture\ndef mocks():\n    # Mock tool kit data for testing\n    user_organisation = Organisation(id=1)\n    toolkit_1 = Toolkit(\n        id=1,\n        name=\"toolkit_1\",\n        description=\"None\",\n        show_toolkit=None,\n        organisation_id=1\n    )\n    toolkit_2 = Toolkit(\n        id=1,\n        name=\"toolkit_2\",\n        description=\"None\",\n        show_toolkit=None,\n        organisation_id=1\n    )\n    user_toolkits = [toolkit_1, toolkit_2]\n    tool_config = ToolConfig(\n        id=1,\n        key=\"test_key\",\n        value=\"test_value\",\n        toolkit_id=1\n    )\n    return user_organisation, user_toolkits, tool_config, toolkit_1, toolkit_2\n\n\n# Test cases\ndef test_update_tool_configs_success():\n    # Test data\n    toolkit_name = \"toolkit_1\"\n    configs = [\n        {\"key\": \"config_1\", \"value\": \"value_1\"},\n        {\"key\": \"config_2\", \"value\": \"value_2\"},\n    ]\n\n    with patch('superagi.models.toolkit.Toolkit.get_toolkit_from_name') as get_toolkit_from_name, \\\n            patch('superagi.controllers.tool_config.db') as mock_db:\n        mock_db.query.return_value.filter_by.return_value.first.side_effect = [\n            # First call to query\n            MagicMock(\n                toolkit_id=1, key=\"config_1\", value=\"old_value_1\"\n            ),\n            # Second call to query\n            MagicMock(\n                toolkit_id=1, key=\"config_2\", value=\"old_value_2\"\n            ),\n        ]\n\n        result = update_tool_config(toolkit_name, configs)\n\n        assert result == {\"message\": \"Tool configs updated successfully\"}\n\n\ndef test_get_all_tool_configs_success(mocks):\n    user_organisation, user_toolkits, tool_config, toolkit_1, toolkit_2 = mocks\n\n    with patch('superagi.helper.auth.get_user_organisation') as mock_get_user_org, \\\n            patch('superagi.controllers.tool_config.db') as mock_db, \\\n            patch('superagi.helper.auth.db') as mock_auth_db:\n        mock_db.session.query.return_value.filter_by.return_value.first.return_value = toolkit_1\n        mock_db.session.query.return_value.filter.return_value.all.side_effect = [\n            [tool_config]\n        ]\n        response = client.get(f\"/tool_configs/get/toolkit/test_toolkit_1\")\n\n        # Assertions\n        assert response.status_code == 200\n        assert response.json() == [\n            {\n                'id': 1,\n                'key': tool_config.key,\n                'value': tool_config.value,\n                'toolkit_id': tool_config.toolkit_id\n            }\n        ]\n\n\ndef test_get_all_tool_configs_toolkit_not_found(mocks):\n    user_organisation, _, _, _, _ = mocks\n\n    with patch('superagi.helper.auth.get_user_organisation') as mock_get_user_org, \\\n            patch('superagi.controllers.tool_config.db') as mock_db, \\\n            patch('superagi.helper.auth.db') as mock_auth_db:\n        mock_db.session.query.return_value.filter.return_value.first.return_value = None\n        response = client.get(f\"/tool_configs/get/toolkit/nonexistent_toolkit\")\n\n        # Assertions\n        assert response.status_code == 404\n        assert response.json() == {'detail': 'ToolKit not found'}\n\ndef test_get_tool_config_success(mocks):\n    # Unpack the fixture data\n    user_organisation, user_toolkits, tool_config, toolkit_1, toolkit_2 = mocks\n\n    # Mock the database session and query functions\n    with patch('superagi.helper.auth.get_user_organisation') as mock_get_user_org, \\\n            patch('superagi.controllers.tool_config.db') as mock_db, \\\n            patch('superagi.helper.auth.db') as mock_auth_db:\n        mock_db.session.query.return_value.filter.return_value.all.return_value = user_toolkits\n        mock_db.session.query.return_value.filter_by.return_value = toolkit_1\n        mock_db.session.query.return_value.filter.return_value.first.return_value = tool_config\n\n        # Call the function\n        response = client.get(f\"/tool_configs/get/toolkit/{toolkit_1.name}/key/{tool_config.key}\")\n\n        # Assertions\n        assert response.status_code == 200\n        assert response.json() == {\n            \"id\": tool_config.id,\n            \"key\": tool_config.key,\n            \"value\": tool_config.value,\n            \"toolkit_id\": tool_config.toolkit_id\n        }\n\n\ndef test_get_tool_config_unauthorized(mocks):\n    # Unpack the fixture data\n    user_organisation, user_toolkits, tool_config, toolkit_1, toolkit_2 = mocks\n\n    # Mock the database session and query functions\n    with patch('superagi.helper.auth.get_user_organisation') as mock_get_user_org, \\\n            patch('superagi.controllers.tool_config.db') as mock_db, \\\n            patch('superagi.helper.auth.db') as mock_auth_db:\n        # Mock the toolkit filtering\n        mock_db.session.query.return_value.filter.return_value.all.return_value = user_toolkits\n\n        response = client.get(f\"/tool_configs/get/toolkit/{toolkit_2.name}/key/{tool_config.key}\")\n\n        # Assertions\n        assert response.status_code == 403\n        assert response.json() == {\"detail\": \"Unauthorized\"}\n\n\ndef test_get_tool_config_not_found(mocks):\n    # Unpack the fixture data\n    user_organisation, user_toolkits, tool_config, toolkit_1, toolkit_2 = mocks\n\n    # Mock the database session and query functions\n    with patch('superagi.helper.auth.get_user_organisation') as mock_get_user_org, \\\n            patch('superagi.controllers.tool_config.db') as mock_db, \\\n            patch('superagi.helper.auth.db') as mock_auth_db:\n        # Mock the toolkit filtering\n        mock_db.session.query.return_value.filter.return_value.all.return_value = user_toolkits\n        mock_db.session.query.return_value.filter_by.return_value = toolkit_1\n        mock_db.session.query.return_value.filter.return_value.first.return_value = None\n\n        # Call the function with a non-existent toolkit\n        response = client.get(f\"/tool_configs/get/toolkit/{toolkit_1.name}/key/{tool_config.key}\")\n\n        # Assertions\n        assert response.status_code == 404\n        assert response.json() == {\"detail\": \"Tool configuration not found\"}\n"}
{"type": "test_file", "path": "tests/unit_tests/agent/test_agent_prompt_template.py", "content": "import pytest\nfrom unittest.mock import patch, mock_open\n\nfrom superagi.agent.agent_prompt_template import AgentPromptTemplate\nfrom superagi.helper.prompt_reader import PromptReader\n\n\n@patch(\"builtins.open\", new_callable=mock_open, read_data=\"test_prompt\")\ndef test_get_super_agi_single_prompt(mock_file):\n    expected_result = {\"prompt\": \"test_prompt\", \"variables\": [\"goals\", \"instructions\", \"constraints\", \"tools\"]}\n    result = AgentPromptTemplate.get_super_agi_single_prompt()\n    assert result == expected_result\n\n@patch(\"builtins.open\", new_callable=mock_open, read_data=\"test_prompt\")\ndef test_start_task_based(mock_file):\n    expected_result = {\"prompt\": \"test_prompt\", \"variables\": [\"goals\", \"instructions\"]}\n    result = AgentPromptTemplate.start_task_based()\n    assert result == expected_result\n\n@patch(\"builtins.open\", new_callable=mock_open, read_data=\"test_prompt\")\ndef test_analyse_task(mock_file):\n    expected_result = {\"prompt\": \"test_prompt\",\n                       \"variables\": [\"goals\", \"instructions\", \"tools\", \"current_task\"]}\n    result = AgentPromptTemplate.analyse_task()\n    assert result == expected_result\n\n@patch(\"builtins.open\", new_callable=mock_open, read_data=\"test_prompt\")\ndef test_create_tasks(mock_file):\n    expected_result = {\"prompt\": \"test_prompt\", \"variables\": [\"goals\", \"instructions\", \"last_task\", \"last_task_result\", \"pending_tasks\"]}\n    result = AgentPromptTemplate.create_tasks()\n    assert result == expected_result\n\n@patch(\"builtins.open\", new_callable=mock_open, read_data=\"test_prompt\")\ndef test_prioritize_tasks(mock_file):\n    expected_result = {\"prompt\": \"test_prompt\", \"variables\": [\"goals\", \"instructions\", \"last_task\", \"last_task_result\", \"pending_tasks\"]}\n    result = AgentPromptTemplate.prioritize_tasks()\n    assert result == expected_result\n"}
{"type": "test_file", "path": "tests/unit_tests/agent/test_agent_iteration_step_handler.py", "content": "from unittest.mock import Mock, patch, MagicMock\n\nimport pytest\n\nfrom superagi.agent.agent_iteration_step_handler import AgentIterationStepHandler\nfrom superagi.agent.agent_message_builder import AgentLlmMessageBuilder\nfrom superagi.agent.agent_prompt_builder import AgentPromptBuilder\nfrom superagi.agent.output_handler import ToolOutputHandler\nfrom superagi.agent.task_queue import TaskQueue\nfrom superagi.agent.tool_builder import ToolBuilder\nfrom superagi.config.config import get_config\nfrom superagi.helper.token_counter import TokenCounter\nfrom superagi.models.agent import Agent\nfrom superagi.models.agent_config import AgentConfiguration\nfrom superagi.models.agent_execution import AgentExecution\nfrom superagi.models.agent_execution_config import AgentExecutionConfiguration\nfrom superagi.models.agent_execution_feed import AgentExecutionFeed\nfrom superagi.models.agent_execution_permission import AgentExecutionPermission\nfrom superagi.models.organisation import Organisation\nfrom superagi.models.tool import Tool\nfrom superagi.models.workflows.agent_workflow_step import AgentWorkflowStep\nfrom superagi.models.workflows.iteration_workflow import IterationWorkflow\nfrom superagi.models.workflows.iteration_workflow_step import IterationWorkflowStep\nfrom superagi.resource_manager.resource_summary import ResourceSummarizer\nfrom superagi.tools.code.write_code import CodingTool\nfrom superagi.tools.resource.query_resource import QueryResourceTool\nfrom superagi.tools.thinking.tools import ThinkingTool\n\n\n# Given\n@pytest.fixture\ndef test_handler():\n    mock_session = Mock()\n    llm = Mock()\n    agent_id = 1\n    agent_execution_id = 1\n\n    # Creating an instance of the class to test\n    handler = AgentIterationStepHandler(mock_session, llm, agent_id, agent_execution_id)\n    return handler\n\ndef test_build_agent_prompt(test_handler, mocker):\n    # Arrange\n    iteration_workflow = IterationWorkflow(has_task_queue=True)\n    agent_config = {'constraints': 'Test constraint'}\n    agent_execution_config = {'goal': 'Test goal', 'instruction': 'Test instruction'}\n    prompt = 'Test prompt'\n    task_queue = TaskQueue(queue_name='Test queue')\n    agent_tools = []\n\n    mocker.patch.object(AgentPromptBuilder, 'replace_main_variables', return_value='Test prompt')\n    mocker.patch.object(AgentPromptBuilder, 'replace_task_based_variables', return_value='Test prompt')\n    mocker.patch.object(task_queue, 'get_last_task_details', return_value={\"task\": \"last task\", \"response\": \"last response\"})\n    mocker.patch.object(task_queue, 'get_first_task', return_value='Test task')\n    mocker.patch.object(task_queue, 'get_tasks', return_value=[])\n    mocker.patch.object(task_queue, 'get_completed_tasks', return_value=[])\n    mocker.patch.object(TokenCounter, 'token_limit', return_value=1000)\n    mocker.patch('superagi.agent.agent_iteration_step_handler.get_config', return_value=600)\n\n    # Act\n    test_handler.task_queue = task_queue\n    result_prompt = test_handler._build_agent_prompt(iteration_workflow, agent_config, agent_execution_config,\n                                                     prompt, agent_tools)\n\n    # Assert\n    assert result_prompt == 'Test prompt'\n    AgentPromptBuilder.replace_main_variables.assert_called_once_with(prompt, agent_execution_config[\"goal\"],\n                                                                      agent_execution_config[\"instruction\"],\n                                                                      agent_config[\"constraints\"], agent_tools, False)\n    AgentPromptBuilder.replace_task_based_variables.assert_called_once()\n    task_queue.get_last_task_details.assert_called_once()\n    task_queue.get_first_task.assert_called_once()\n    task_queue.get_tasks.assert_called_once()\n    task_queue.get_completed_tasks.assert_called_once()\n    TokenCounter.token_limit.assert_called_once()\n\ndef test_build_tools(test_handler, mocker):\n    # Arrange\n    agent_config = {'model': 'gpt-3', 'tools': [1, 2, 3], 'resource_summary': True}\n    agent_execution_config = {'goal': 'Test goal', 'instruction': 'Test instruction', 'tools':[1]}\n\n    mocker.patch.object(AgentConfiguration, 'get_model_api_key', return_value={'api_key':'test_api_key','provider':'test_provider'})\n    mocker.patch.object(ToolBuilder, 'build_tool')\n    mocker.patch.object(ToolBuilder, 'set_default_params_tool', return_value=ThinkingTool())\n    mocker.patch.object(ResourceSummarizer, 'fetch_or_create_agent_resource_summary', return_value=True)\n    mocker.patch('superagi.models.tool.Tool')\n    test_handler.session.query.return_value.filter.return_value.all.return_value = [ThinkingTool()]\n\n    # Act\n    agent_tools = test_handler._build_tools(agent_config, agent_execution_config)\n\n    # Assert\n    assert isinstance(agent_tools[0], ThinkingTool)\n    assert ToolBuilder.build_tool.call_count == 1\n    assert ToolBuilder.set_default_params_tool.call_count == 3\n    assert AgentConfiguration.get_model_api_key.call_count == 1\n    assert ResourceSummarizer.fetch_or_create_agent_resource_summary.call_count == 1\n\n\ndef test_handle_wait_for_permission(test_handler, mocker):\n    # Arrange\n    mock_agent_execution = mocker.Mock(spec=AgentExecution)\n    mock_agent_execution.status = \"WAITING_FOR_PERMISSION\"\n    mock_iteration_workflow_step = mocker.Mock(spec=IterationWorkflowStep)\n    mock_iteration_workflow_step.next_step_id = 123\n    agent_config = {'model': 'gpt-3', 'tools': [1, 2, 3]}\n    agent_execution_config = {'goal': 'Test goal', 'instruction': 'Test instruction'}\n\n    mock_permission = mocker.Mock(spec=AgentExecutionPermission)\n    mock_permission.status = \"APPROVED\"\n    mock_permission.user_feedback = \"Test feedback\"\n    mock_permission.tool_name = \"Test tool\"\n    test_handler._build_tools = Mock(return_value=[ThinkingTool()])\n    test_handler.session.query.return_value.filter.return_value.first.return_value = mock_permission\n    # AgentExecutionPermission.filter.return_value.first.return_value = mock_permission\n\n    mock_tool_output = mocker.MagicMock()\n    mock_tool_output.result = \"Test result\"\n    ToolOutputHandler.handle_tool_response = Mock(return_value=mock_tool_output)\n\n    # Act\n    result = test_handler._handle_wait_for_permission(\n        mock_agent_execution, agent_config, agent_execution_config, mock_iteration_workflow_step)\n\n    # Assert\n    test_handler._build_tools.assert_called_once_with(agent_config, agent_execution_config)\n    ToolOutputHandler.handle_tool_response.assert_called_once()\n    assert mock_agent_execution.status == \"RUNNING\"\n    assert result\n\n"}
{"type": "test_file", "path": "tests/unit_tests/apm/test_analytics_helper.py", "content": "import pytest\nfrom superagi.models.events import Event\nfrom superagi.apm.analytics_helper import AnalyticsHelper\nfrom unittest.mock import MagicMock\n\n@pytest.fixture\ndef organisation_id():\n    return 1\n\n@pytest.fixture\ndef mock_session():\n    return MagicMock()\n\n@pytest.fixture\ndef analytics_helper(mock_session, organisation_id):\n    return AnalyticsHelper(mock_session, organisation_id)\n\ndef test_calculate_run_completed_metrics(analytics_helper, mock_session):\n    mock_session.query().all.return_value = [MagicMock()]\n    result = analytics_helper.calculate_run_completed_metrics()\n    assert isinstance(result, dict)\n\ndef test_fetch_agent_data(analytics_helper, mock_session):\n    mock_session.query().all.return_value = [MagicMock()]\n    result = analytics_helper.fetch_agent_data()\n    assert isinstance(result, dict)\n\ndef test_fetch_agent_runs(analytics_helper, mock_session):\n    mock_session.query().all.return_value = [MagicMock()]\n    result = analytics_helper.fetch_agent_runs(1)\n    assert isinstance(result, list)\n\ndef test_get_active_runs(analytics_helper, mock_session):\n    mock_session.query().all.return_value = [MagicMock()]\n    result = analytics_helper.get_active_runs()\n    assert isinstance(result, list)"}
{"type": "test_file", "path": "tests/unit_tests/agent/test_task_queue.py", "content": "import unittest\nfrom unittest.mock import patch\n\nfrom superagi.agent.task_queue import TaskQueue\n\n\nclass TaskQueueTests(unittest.TestCase):\n    def setUp(self):\n        self.queue_name = \"test_queue\"\n        self.queue = TaskQueue(self.queue_name)\n\n    @patch.object(TaskQueue, 'add_task')\n    def test_add_task(self, mock_add_task):\n        task = \"Do something\"\n        self.queue.add_task(task)\n        mock_add_task.assert_called_with(task)\n\n    @patch.object(TaskQueue, 'complete_task')\n    def test_complete_task(self, mock_complete_task):\n        task = \"Do something\"\n        response = \"Task completed\"\n        self.queue.complete_task(response)\n        mock_complete_task.assert_called_with(response)\n\n    @patch.object(TaskQueue, 'get_first_task')\n    def test_get_first_task(self, mock_get_first_task):\n        self.queue.get_first_task()\n        mock_get_first_task.assert_called()\n\n    @patch.object(TaskQueue, 'get_tasks')\n    def test_get_tasks(self, mock_get_tasks):\n        self.queue.get_tasks()\n        mock_get_tasks.assert_called()\n\n    @patch.object(TaskQueue, 'get_completed_tasks')\n    def test_get_completed_tasks(self, mock_get_completed_tasks):\n        self.queue.get_completed_tasks()\n        mock_get_completed_tasks.assert_called()\n\n    @patch.object(TaskQueue, 'clear_tasks')\n    def test_clear_tasks(self, mock_clear_tasks):\n        self.queue.clear_tasks()\n        mock_clear_tasks.assert_called()\n\n    @patch.object(TaskQueue, 'get_last_task_details')\n    def test_get_last_task_details(self, mock_get_last_task_details):\n        self.queue.get_last_task_details()\n        mock_get_last_task_details.assert_called()\n\n\nif __name__ == '__main__':\n    unittest.main()\n"}
{"type": "test_file", "path": "tests/tools/google_calendar/create_event_test.py", "content": "import unittest\nfrom unittest.mock import MagicMock, patch\nfrom pydantic import ValidationError\nfrom datetime import datetime, timedelta\nfrom superagi.tools.google_calendar.create_calendar_event import CreateEventCalendarInput, CreateEventCalendarTool\nfrom superagi.helper.google_calendar_creds import GoogleCalendarCreds\nfrom superagi.helper.calendar_date import CalendarDate\n\nclass TestCreateEventCalendarInput(unittest.TestCase):\n    def test_create_event_calendar_input_valid(self):\n        input_data = {\n            \"event_name\": \"Test Event\",\n            \"description\": \"A test event.\",\n            \"start_date\": \"2022-01-01\",\n            \"start_time\": \"12:00:00\",\n            \"end_date\": \"2022-01-01\",\n            \"end_time\": \"13:00:00\",\n            \"attendees\": [\"test@example.com\"],\n            \"location\": \"London\"\n        }\n        try:\n            CreateEventCalendarInput(**input_data)\n        except ValidationError:\n            self.fail(\"ValidationError raised with valid input_data\")\n\n    def test_create_event_calendar_input_invalid(self):\n        input_data = {\n            \"event_name\": \"Test Event\",\n            \"description\": \"A test event.\",\n            \"start_date\": \"2022-99-99\",\n            \"start_time\": \"12:60:60\",\n            \"end_date\": \"2022-99-99\",\n            \"end_time\": \"13:60:60\",\n            \"attendees\": [\"test@example.com\"],\n            \"location\": \"London\"\n        }\n        with self.assertRaises(ValidationError):\n            CreateEventCalendarInput(**input_data)\n\nclass TestCreateEventCalendarTool(unittest.TestCase):\n    def setUp(self):\n        self.create_event_tool = CreateEventCalendarTool()\n    @patch.object(GoogleCalendarCreds, \"get_credentials\")\n    @patch.object(CalendarDate, \"create_event_dates\")\n\n    def test_execute(self, mock_create_event_dates, mock_get_credentials):\n        mock_get_credentials.return_value = {\n            \"success\": True,\n            \"service\": MagicMock()\n        }\n        mock_date_utc = {\n            \"start_datetime_utc\": (datetime.utcnow() + timedelta(hours=1)).isoformat(),\n            \"end_datetime_utc\": (datetime.utcnow() + timedelta(hours=2)).isoformat(),\n            \"timeZone\": \"UTC\"\n        }\n        mock_create_event_dates.return_value = mock_date_utc\n        mock_service = MagicMock()\n        mock_service.events.return_value = MagicMock()\n        output_str_expected = f\"Event Test Event at {mock_date_utc['start_datetime_utc']} created successfully, link for the event {'https://somerandomlink'}\"\n        output_str = self.create_event_tool._execute(\"Test Event\", \"A test event\", [\"test@example.com\"], start_date=\"2022-01-01\", start_time=\"12:00:00\", end_date=\"2022-01-01\", end_time=\"13:00:00\", location=\"London\")\n        self.assertEqual(output_str, output_str_expected)\n        event = {\n            \"summary\": \"Test Event\",\n            \"description\": \"A test event\",\n            \"start\": {\n                \"dateTime\": mock_date_utc[\"start_datetime_utc\"],\n                \"timeZone\": mock_date_utc[\"timeZone\"]\n            },\n            \"end\": {\n                \"dateTime\": mock_date_utc[\"end_datetime_utc\"],\n                \"timeZone\": mock_date_utc[\"timeZone\"]\n            },\n            \"attendees\": [{\"email\": \"test@example.com\"}],\n            \"location\": \"London\"\n        }\n        mock_get_credentials.assert_called_once()\n        mock_create_event_dates.assert_called_once_with(mock_service, \"2022-01-01\", \"12:00:00\", \"2022-01-01\", \"13:00:00\")\n        mock_service.events().insert.assert_called_once_with(calendarId=\"primary\", body=event, conferenceDataVersion=1)\n\nif __name__ == \"__main__\":\n    unittest.main()"}
{"type": "test_file", "path": "tests/unit_tests/controllers/api/__init__.py", "content": ""}
{"type": "test_file", "path": "tests/unit_tests/agent/__init__.py", "content": ""}
{"type": "test_file", "path": "tests/unit_tests/__init__.py", "content": ""}
{"type": "test_file", "path": "tests/integration_tests/vector_embeddings/__init__.py", "content": ""}
{"type": "test_file", "path": "tests/unit_tests/controllers/__init__.py", "content": ""}
{"type": "test_file", "path": "tests/integration_tests/vector_store/__init__.py", "content": ""}
{"type": "test_file", "path": "tests/unit_tests/apm/__init__.py", "content": ""}
{"type": "test_file", "path": "tests/unit_tests/apm/test_knowledge_handler.py", "content": "import pytest\nfrom unittest.mock import MagicMock\nfrom superagi.apm.knowledge_handler import KnowledgeHandler\nfrom fastapi import HTTPException\nfrom datetime import datetime\nimport pytz\n\n@pytest.fixture\ndef organisation_id():\n    return 1\n\n@pytest.fixture\ndef mock_session():\n    return MagicMock()\n\n@pytest.fixture\ndef knowledge_handler(mock_session, organisation_id):\n    return KnowledgeHandler(mock_session, organisation_id)\n\ndef test_get_knowledge_usage_by_name(knowledge_handler, mock_session):\n    knowledge_handler.session = mock_session\n    knowledge_name = 'Knowledge1'\n    mock_knowledge_event = MagicMock()\n    mock_knowledge_event.knowledge_unique_agents = 5\n    mock_knowledge_event.knowledge_name = knowledge_name\n    mock_knowledge_event.id = 1\n\n    mock_session.query.return_value.filter_by.return_value.filter.return_value.first.return_value = mock_knowledge_event\n    mock_session.query.return_value.filter.return_value.group_by.return_value.first.return_value = mock_knowledge_event\n    mock_session.query.return_value.filter.return_value.count.return_value = 10\n\n    result = knowledge_handler.get_knowledge_usage_by_name(knowledge_name)\n\n    assert isinstance(result, dict)\n    assert result == {\n        'knowledge_unique_agents': 5,\n        'knowledge_calls': 10\n    }\n\n    mock_session.query.return_value.filter_by.return_value.filter.return_value.first.return_value = None\n\n    with pytest.raises(HTTPException):\n        knowledge_handler.get_knowledge_usage_by_name('NonexistentKnowledge')\n\ndef test_get_knowledge_events_by_name(knowledge_handler, mock_session):\n    knowledge_name = 'knowledge1'\n    knowledge_handler.session = mock_session\n    knowledge_handler.organisation_id = 1\n\n    mock_knowledge = MagicMock()\n    mock_knowledge.id = 1\n    mock_session.query().filter_by().filter().first.return_value = mock_knowledge\n\n    result_obj = MagicMock()\n    result_obj.agent_id = 1\n    result_obj.created_at = datetime.now()\n    result_obj.event_name = 'knowledge_picked'\n    result_obj.event_property = {'knowledge_name': 'knowledge1', 'agent_execution_id': '1'}\n    result_obj2 = MagicMock()\n    result_obj2.agent_id = 1\n    result_obj2.event_name = 'run_completed'\n    result_obj2.event_property = {'tokens_consumed': 10, 'calls': 5, 'name': 'Runner', 'agent_execution_id': '1'}\n    result_obj3 = MagicMock()\n    result_obj3.agent_id = 1\n    result_obj3.event_name = 'agent_created'\n    result_obj3.event_property = {'agent_name': 'A1', 'model': 'M1'}\n\n    mock_session.query().filter().all.side_effect = [[result_obj], [result_obj2], [result_obj3]]\n    \n    user_timezone = MagicMock()\n    user_timezone.value = 'America/New_York'\n    mock_session.query().filter().first.return_value = user_timezone\n    \n    result = knowledge_handler.get_knowledge_events_by_name(knowledge_name)\n\n    assert isinstance(result, list)\n    assert len(result) == 1\n    for item in result:\n        assert 'agent_execution_id' in item\n        assert 'created_at' in item\n        assert 'tokens_consumed' in item\n        assert 'calls' in item\n        assert 'agent_execution_name' in item\n        assert 'agent_name' in item\n        assert 'model' in item\n\n\ndef test_get_knowledge_events_by_name_knowledge_not_found(knowledge_handler, mock_session):\n    knowledge_name = \"knowledge1\"\n    not_found_message = 'Knowledge not found'\n\n    mock_session.query().filter_by().filter().first.return_value = None\n\n    try:\n        knowledge_handler.get_knowledge_events_by_name(knowledge_name)\n        assert False, \"Expected HTTPException has not been raised\"\n    except HTTPException as e:\n        assert str(e.detail) == not_found_message, f\"Expected {not_found_message}, got {e.detail}\"\n    finally:\n        assert mock_session.query().filter_by().filter().first.called, \"first() function not called\""}
{"type": "test_file", "path": "tests/unit_tests/apm/test_tools_handler.py", "content": "import pytest\nfrom unittest.mock import MagicMock, patch\nfrom fastapi import HTTPException\nfrom superagi.apm.tools_handler import ToolsHandler\nfrom sqlalchemy.orm import Session\nfrom superagi.models.agent_config import AgentConfiguration\n\nfrom datetime import datetime\nimport pytz\n\n@pytest.fixture\ndef organisation_id():\n    return 1\n\n@pytest.fixture\ndef mock_session():\n    return MagicMock()\n\n@pytest.fixture\ndef tools_handler(mock_session, organisation_id):\n    return ToolsHandler(mock_session, organisation_id)\n\ndef test_calculate_tool_usage(tools_handler, mock_session):\n    tool_used_subquery = MagicMock()\n    agent_count_subquery = MagicMock()\n    total_usage_subquery = MagicMock()\n\n    tool_used_subquery.c.tool_name = 'Tool1'\n    tool_used_subquery.c.agent_id = 1\n    agent_count_subquery.c.tool_name = 'Tool1'\n    agent_count_subquery.c.unique_agents = 1\n    total_usage_subquery.c.tool_name = 'Tool1'\n    total_usage_subquery.c.total_usage = 5\n\n    mock_session.query.return_value.filter_by.return_value.subquery.return_value = tool_used_subquery\n    mock_session.query.return_value.group_by.return_value.subquery.side_effect = [agent_count_subquery, total_usage_subquery]\n\n    result_obj = MagicMock()\n    result_obj.tool_name = 'Tool1'\n    result_obj.unique_agents = 1\n    result_obj.total_usage = 5\n\n    mock_session.query.return_value.join.return_value.all.return_value = [result_obj]\n\n    tools_handler.get_tool_and_toolkit = MagicMock(return_value={'tool1': 'Toolkit1'})\n\n    result = tools_handler.calculate_tool_usage()\n\n    assert isinstance(result, list)\n\n    expected_output = [{'tool_name': 'Tool1', 'unique_agents': 1, 'total_usage': 5, 'toolkit': 'Toolkit1'}]\n\n    assert result == expected_output\n\ndef test_get_tool_and_toolkit(tools_handler, mock_session):\n    result_obj = MagicMock()\n    result_obj.tool_name = 'tool 1'\n    result_obj.toolkit_name = 'toolkit 1'\n    \n    mock_session.query().join().all.return_value = [result_obj]\n    \n    output = tools_handler.get_tool_and_toolkit()\n    \n    assert isinstance(output, dict)\n    assert output == {'tool 1': 'toolkit 1'} \n\ndef test_get_tool_usage_by_name(tools_handler, mock_session):\n    tools_handler.session = mock_session\n    tool_name = 'Tool1'\n    formatted_tool_name = tool_name.lower().replace(\" \", \"\")\n\n    mock_tool = MagicMock()\n    mock_tool.name = tool_name\n    \n    mock_tool_event = MagicMock()\n    mock_tool_event.tool_name = formatted_tool_name\n    mock_tool_event.tool_calls = 10\n    mock_tool_event.tool_unique_agents = 5\n    \n    mock_session.query.return_value.filter_by.return_value.first.return_value = mock_tool\n    mock_session.query.return_value.filter.return_value.group_by.return_value.first.return_value = mock_tool_event\n\n    result = tools_handler.get_tool_usage_by_name(tool_name=tool_name)\n  \n    assert isinstance(result, dict)\n    assert result == {\n        'tool_calls': 10,\n        'tool_unique_agents': 5\n    }\n\n    mock_session.query.return_value.filter_by.return_value.first.return_value = None\n\n    with pytest.raises(HTTPException):\n        tools_handler.get_tool_usage_by_name(tool_name=\"NonexistentTool\")\n\ndef test_get_tool_events_by_name(tools_handler, mock_session):\n    tool_name = 'Tool1'\n    tools_handler.session = mock_session\n    tools_handler.organisation_id = 1\n\n    mock_tool = MagicMock()\n    mock_tool.id = 1\n    mock_session.query().filter_by().first.return_value = mock_tool\n\n    result_obj = MagicMock()\n    result_obj.agent_id = 1\n    result_obj.id = 1\n    result_obj.created_at = datetime.now()\n    result_obj.event_name = 'tool_used'\n    result_obj.event_property = {'tool_name': 'tool1', 'agent_execution_id': '1'}\n    result_obj2 = MagicMock()\n    result_obj2.agent_id = 1\n    result_obj2.id = 2\n    result_obj2.event_name = 'run_completed'\n    result_obj2.event_property = {'tokens_consumed': 10, 'calls': 5, 'name': 'Runner', 'agent_execution_id': '1'}\n    result_obj3 = MagicMock()\n    result_obj3.agent_id = 1\n    result_obj3.event_name = 'agent_created'\n    result_obj3.event_property = {'agent_name': 'A1', 'model': 'M1'}\n\n    mock_session.query().filter().all.side_effect = [[result_obj], [result_obj2], [result_obj3], []]\n    \n    user_timezone = MagicMock()\n    user_timezone.value = 'America/New_York'\n    mock_session.query().filter().first.return_value = user_timezone\n    \n    result = tools_handler.get_tool_events_by_name(tool_name)\n\n    assert isinstance(result, list)\n    assert len(result) == 1\n    for item in result:\n        assert 'agent_execution_id' in item\n        assert 'created_at' in item\n        assert 'tokens_consumed' in item\n        assert 'calls' in item\n        assert 'agent_execution_name' in item\n        assert 'agent_name' in item\n        assert 'model' in item\n\ndef test_get_tool_events_by_name_tool_not_found(tools_handler, mock_session):\n    tool_name = \"tool1\"\n    \n    mock_session.query().filter_by().first.return_value = None\n    with pytest.raises(HTTPException):\n        tools_handler.get_tool_events_by_name(tool_name)\n        \n    assert mock_session.query().filter_by().first.called"}
{"type": "test_file", "path": "tests/unit_tests/agent/test_agent_prompt_builder.py", "content": "from unittest.mock import Mock\nfrom unittest.mock import patch\n\nfrom superagi.agent.agent_prompt_builder import AgentPromptBuilder\nfrom superagi.tools.base_tool import BaseTool\n\n\ndef test_add_list_items_to_string():\n    items = ['item1', 'item2', 'item3']\n    result = AgentPromptBuilder.add_list_items_to_string(items)\n    assert result == '1. item1\\n2. item2\\n3. item3\\n'\n\n\ndef test_clean_prompt():\n    prompt = '   some   text  with    extra spaces     '\n    result = AgentPromptBuilder.clean_prompt(prompt)\n    assert result == 'some text with extra spaces'\n\n\n@patch('superagi.agent.agent_prompt_builder.AgentPromptBuilder.add_list_items_to_string')\n@patch('superagi.agent.agent_prompt_builder.AgentPromptBuilder.add_tools_to_prompt')\ndef test_replace_main_variables(mock_add_tools_to_prompt, mock_add_list_items_to_string):\n    super_agi_prompt = \"{goals} {instructions} {task_instructions} {constraints} {tools}\"\n    goals = ['goal1', 'goal2']\n    instructions = ['instruction1']\n    constraints = ['constraint1']\n    tools = [Mock(spec=BaseTool)]\n\n    # Mocking\n    mock_add_list_items_to_string.side_effect = lambda x: ', '.join(x)\n    mock_add_tools_to_prompt.return_value = 'tools_str'\n\n    result = AgentPromptBuilder.replace_main_variables(super_agi_prompt, goals, instructions, constraints, tools)\n\n    assert 'goal1, goal2 INSTRUCTION' in result\n    assert 'instruction1' in result\n    assert 'constraint1' in result\n\n\n@patch('superagi.agent.agent_prompt_builder.TokenCounter.count_message_tokens')\ndef test_replace_task_based_variables(mock_count_message_tokens):\n    super_agi_prompt = \"{current_task} {last_task} {last_task_result} {pending_tasks} {completed_tasks} {task_history}\"\n    current_task = \"task1\"\n    last_task = \"task2\"\n    last_task_result = \"result1\"\n    pending_tasks = [\"task3\", \"task4\"]\n    completed_tasks = [{'task': 'task1', 'response': 'response1'}, {'task': 'task2', 'response': 'response2'}]\n    token_limit = 2000\n\n    # Mocking\n    mock_count_message_tokens.return_value = 50\n\n    result = AgentPromptBuilder.replace_task_based_variables(super_agi_prompt, current_task, last_task, last_task_result,\n                                                             pending_tasks, completed_tasks, token_limit)\n\n    expected_result = f\"{current_task} {last_task} {last_task_result} {str(pending_tasks)} {str([x['task'] for x in completed_tasks])} \\nTask: {completed_tasks[-1]['task']}\\nResult: {completed_tasks[-1]['response']}\\nTask: {completed_tasks[-2]['task']}\\nResult: {completed_tasks[-2]['response']}\\n\"\n\n    assert result == expected_result\n\n\n@patch('superagi.agent.agent_prompt_builder.TokenCounter.count_message_tokens')\ndef test_replace_task_based_variables(mock_count_message_tokens):\n    super_agi_prompt = \"{current_task} {last_task} {last_task_result} {pending_tasks} {completed_tasks} {task_history}\"\n    current_task = \"task1\"\n    last_task = \"task2\"\n    last_task_result = \"result1\"\n    pending_tasks = [\"task3\", \"task4\"]\n    completed_tasks = [{'task': 'task1', 'response': 'response1'}, {'task': 'task2', 'response': 'response2'}]\n    token_limit = 2000\n\n    # Mocking\n    mock_count_message_tokens.return_value = 50\n\n    result = AgentPromptBuilder.replace_task_based_variables(super_agi_prompt, current_task, last_task, last_task_result,\n                                                             pending_tasks, completed_tasks, token_limit)\n\n    # expected_result = f\"{current_task} {last_task} {last_task_result} {str(pending_tasks)} {str([x['task'] for x in reversed(completed_tasks)])} \\nTask: {completed_tasks[-1]['task']}\\nResult: {completed_tasks[-1]['response']}\\nTask: {completed_tasks[-2]['task']}\\nResult: {completed_tasks[-2]['response']}\\n\"\n\n    assert \"task1\" in result\n    assert \"task2\" in result\n    assert \"result1\" in result\n    assert \"task3\" in result\n    assert \"task3\" in result\n    assert \"response1\" in result\n    assert \"response2\" in result"}
{"type": "test_file", "path": "tests/__init__.py", "content": ""}
{"type": "test_file", "path": "tests/integration_tests/vector_store/test_qdrant.py", "content": "import pytest\nimport numpy as np\n\nfrom superagi.vector_store import qdrant\nfrom superagi.vector_store.embedding.openai import OpenAiEmbedding\nfrom qdrant_client.models import Distance, VectorParams\nfrom qdrant_client import QdrantClient\n\n\n@pytest.fixture\ndef client():\n    client = QdrantClient(\":memory:\")\n    yield client\n\n\n@pytest.fixture\ndef mock_openai_embedding(monkeypatch):\n    monkeypatch.setattr(\n        OpenAiEmbedding,\n        \"get_embedding\",\n        lambda self, text: np.random.random(3).tolist(),\n    )\n\n\n@pytest.fixture\ndef store(client, mock_openai_embedding):\n    client.create_collection(\n        collection_name=\"Test_collection\",\n        vectors_config=VectorParams(size=3, distance=Distance.COSINE),\n    )\n    yield qdrant.Qdrant(client, OpenAiEmbedding(api_key=\"test_api_key\"), \"Test_collection\")\n    client.delete_collection(\"Test_collection\")\n\n\ndef test_add_texts(store):\n    car_companies = [\n        \"Rolls-Royce\",\n        \"Bentley\",\n        \"Ferrari\",\n        \"Lamborghini\",\n        \"Aston Martin\",\n        \"Porsche\",\n        \"Bugatti\",\n        \"Maserati\",\n        \"McLaren\",\n        \"Mercedes-Benz\"\n    ]\n    assert len(store.add_texts(car_companies)) == len(car_companies)\n\n\ndef test_get_matching_text(store):\n    car_companies = [\n        \"Rolls-Royce\",\n        \"Bentley\",\n        \"Ferrari\",\n        \"Lamborghini\",\n        \"Aston Martin\",\n        \"Porsche\",\n        \"Bugatti\",\n        \"Maserati\",\n        \"McLaren\",\n        \"Mercedes-Benz\"\n    ]\n    store.add_texts(car_companies)\n    assert len(store.get_matching_text(k=2, text=\"McLaren\")) == 2\n"}
{"type": "test_file", "path": "tests/tools/google_calendar/delete_event_test.py", "content": "import unittest\nfrom unittest.mock import Mock, patch\nfrom pydantic import ValidationError\nfrom superagi.tools.google_calendar.delete_calendar_event import DeleteCalendarEventInput, DeleteCalendarEventTool\n\nclass TestDeleteCalendarEventInput(unittest.TestCase):\n    def test_valid_input(self):\n        input_data = {\"event_id\": \"123456\"}\n        input_obj = DeleteCalendarEventInput(**input_data)\n        self.assertEqual(input_obj.event_id, \"123456\")\n\n    def test_invalid_input(self):\n        input_data = {\"event_id\": \"\"}\n        with self.assertRaises(ValidationError):\n            DeleteCalendarEventInput(**input_data)\n\nclass TestDeleteCalendarEventTools(unittest.TestCase):\n    def setUp(self):\n        self.delete_tool = DeleteCalendarEventTool()\n    @patch(\"your_module.GoogleCalendarCreds\")\n\n    def test_execute_delete_event_with_valid_id(self, mock_google_calendar_creds):\n        credentials_obj = Mock()\n        credentials_obj.get_credentials.return_value = {\"success\": True, \"service\": Mock()}\n        mock_google_calendar_creds.return_value = credentials_obj\n        self.assertEqual(self.delete_tool._execute(\"123456\"), \"Event Successfully deleted from your Google Calendar\")\n    @patch(\"your_module.GoogleCalendarCreds\")\n\n    def test_execute_delete_event_with_no_id(self, mock_google_calendar_creds):\n        self.assertEqual(self.delete_tool._execute(\"None\"), \"Add Event ID to delete an event from Google Calendar\")\n    @patch(\"your_module.GoogleCalendarCreds\")\n\n    def test_execute_delete_event_with_no_credentials(self, mock_google_calendar_creds):\n        credentials_obj = Mock()\n        credentials_obj.get_credentials.return_value = {\"success\": False}\n        mock_google_calendar_creds.return_value = credentials_obj\n        self.assertEqual(self.delete_tool._execute(\"123456\"), \"Kindly connect to Google Calendar\")\n\nif __name__ == \"__main__\":\n    unittest.main()"}
{"type": "test_file", "path": "tests/integration_tests/__init__.py", "content": ""}
{"type": "test_file", "path": "tests/unit_tests/helper/__init__.py", "content": ""}
{"type": "test_file", "path": "tests/unit_tests/controllers/test_analytics.py", "content": "from unittest.mock import patch, MagicMock\nimport pytest\nfrom fastapi.testclient import TestClient\nfrom main import app\n\nclient = TestClient(app)\n\n@patch('superagi.controllers.analytics.db')\ndef test_get_metrics_success(mock_get_db):\n    with patch('superagi.helper.auth.get_user_organisation') as mock_get_user_org, \\\n                patch('superagi.controllers.analytics.db') as mock_db, \\\n                patch('superagi.controllers.analytics.AnalyticsHelper') as mock_helper, \\\n                patch('superagi.helper.auth.db') as mock_auth_db:\n        mock_helper().calculate_run_completed_metrics.return_value = {'total_tokens': 10, 'total_calls': 5, 'runs_completed': 2}\n        response = client.get(\"/analytics/metrics\")\n        assert response.status_code == 200\n        assert response.json() == {'total_tokens': 10, 'total_calls': 5, 'runs_completed': 2}\n\n@patch('superagi.controllers.analytics.db')\ndef test_get_agents_success(mock_get_db):\n    with patch('superagi.helper.auth.get_user_organisation') as mock_get_user_org, \\\n                    patch('superagi.controllers.analytics.db') as mock_db, \\\n                    patch('superagi.controllers.analytics.AnalyticsHelper') as mock_helper, \\\n                    patch('superagi.helper.auth.db') as mock_auth_db:\n        mock_helper().fetch_agent_data.return_value = {\"agent_details\": \"mock_details\", \"model_info\": \"mock_info\"}\n        response = client.get(\"/analytics/agents/all\")\n        assert response.status_code == 200\n        assert response.json() == {\"agent_details\": \"mock_details\", \"model_info\": \"mock_info\"}\n\n@patch('superagi.controllers.analytics.db')\ndef test_get_agent_runs_success(mock_get_db):\n    with patch('superagi.helper.auth.get_user_organisation') as mock_get_user_org, \\\n                    patch('superagi.controllers.analytics.db') as mock_db, \\\n                    patch('superagi.controllers.analytics.AnalyticsHelper') as mock_helper, \\\n                    patch('superagi.helper.auth.db') as mock_auth_db:\n        mock_helper().fetch_agent_runs.return_value = \"mock_agent_runs\"\n        response = client.get(\"/analytics/agents/1\")\n        assert response.status_code == 200\n        assert response.json() == \"mock_agent_runs\"\n\n@patch('superagi.controllers.analytics.db')\ndef test_get_active_runs_success(mock_get_db):\n    with patch('superagi.helper.auth.get_user_organisation') as mock_get_user_org, \\\n                    patch('superagi.controllers.analytics.db') as mock_db, \\\n                    patch('superagi.controllers.analytics.AnalyticsHelper') as mock_helper, \\\n                    patch('superagi.helper.auth.db') as mock_auth_db:\n        mock_helper().get_active_runs.return_value = [\"mock_run_1\", \"mock_run_2\"]\n        response = client.get(\"/analytics/runs/active\")\n        assert response.status_code == 200\n        assert response.json() == [\"mock_run_1\", \"mock_run_2\"]\n\n@patch('superagi.controllers.analytics.db')\ndef test_get_tools_user_success(mock_get_db):\n    with patch('superagi.helper.auth.get_user_organisation') as mock_get_user_org, \\\n                    patch('superagi.controllers.analytics.db') as mock_db, \\\n                    patch('superagi.controllers.analytics.ToolsHandler') as mock_handler, \\\n                    patch('superagi.helper.auth.db') as mock_auth_db:\n        mock_handler().calculate_tool_usage.return_value = [\"tool1\", \"tool2\"]\n        response = client.get(\"/analytics/tools/used\")\n        assert response.status_code == 200\n        assert response.json() == [\"tool1\", \"tool2\"]"}
{"type": "test_file", "path": "tests/unit_tests/controllers/test_update_agent_config_table.py", "content": "import pytest\nfrom unittest.mock import patch, Mock\nfrom superagi.models.agent_config import AgentConfiguration\nfrom superagi.controllers.types.agent_execution_config import AgentRunIn\n\ndef test_update_existing_toolkits():\n    agent_id = 1\n    updated_details = AgentRunIn(\n        agent_workflow=\"test\", constraints=[\"c1\", \"c2\"], toolkits=[1, 2],\n        tools=[1, 2, 3], exit=\"exit\", iteration_interval=1,\n        model=\"test\", permission_type=\"p\", LTM_DB=\"LTM\", max_iterations=100\n    )\n\n    # Mock AgentConfiguration instance for the agent_configs list\n    existing_toolkits_config = Mock(spec=AgentConfiguration)\n    existing_toolkits_config.key = \"toolkits\"\n    existing_toolkits_config.value = [3, 4]\n\n    agent_configs = [existing_toolkits_config]\n\n    mock_session = Mock()\n\n    # Mock the query filter behavior for existing configurations\n    mock_session.query().filter().all.return_value = agent_configs\n\n    result = AgentConfiguration.update_agent_configurations_table(mock_session, agent_id, updated_details)\n\n    #Check whether the value gets updated or not\n    assert existing_toolkits_config.value == '[1, 2]'\n    assert mock_session.commit.called_once()\n    assert result == \"Details updated successfully\"\n"}
{"type": "test_file", "path": "tests/unit_tests/apm/test_event_handler.py", "content": "import pytest\nfrom sqlalchemy.exc import SQLAlchemyError\nfrom superagi.models.events import Event\nfrom unittest.mock import MagicMock\n\nfrom superagi.apm.event_handler import EventHandler\n\n@pytest.fixture\ndef mock_session():\n    return MagicMock()\n\n@pytest.fixture\ndef event_handler(mock_session):\n    return EventHandler(mock_session)\n\ndef test_create_event_success(event_handler, mock_session):\n    mock_session.add = MagicMock()\n    mock_session.commit = MagicMock()\n    event = event_handler.create_event('test', {}, 1, 1, 100)\n\n    assert isinstance(event, Event)\n    mock_session.add.assert_called_once()\n    mock_session.commit.assert_called_once()\n\ndef test_create_event_failure(event_handler, mock_session):\n    mock_session.commit = MagicMock(side_effect=SQLAlchemyError())\n    event = event_handler.create_event('test', {}, 1, 1, 100)\n    assert event is None"}
{"type": "test_file", "path": "tests/integration_tests/vector_embeddings/test_weaviate.py", "content": "import unittest\nfrom superagi.vector_embeddings.base import VectorEmbeddings\nfrom superagi.vector_embeddings.weaviate import Weaviate\n\nclass TestWeaviate(unittest.TestCase):\n\n    def setUp(self):\n        self.weaviate = Weaviate(uuid=\"1234\", embeds=[0.1, 0.2, 0.3, 0.4], metadata={\"info\": \"sample data\"})\n\n    def test_init(self):\n        self.assertEqual(self.weaviate.uuid, \"1234\")\n        self.assertEqual(self.weaviate.embeds, [0.1, 0.2, 0.3, 0.4])\n        self.assertEqual(self.weaviate.metadata, {\"info\": \"sample data\"})\n\n    def test_get_vector_embeddings_from_chunks(self):\n        expected_result = {\n            \"ids\": \"1234\",\n            \"data_object\": {\"info\": \"sample data\"},\n            \"vectors\": [0.1, 0.2, 0.3, 0.4]\n        }\n        self.assertEqual(self.weaviate.get_vector_embeddings_from_chunks(), expected_result)\n\n\nif __name__ == '__main__':\n    unittest.main()\n"}
{"type": "test_file", "path": "tests/unit_tests/agent/test_tool_executor.py", "content": "import pytest\nfrom unittest.mock import Mock, patch\n\nfrom pydantic import ValidationError\n\nfrom superagi.agent.common_types import ToolExecutorResponse\nfrom superagi.agent.tool_executor import ToolExecutor\n\nclass MockTool:\n    def __init__(self, name):\n        self.name = name\n\n    def execute(self, args):\n        return self.name\n\n@pytest.fixture\ndef mock_tools():\n    return [MockTool(name=f'tool{i}') for i in range(5)]\n\n@pytest.fixture\ndef executor(mock_tools):\n    return ToolExecutor(organisation_id=1, agent_id=1, tools=mock_tools, agent_execution_id=1)\n\ndef test_tool_executor_finish(executor):\n    res = executor.execute(None, 'finish', {})\n    assert res.status == 'COMPLETE'\n    assert res.result == ''\n\n@patch('superagi.agent.tool_executor.EventHandler')\ndef test_tool_executor_success(mock_event_handler, executor, mock_tools):\n    for i, tool in enumerate(mock_tools):\n        res = executor.execute(None, f'tool{i}', {'agent_execution_id': 1})\n        assert res.status == 'SUCCESS'\n        assert res.result == f'Tool {tool.name} returned: {tool.name}'\n        assert res.retry == False\n\n@patch('superagi.agent.tool_executor.EventHandler')\ndef test_tool_executor_generic_error(mock_event_handler, executor):\n    tool = MockTool('error_tool')\n    tool.execute = Mock(side_effect=Exception('generic error'))\n    executor.tools.append(tool)\n\n    res = executor.execute(None, 'error_tool', {})\n    assert res.status == 'ERROR'\n    assert 'Error1: generic error' in res.result\n    assert res.retry == True\n\ndef test_tool_executor_unknown_tool(executor):\n    res = executor.execute(None, 'unknown_tool', {})\n    assert res.status == 'ERROR'\n    assert \"Unknown tool 'unknown_tool'\" in res.result\n    assert res.retry == True\n\ndef test_clean_tool_args(executor):\n    args = {\"arg1\": {\"value\": 1}, \"arg2\": 2}\n    clean_args = executor.clean_tool_args(args)\n    assert clean_args == {\"arg1\": 1, \"arg2\": 2}"}
{"type": "test_file", "path": "tests/unit_tests/controllers/test_publish_agent.py", "content": "import pytest\nfrom fastapi.testclient import TestClient\nfrom unittest.mock import create_autospec, patch\nfrom main import app\nfrom superagi.models.agent import Agent\nfrom superagi.models.agent_config import AgentConfiguration\nfrom superagi.models.agent_execution import AgentExecution\nfrom superagi.models.agent_execution_config import AgentExecutionConfiguration\nfrom superagi.models.organisation import Organisation\nfrom superagi.models.user import User\nfrom sqlalchemy.orm import Session\n\nclient = TestClient(app)\n\n@pytest.fixture\ndef mocks():\n    # Mock tool kit data for testing\n    mock_agent = Agent(id=1, name=\"test_agent\", project_id=1, description=\"testing\", agent_workflow_id=1, is_deleted=False)\n    mock_agent_config = AgentConfiguration(id=1, agent_id=1, key=\"test_key\", value=\"['test']\")\n    mock_execution = AgentExecution(id=1, agent_id=1, name=\"test_execution\")\n    mock_execution_config = [AgentExecutionConfiguration(id=1, agent_execution_id=1, key=\"test_key\", value=\"['test']\")]\n    return mock_agent,mock_agent_config,mock_execution,mock_execution_config\n\ndef test_publish_template(mocks):\n    with patch('superagi.helper.auth.get_user_organisation') as mock_get_user_org, \\\n        patch('superagi.helper.auth.get_current_user') as mock_get_user, \\\n        patch('superagi.helper.auth.db') as mock_auth_db,\\\n        patch('superagi.controllers.agent_template.db') as mock_db:\n    \n            mock_session = create_autospec(Session)\n            mock_agent, mock_agent_config, mock_execution, mock_execution_config = mocks  \n\n            mock_session.query.return_value.filter.return_value.first.return_value = mock_agent\n            mock_session.query.return_value.filter.return_value.all.return_value = [mock_agent_config]\n            mock_session.query.return_value.filter.return_value.order_by.return_value.first.return_value = mock_execution\n            mock_session.query.return_value.filter.return_value.all.return_value = mock_execution_config \n\n            with patch('superagi.controllers.agent_execution_config.AgentExecution.get_agent_execution_from_id') as mock_get_exec:\n                mock_get_exec.return_value = mock_execution\n                response = client.post(\"/agent_templates/publish_template/agent_execution_id/1\")    \n                assert response.status_code == 201"}
{"type": "test_file", "path": "tests/unit_tests/agent/test_agent_message_builder.py", "content": "import pytest\nfrom unittest.mock import patch, Mock\n\nfrom superagi.agent.agent_message_builder import AgentLlmMessageBuilder\nfrom superagi.models.agent_execution_feed import AgentExecutionFeed\n\n\n@patch('superagi.helper.token_counter.TokenCounter.token_limit')\n@patch('superagi.config.config.get_config')\ndef test_build_agent_messages(mock_get_config, mock_token_limit):\n    mock_session = Mock()\n    llm = Mock()\n    llm_model = Mock()\n    agent_id = 1\n    agent_execution_id = 1\n    prompt = \"start\"\n    agent_feeds = []\n    completion_prompt = \"end\"\n\n    # Mocking\n    mock_token_limit.return_value = 1000\n    mock_get_config.return_value = 600\n\n    builder = AgentLlmMessageBuilder(mock_session, llm, llm_model, agent_id, agent_execution_id)\n    messages = builder.build_agent_messages(prompt, agent_feeds, history_enabled=True, completion_prompt=completion_prompt)\n\n    # Test prompt message\n    assert messages[0] == {\"role\": \"system\", \"content\": prompt}\n\n    # Test initial feeds\n    assert mock_session.add.call_count == len(messages)\n    assert mock_session.commit.call_count == len(messages)\n\n    # Check if AgentExecutionFeed object is created and added to session\n    for i in range(len(messages)):\n        args, _ = mock_session.add.call_args_list[i]\n        feed_obj = args[0]\n        assert isinstance(feed_obj, AgentExecutionFeed)\n        assert feed_obj.agent_execution_id == agent_execution_id\n        assert feed_obj.agent_id == agent_id\n        assert feed_obj.feed == messages[i][\"content\"]\n        assert feed_obj.role == messages[i][\"role\"]\n\n@patch('superagi.models.agent_execution_config.AgentExecutionConfiguration.fetch_value')\n@patch('superagi.models.agent_execution_config.AgentExecutionConfiguration.add_or_update_agent_execution_config')\n@patch('superagi.agent.agent_message_builder.AgentLlmMessageBuilder._build_prompt_for_recursive_ltm_summary_using_previous_ltm_summary')\n@patch('superagi.agent.agent_message_builder.AgentLlmMessageBuilder._build_prompt_for_ltm_summary')\n@patch('superagi.helper.token_counter.TokenCounter.count_text_tokens')\n@patch('superagi.helper.token_counter.TokenCounter.token_limit')\ndef test_build_ltm_summary(mock_token_limit, mock_count_text_tokens, mock_build_prompt_for_ltm_summary,\n                           mock_build_prompt_for_recursive_ltm_summary, mock_add_or_update_agent_execution_config,\n                           mock_fetch_value):\n    mock_session = Mock()\n    llm = Mock()\n    llm_model = Mock()\n    agent_id = 1\n    agent_execution_id = 1\n\n    builder = AgentLlmMessageBuilder(mock_session, llm, llm_model, agent_id, agent_execution_id)\n\n    past_messages = [{\"role\": \"user\", \"content\": \"Hello\"}, {\"role\": \"assistant\", \"content\": \"Hi\"}]\n    output_token_limit = 100\n\n    mock_token_limit.return_value = 1000\n    mock_count_text_tokens.return_value = 200\n    mock_build_prompt_for_ltm_summary.return_value = \"ltm_summary_prompt\"\n    mock_build_prompt_for_recursive_ltm_summary.return_value = \"recursive_ltm_summary_prompt\"\n    mock_fetch_value.return_value = Mock(value=\"ltm_summary\")\n    llm.chat_completion.return_value = {\"content\": \"ltm_summary\"}\n\n    ltm_summary = builder._build_ltm_summary(past_messages, output_token_limit)\n\n    assert ltm_summary == \"ltm_summary\"\n\n    mock_add_or_update_agent_execution_config.assert_called_once()\n\n    llm.chat_completion.assert_called_once_with([{\"role\": \"system\", \"content\": \"You are GPT Prompt writer\"},\n                                                 {\"role\": \"assistant\", \"content\": \"ltm_summary_prompt\"}])\n\n@patch('superagi.helper.prompt_reader.PromptReader.read_agent_prompt')\ndef test_build_prompt_for_ltm_summary(mock_read_agent_prompt):\n    mock_session = Mock()\n    llm = Mock()\n    llm_model = Mock()\n    agent_id = 1\n    agent_execution_id = 1\n\n    builder = AgentLlmMessageBuilder(mock_session, llm, llm_model, agent_id, agent_execution_id)\n\n    past_messages = [{\"role\": \"user\", \"content\": \"Hello\"}, {\"role\": \"assistant\", \"content\": \"Hi\"}]\n    token_limit = 100\n\n    mock_read_agent_prompt.return_value = \"{past_messages}\\n{char_limit}\"\n\n    prompt = builder._build_prompt_for_ltm_summary(past_messages, token_limit)\n\n    assert \"user: Hello\\nassistant: Hi\\n\" in prompt\n    assert \"400\" in prompt\n\n\n@patch('superagi.helper.prompt_reader.PromptReader.read_agent_prompt')\ndef test_build_prompt_for_recursive_ltm_summary_using_previous_ltm_summary(mock_read_agent_prompt):\n    mock_session = Mock()\n    llm = Mock()\n    llm_model = Mock()\n    agent_id = 1\n    agent_execution_id = 1\n\n    builder = AgentLlmMessageBuilder(mock_session, llm, llm_model, agent_id, agent_execution_id)\n\n    previous_ltm_summary = \"Summary\"\n    past_messages = [{\"role\": \"user\", \"content\": \"Hello\"}, {\"role\": \"assistant\", \"content\": \"Hi\"}]\n    token_limit = 100\n\n    mock_read_agent_prompt.return_value = \"{previous_ltm_summary}\\n{past_messages}\\n{char_limit}\"\n\n    prompt = builder._build_prompt_for_recursive_ltm_summary_using_previous_ltm_summary(previous_ltm_summary, past_messages, token_limit)\n\n    assert \"Summary\" in prompt\n    assert \"user: Hello\\nassistant: Hi\\n\" in prompt\n    assert \"400\" in prompt\n"}
{"type": "test_file", "path": "tests/unit_tests/agent/test_output_handler.py", "content": "import pytest\nfrom unittest.mock import Mock, patch, MagicMock\n\nfrom superagi.agent.common_types import ToolExecutorResponse\nfrom superagi.agent.output_handler import ToolOutputHandler, TaskOutputHandler, ReplaceTaskOutputHandler\nfrom superagi.agent.output_parser import AgentSchemaOutputParser, AgentGPTAction\nfrom superagi.agent.task_queue import TaskQueue\nfrom superagi.agent.tool_executor import ToolExecutor\nfrom superagi.helper.json_cleaner import JsonCleaner\nfrom superagi.models.agent import Agent\nfrom superagi.models.agent_execution_permission import AgentExecutionPermission\nimport numpy as np\nfrom superagi.agent.output_handler import ToolOutputHandler\n\n\n# Test for ToolOutputHandler\n@patch.object(TaskQueue, 'complete_task')\n@patch.object(TaskQueue, 'get_tasks')\n@patch.object(TaskQueue, 'get_completed_tasks')\n@patch.object(AgentSchemaOutputParser, 'parse')\ndef test_tool_output_handle(parse_mock, execute_mock, get_completed_tasks_mock, complete_task_mock):\n    # Arrange\n    agent_execution_id = 11\n    agent_config = {\"agent_id\": 22, \"permission_type\": \"unrestricted\"}\n    assistant_reply = '{\"tool\": {\"name\": \"someAction\", \"args\": [\"arg1\", \"arg2\"]}}'\n    parse_mock.return_value = AgentGPTAction(name=\"someAction\", args=[\"arg1\", \"arg2\"])\n\n    # Define what the mock response status should be\n    execute_mock.return_value = Mock(status='PENDING', is_permission_required=False)\n\n    handler = ToolOutputHandler(agent_execution_id, agent_config, [],None)\n\n    # Mock session\n    session_mock = MagicMock()\n    session_mock.query.return_value.filter.return_value.first.return_value = Mock()\n    handler._check_for_completion = Mock(return_value=Mock(status='PENDING', is_permission_required=False))\n    handler.handle_tool_response = Mock(return_value=Mock(status='PENDING', is_permission_required=False))\n    # Act\n    response = handler.handle(session_mock, assistant_reply)\n\n    # Assert\n    assert response.status == \"PENDING\"\n    parse_mock.assert_called_with(assistant_reply)\n    assert session_mock.add.call_count == 2\n\n\n\n@patch('superagi.agent.output_handler.TokenTextSplitter')\ndef test_add_text_to_memory(TokenTextSplitter_mock):\n    # Arrange\n    agent_execution_id = 1\n    agent_config = {\"agent_id\": 2}\n    tool_output_handler = ToolOutputHandler(agent_execution_id, agent_config,[], None)\n\n    assistant_reply = '{\"thoughts\": {\"text\": \"This is a task.\"}}'\n    tool_response_result = '[\"Task completed.\"]'\n\n    text_splitter_mock = MagicMock()\n    TokenTextSplitter_mock.return_value = text_splitter_mock\n    text_splitter_mock.split_text.return_value = [\"This is a task.\", \"Task completed.\"]\n\n    # Mock the VectorStore memory\n    memory_mock = MagicMock()\n    tool_output_handler.memory = memory_mock\n\n    # Act\n    tool_output_handler.add_text_to_memory(assistant_reply, tool_response_result)\n\n    # Assert\n    TokenTextSplitter_mock.assert_called_once_with(chunk_size=1024, chunk_overlap=10)\n    text_splitter_mock.split_text.assert_called_once_with('This is a task.[\"Task completed.\"]')\n    memory_mock.add_texts.assert_called_once_with([\"This is a task.\", \"Task completed.\"], [{\"agent_execution_id\": agent_execution_id}, {\"agent_execution_id\": agent_execution_id}])  \n\n\n@patch('superagi.models.agent_execution_permission.AgentExecutionPermission')\ndef test_tool_handler_check_permission_in_restricted_mode(op_mock):\n    # Mock the session\n    session_mock = MagicMock()\n\n    # Arrange\n    agent_execution_id = 1\n    agent_config = {\"agent_id\": 2, \"permission_type\": \"RESTRICTED\"}\n    assistant_reply = '{\"tool\": {\"name\": \"someAction\", \"args\": [\"arg1\", \"arg2\"]}}'\n    op_mock.parse.return_value = AgentGPTAction(name=\"someAction\", args=[\"arg1\", \"arg2\"])\n    tool = MagicMock()\n    tool.name = \"someAction\"\n    tool.permission_required = True\n    handler = ToolOutputHandler(agent_execution_id, agent_config, [tool],None)\n\n    # Act\n    response = handler._check_permission_in_restricted_mode(session_mock, assistant_reply)\n\n    # Assert\n    assert response.is_permission_required\n    assert response.status == \"WAITING_FOR_PERMISSION\"\n    session_mock.add.assert_called_once()\n    session_mock.commit.assert_called_once()\n\n\n# Test for TaskOutputHandler\n@patch.object(TaskQueue, 'add_task')\n@patch.object(TaskQueue, 'get_tasks')\n@patch.object(JsonCleaner, 'extract_json_array_section')\ndef test_task_output_handle_method(extract_json_array_section_mock, get_tasks_mock, add_task_mock):\n    # Arrange\n    agent_execution_id = 1\n    agent_config = {\"agent_id\": 2}\n    assistant_reply = '[\"task1\", \"task2\", \"task3\"]'\n    tasks = [\"task1\", \"task2\", \"task3\"]\n    extract_json_array_section_mock.return_value = str(tasks)\n    get_tasks_mock.return_value = tasks\n    handler = TaskOutputHandler(agent_execution_id, agent_config)\n\n    # Mock session\n    session_mock = MagicMock()\n\n    # Act\n    response = handler.handle(session_mock, assistant_reply)\n\n    # Assert\n    extract_json_array_section_mock.assert_called_once_with(assistant_reply)\n    assert add_task_mock.call_count == len(tasks)\n    assert session_mock.add.call_count == len(tasks)\n    get_tasks_mock.assert_called_once()\n    assert response.status == \"PENDING\"\n\n\n# Test for ReplaceTaskOutputHandler\n@patch.object(TaskQueue, 'clear_tasks')\n@patch.object(TaskQueue, 'add_task')\n@patch.object(TaskQueue, 'get_tasks')\n@patch.object(JsonCleaner, 'extract_json_array_section')\ndef test_handle_method(extract_json_array_section_mock, get_tasks_mock, add_task_mock, clear_tasks_mock):\n    # Arrange\n    agent_execution_id = 1\n    agent_config = {}\n    assistant_reply = '[\"task1\", \"task2\", \"task3\"]'\n    tasks = [\"task1\", \"task2\", \"task3\"]\n    extract_json_array_section_mock.return_value = str(tasks)\n    get_tasks_mock.return_value = tasks\n    handler = ReplaceTaskOutputHandler(agent_execution_id, agent_config)\n\n    # Mock session\n    session_mock = MagicMock()\n\n    # Act\n    response = handler.handle(session_mock, assistant_reply)\n\n    # Assert\n    extract_json_array_section_mock.assert_called_once_with(assistant_reply)\n    clear_tasks_mock.assert_called_once()\n    assert add_task_mock.call_count == len(tasks)\n    get_tasks_mock.assert_called_once()\n    assert response.status == \"PENDING\"\n"}
{"type": "test_file", "path": "tests/unit_tests/controllers/test_agent_execution_config.py", "content": "from unittest.mock import patch\n\nimport pytest\nfrom fastapi.testclient import TestClient\n\nfrom main import app\nfrom superagi.models.agent import Agent\nfrom superagi.models.agent_config import AgentConfiguration\nfrom superagi.models.agent_execution import AgentExecution\nfrom superagi.models.agent_execution_config import AgentExecutionConfiguration\n\nclient = TestClient(app)\n\n@pytest.fixture\ndef mocks():\n    # Mock tool kit data for testing\n    mock_agent = Agent(id=1, name=\"test_agent\", project_id=1, description=\"testing\", agent_workflow_id=1, is_deleted=False)\n    mock_agent_config = AgentConfiguration(id=1, agent_id=1, key=\"test_key\", value=\"['test']\")\n    mock_execution = AgentExecution(id=54, agent_id=1, name=\"test_execution\")\n    mock_execution_config = [AgentExecutionConfiguration(id=64, agent_execution_id=1, key=\"test_key\", value=\"['test']\")]\n    return mock_agent,mock_agent_config,mock_execution,mock_execution_config\n\n\ndef test_get_agent_execution_configuration_not_found_failure():\n    with patch('superagi.controllers.agent_execution_config.db') as mock_db:\n        mock_db.session.query.return_value.filter.return_value.all.return_value = []\n        mock_db.session.query.return_value.filter.return_value.first.return_value = None\n        response = client.get(\"/agent_executions_configs/details/agent_id/1/agent_execution_id/1\")\n\n        assert response.status_code == 404\n        assert response.json() == {\"detail\": \"Agent not found\"}\n\n\ndef test_get_agent_execution_configuration_success(mocks):\n    with patch('superagi.controllers.agent_execution_config.db') as mock_db:\n        mock_agent, mock_agent_config, mock_execution, mock_execution_config = mocks\n\n        # Configure the mock objects to return the mock values\n        mock_db.session.query.return_value.filter.return_value.first.return_value = mock_agent\n        mock_db.session.query.return_value.filter.return_value.all.return_value = [mock_agent_config]\n        mock_db.session.query.return_value.filter.return_value.order_by.return_value.first.return_value = mock_execution\n        mock_db.session.query.return_value.filter.return_value.all.return_value = mock_execution_config\n\n        # Mock the AgentExecution.get_agent_execution_from_id method to return the mock_execution object\n        with patch('superagi.controllers.agent_execution_config.AgentExecution.get_agent_execution_from_id') as mock_get_exec:\n            mock_get_exec.return_value = mock_execution\n\n            response = client.get(\"/agent_executions_configs/details/agent_id/1/agent_execution_id/1\")\n\n            assert response.status_code == 200\n\n\n"}
{"type": "test_file", "path": "tests/unit_tests/agent/test_output_parser.py", "content": "import pytest\n\nfrom superagi.agent.output_parser import AgentGPTAction, AgentSchemaOutputParser\n\nimport pytest\n\ndef test_agent_schema_output_parser():\n    parser = AgentSchemaOutputParser()\n\n    # Test with valid json response\n    response = '```{\"tool\": {\"name\": \"Tool1\", \"args\": {}}}```'\n    parsed = parser.parse(response)\n    assert isinstance(parsed, AgentGPTAction)\n    assert parsed.name == 'Tool1'\n    assert parsed.args == {}\n\n    # Test with valid json but with boolean values\n    response = \"```{'tool': {'name': 'Tool1', 'args': 'arg1'}, 'status': True}```\"\n    parsed = parser.parse(response)\n    assert isinstance(parsed, AgentGPTAction)\n    assert parsed.name == 'Tool1'\n    assert parsed.args == 'arg1'\n\n    # Test with invalid json response\n    response = \"invalid response\"\n    with pytest.raises(Exception):\n        parsed = parser.parse(response)\n\n    # Test with empty json response\n    response = \"\"\n    with pytest.raises(Exception):\n        parsed = parser.parse(response)\n\n\n\n"}
{"type": "test_file", "path": "tests/unit_tests/apm/test_call_log_helper.py", "content": "import pytest\nfrom sqlalchemy.exc import SQLAlchemyError\nfrom superagi.models.call_logs import CallLogs\nfrom superagi.models.agent import Agent\nfrom superagi.models.tool import Tool\nfrom superagi.models.toolkit import Toolkit\nfrom unittest.mock import MagicMock\n\nfrom superagi.apm.call_log_helper import CallLogHelper\n\n@pytest.fixture\ndef mock_session():\n    return MagicMock()\n\n@pytest.fixture\ndef mock_agent():\n    return MagicMock()\n\n@pytest.fixture\ndef mock_tool():\n    return MagicMock()\n\n@pytest.fixture\ndef mock_toolkit():\n    return MagicMock()\n\n@pytest.fixture\ndef call_log_helper(mock_session):\n    return CallLogHelper(mock_session, 1)\n\ndef test_create_call_log_success(call_log_helper, mock_session):\n    mock_session.add = MagicMock()\n    mock_session.commit = MagicMock()\n    call_log = call_log_helper.create_call_log('test', 1, 10, 'test_tool', 'test_model')\n\n    assert isinstance(call_log, CallLogs)\n    mock_session.add.assert_called_once()\n    mock_session.commit.assert_called_once()\n\ndef test_create_call_log_failure(call_log_helper, mock_session):\n    mock_session.commit = MagicMock(side_effect=SQLAlchemyError())\n    call_log = call_log_helper.create_call_log('test', 1, 10, 'test_tool', 'test_model')\n    assert call_log is None\n\ndef test_fetch_data_success(call_log_helper, mock_session):\n    mock_session.query = MagicMock()\n\n    # creating mock results\n    summary_result = (1, 1, 1)\n    runs = [CallLogs(\n        agent_execution_name='test',\n        agent_id=1,\n        tokens_consumed=10,\n        tool_used='test_tool',\n        model='test_model',\n        org_id=1\n    )]\n    agents = [Agent(name='test_agent')]\n    tools = [Tool(name='test_tool', toolkit_id=1)]\n    toolkits = [Toolkit(name='test_toolkit')]\n\n    # setup return values for the mock methods\n    mock_session.query().filter().first.side_effect = [summary_result, runs, agents, toolkits, tools]\n\n    result = call_log_helper.fetch_data('test_model')\n\n    assert result is not None\n    assert 'model' in result\n    assert 'total_tokens' in result\n    assert 'total_calls' in result\n    assert 'total_agents' in result\n    assert 'runs' in result\n\ndef test_fetch_data_failure(call_log_helper, mock_session):\n    mock_session.query = MagicMock(side_effect=SQLAlchemyError())\n    result = call_log_helper.fetch_data('test_model')\n\n    assert result is None"}
{"type": "test_file", "path": "tests/unit_tests/controllers/test_user.py", "content": "from unittest.mock import patch\n\nimport pytest\nfrom fastapi.testclient import TestClient\n\nfrom main import app\nfrom superagi.models.user import User\n\nclient = TestClient(app)\n\n# Define a fixture for an authenticated user\n@pytest.fixture\ndef authenticated_user():\n    # Create a mock user object with necessary attributes\n    user = User()\n\n    # Set user attributes\n    user.id = 1  # User ID\n    user.username = \"testuser\"  # User's username\n    user.email = \"super6@agi.com\"  # User's email\n    user.first_login_source = None  # User's first login source\n    user.token = \"mock-jwt-token\"\n\n    return user\n\n# Test case for updating first login source when it's not set\ndef test_update_first_login_source(authenticated_user):\n    with patch('superagi.helper.auth.db') as mock_auth_db:\n        source = \"github\"  # Specify the source you want to set\n\n        mock_auth_db.session.query.return_value.filter.return_value.first.return_value = authenticated_user\n        response = client.post(f\"users/first_login_source/{source}\", headers={\"Authorization\": f\"Bearer {authenticated_user.token}\"})\n\n        # Verify the HTTP response\n        assert response.status_code == 200\n        assert \"first_login_source\" in response.json()  # Check if the \"first_login_source\" field is in the response\n        assert response.json()[\"first_login_source\"] == \"github\"  # Check if the \"source\" field equals \"github\""}
{"type": "test_file", "path": "tests/unit_tests/controllers/test_agent_execution_feeds.py", "content": "from unittest.mock import MagicMock, Mock, create_autospec, patch\nimport pytest\nfrom fastapi.testclient import TestClient\nfrom fastapi import HTTPException\nfrom main import app\nfrom fastapi_sqlalchemy import db\nfrom superagi.controllers.agent_execution_feed import get_agent_execution_feed\n\n@patch('superagi.controllers.agent_execution_feed.db')\ndef test_get_agent_execution_feed(mock_query):\n    mock_session = create_autospec(pytest.Session)\n    \n    AgentExecution = MagicMock()\n    agent_execution = AgentExecution()\n    agent_execution.status = \"PAUSED\"\n    agent_execution.last_shown_error_id = None\n    \n    AgentExecutionFeed = MagicMock()\n    agent_execution_feed = AgentExecutionFeed()\n    agent_execution_feed.error_message = None\n    \n    feeds = [agent_execution_feed]\n    \n    check_auth = MagicMock()\n    AuthJWT = MagicMock()\n    check_auth.return_value = AuthJWT  \n    asc = MagicMock()\n    \n    AgentExecutionPermission = MagicMock()\n    agent_execution_permission = AgentExecutionPermission()\n    agent_execution_permission.id = 1\n    agent_execution_permission.created_at = \"2021-12-13T00:00:00\"\n    agent_execution_permission.response = \"Yes\"\n    agent_execution_permission.status = \"Completed\"\n    agent_execution_permission.tool_name = \"Tool1\"\n    agent_execution_permission.question = \"Question1\"\n    agent_execution_permission.user_feedback = \"Feedback1\"\n        \n    permissions = [agent_execution_permission]\n    mock_agent_execution = Mock() \n    mock_query.return_value.filter.return_value.first.return_value = mock_agent_execution\n    mock_agent_execution_id = 1\n    assert get_agent_execution_feed(mock_agent_execution_id)"}
{"type": "test_file", "path": "tests/unit_tests/agent/test_tool_builder.py", "content": "import pytest\nfrom unittest.mock import Mock, patch\n\nfrom superagi.agent.tool_builder import ToolBuilder\nfrom superagi.models.tool import Tool\n\n\n@pytest.fixture\ndef session():\n    return Mock()\n\n@pytest.fixture\ndef agent_id():\n    return 1\n\n@pytest.fixture\ndef tool_builder(session, agent_id):\n    return ToolBuilder(session, agent_id)\n\n@pytest.fixture\ndef tool():\n    tool = Mock(spec=Tool)\n    tool.file_name = 'test.py'\n    tool.folder_name = 'test_folder'\n    tool.class_name = 'TestClass'\n    return tool\n\n@pytest.fixture\ndef agent_config():\n    return {\"model\": \"gpt4\"}\n\n@pytest.fixture\ndef agent_execution_config():\n    return {\"goal\": \"Test Goal\", \"instruction\": \"Test Instruction\"}\n\n@patch('superagi.agent.tool_builder.importlib.import_module')\n@patch('superagi.agent.tool_builder.getattr')\ndef test_build_tool(mock_getattr, mock_import_module, tool_builder, tool):\n    mock_module = Mock()\n    mock_class = Mock()\n    mock_import_module.return_value = mock_module\n    mock_getattr.return_value = mock_class\n\n    result_tool = tool_builder.build_tool(tool)\n\n    mock_import_module.assert_called_with('.test_folder.test')\n    mock_getattr.assert_called_with(mock_module, tool.class_name)\n\n    assert result_tool.toolkit_config.session == tool_builder.session\n    assert result_tool.toolkit_config.toolkit_id == tool.toolkit_id"}
{"type": "test_file", "path": "tests/tools/google_calendar/event_details_test.py", "content": "import unittest\nfrom unittest.mock import MagicMock, patch\nfrom pydantic import ValidationError\nfrom superagi.tools.google_calendar.event_details_calendar import EventDetailsCalendarInput, EventDetailsCalendarTool\nfrom superagi.helper.google_calendar_creds import GoogleCalendarCreds\n\nclass TestEventDetailsCalendarInput(unittest.TestCase):\n    def test_invalid_input(self):\n        with self.assertRaises(ValidationError):\n            EventDetailsCalendarInput(event_id=None)\n    \n    def test_valid_input(self):\n        input_data = EventDetailsCalendarInput(event_id=\"test_event_id\")\n        self.assertEqual(input_data.event_id, \"test_event_id\")\n\nclass TestEventDetailsCalendarTool(unittest.TestCase):\n    def setUp(self):\n        self.tool = EventDetailsCalendarTool()\n\n    def test_no_credentials(self):\n        with patch.object(GoogleCalendarCreds, 'get_credentials') as mock_get_credentials:\n            mock_get_credentials.return_value = {\"success\": False}\n            result = self.tool._execute(event_id=\"test_event_id\")\n            self.assertEqual(result, \"Kindly connect to Google Calendar\")\n\n    def test_no_event_id(self):\n        with patch.object(GoogleCalendarCreds, 'get_credentials') as mock_get_credentials:\n            mock_get_credentials.return_value = {\"success\": True}\n            result = self.tool._execute(event_id=\"None\")\n            self.assertEqual(result, \"Add Event ID to fetch details of an event from Google Calendar\")\n\n    def test_valid_event(self):\n        event_data = {\n            'summary': 'Test Meeting',\n            'start': {'dateTime': '2022-01-01T09:00:00'},\n            'end': {'dateTime': '2022-01-01T10:00:00'},\n            'attendees': [{'email': 'attendee1@example.com'},\n                          {'email': 'attendee2@example.com'}]\n        }\n        with patch.object(GoogleCalendarCreds, 'get_credentials') as mock_get_credentials:\n            with patch('your_module.base64.b64decode') as mock_b64decode:\n                mock_get_credentials.return_value = {\"success\": True, \"service\": MagicMock()}\n                service = mock_get_credentials.return_value[\"service\"]\n                service.events().get.return_value.execute.return_value = event_data\n                mock_b64decode.return_value.decode.return_value = \"decoded_event_id\"\n                result = self.tool._execute(event_id=\"test_event_id\")\n                mock_b64decode.assert_called_once_with(\"test_event_id\")\n                service.events().get.assert_called_once_with(calendarId=\"primary\", eventId=\"decoded_event_id\")\n                expected_output = (\"Event details for the event id 'test_event_id' is - \\n\"\n                                   \"Summary : Test Meeting\\n\"\n                                   \"Start Date and Time : 2022-01-01T09:00:00\\n\"\n                                   \"End Date and Time : 2022-01-01T10:00:00\\n\"\n                                   \"Attendees : attendee1@example.com,attendee2@example.com\")\n                self.assertEqual(result, expected_output)\n\nif __name__ == '__main__':\n    unittest.main()"}
{"type": "test_file", "path": "tests/unit_tests/controllers/test_agent_template.py", "content": "from unittest.mock import patch, MagicMock\nfrom superagi.models.agent_template import AgentTemplate\nfrom superagi.models.agent_template_config import AgentTemplateConfig\nfrom fastapi.testclient import TestClient\nfrom main import app\n\nclient = TestClient(app)\n\n@patch('superagi.controllers.agent_template.db')\n@patch('superagi.helper.auth.db')\n@patch('superagi.helper.auth.get_user_organisation')\ndef test_edit_agent_template_success(mock_get_user_org, mock_auth_db, mock_db):\n    # Create a mock agent template\n    mock_agent_template = AgentTemplate(id=1, name=\"Test Agent Template\", description=\"Test Description\")\n    # mock_agent_goals = AgentTemplateConfig()\n\n    # Create a mock edited agent configuration\n    mock_updated_agent_configs = {\n        \"name\": \"Updated Agent Template\",\n        \"description\": \"Updated Description\",\n        \"agent_configs\": {\n            \"agent_workflow\": \"Don't Maintain Task Queue\",\n            \"goal\": [\"Create a simple pacman game for me.\", \"Write all files properly.\"],\n            \"instruction\": [\"write spec\",\"write code\",\"improve the code\",\"write test\"],\n            \"constraints\": [\"If you are unsure how you previously did something or want to recall past events, thinking about similar events will help you remember.\",\"Ensure the tool and args are as per current plan and reasoning\",\"Exclusively use the tools listed under \\\"TOOLS\\\"\",\"REMEMBER to format your response as JSON, using double quotes (\\\"\\\") around keys and string values, and commas (,) to separate items in arrays and objects. IMPORTANTLY, to use a JSON object as a string in another JSON object, you need to escape the double quotes.\"],\n            \"tools\": [\"Read Email\", \"Send Email\", \"Write File\"],\n            \"exit\": \"No exit criterion\",\n            \"iteration_interval\": 500,\n            \"model\": \"gpt-4\",\n            \"max_iterations\": 25,\n            \"permission_type\": \"God Mode\",\n            \"LTM_DB\": \"Pinecone\"\n        }\n    }    \n\n    # Mocking the user organisation\n    mock_get_user_org.return_value = MagicMock(id=1)\n\n    # Create a session mock\n    session_mock = MagicMock()\n    mock_db.session = session_mock\n    mock_db.session.query.return_value.filter.return_value.first.return_value = mock_agent_template\n    mock_db.session.commit.return_value = None\n    mock_db.session.add.return_value = None\n    mock_db.session.flush.return_value = None\n\n    mock_agent_template_config = AgentTemplateConfig(agent_template_id = 1, key=\"goal\", value=[\"Create a simple pacman game for me.\", \"Write all files properly.\"])\n\n\n    # Call the endpoint\n    response = client.put(\"agent_templates/update_agent_template/1\", json=mock_updated_agent_configs)\n\n    assert response.status_code == 200\n        \n    # Verify changes in the mock agent template\n    assert mock_agent_template.name == \"Updated Agent Template\"\n    assert mock_agent_template.description == \"Updated Description\"\n    assert mock_agent_template_config.key == \"goal\"\n    assert mock_agent_template_config.value == [\"Create a simple pacman game for me.\", \"Write all files properly.\"]\n\n\n    session_mock.commit.assert_called()\n    session_mock.flush.assert_called()\n\n\n@patch('superagi.controllers.agent_template.db')\n@patch('superagi.helper.auth.db')\n@patch('superagi.helper.auth.get_user_organisation')\ndef test_edit_agent_template_failure(mock_get_user_org, mock_auth_db, mock_db):\n    # Setup: The user organisation exists, but the agent template does not exist.\n    mock_get_user_org.return_value = MagicMock(id=1)\n\n    # Create a session mock\n    session_mock = MagicMock()\n    mock_db.session = session_mock\n    mock_db.session.query.return_value.filter.return_value.first.return_value = None\n\n    # Call the endpoint\n    response = client.put(\"agent_templates/update_agent_template/1\", json={})\n\n    # Verify: The response status code should be 404, indicating that the agent template was not found.\n    assert response.status_code == 404\n    assert response.json() == {\"detail\": \"Agent Template not found\"}\n\n    # Verify: The database commit method should not have been called because the agent template was not found.\n    session_mock.commit.assert_not_called()\n    session_mock.flush.assert_not_called()\n\n\n@patch('superagi.controllers.agent_template.db')\n@patch('superagi.helper.auth.db')\n@patch('superagi.helper.auth.get_user_organisation')\ndef test_edit_agent_template_with_new_config_success(mock_get_user_org, mock_auth_db, mock_db):\n    # Create a mock agent template\n    mock_agent_template = AgentTemplate(id=1, name=\"Test Agent Template\", description=\"Test Description\")\n\n    # Create a mock edited agent configuration\n    mock_updated_agent_configs = {\n        \"name\": \"Updated Agent Template\",\n        \"description\": \"Updated Description\",\n        \"agent_configs\": {\n            \"new_config_key\": \"New config value\",\n            \"agent_workflow\": \"Don't Maintain Task Queue\", # This is a new config\n        }\n    }    \n\n    # Mocking the user organisation\n    mock_get_user_org.return_value = MagicMock(id=1)\n\n    # Create a session mock\n    session_mock = MagicMock()\n    mock_db.session = session_mock\n    mock_db.session.query.return_value.filter.return_value.first.return_value = mock_agent_template\n    mock_db.session.commit.return_value = None\n    mock_db.session.add.return_value = None\n    mock_db.session.flush.return_value = None\n\n    # Call the endpoint\n    response = client.put(\"agent_templates/update_agent_template/1\", json=mock_updated_agent_configs)\n\n    assert response.status_code == 200\n\n    # Verify changes in the mock agent template\n    assert mock_agent_template.name == \"Updated Agent Template\"\n    assert mock_agent_template.description == \"Updated Description\"\n\n    session_mock.commit.assert_called()\n    session_mock.flush.assert_called()"}
{"type": "test_file", "path": "tests/unit_tests/controllers/test_agent_execution.py", "content": "from unittest.mock import patch\nfrom unittest import mock\nimport pytest\nfrom fastapi.testclient import TestClient\n\nfrom main import app\nfrom superagi.models.agent_schedule import AgentSchedule\nfrom datetime import datetime\n\nclient = TestClient(app)\n\n@pytest.fixture\ndef mock_patch_schedule_input():\n    return {\n        \"agent_id\": 1,\n        \"start_time\": \"2023-02-02 01:00:00\",\n        \"recurrence_interval\": \"2 Hours\",\n        \"expiry_date\": \"2023-12-30 01:00:00\",\n        \"expiry_runs\": -1\n    }\n\n@pytest.fixture\ndef mock_schedule():\n    # Mock schedule data for testing\n    return AgentSchedule(id=1, agent_id=1, status=\"SCHEDULED\")\n\n# An agent is already scheduled and is simply being updated, we assert for the updated values here\ndef test_schedule_existing_agent_already_scheduled(mock_patch_schedule_input, mock_schedule):\n    with patch('superagi.controllers.agent_execution.db') as mock_db:\n        mock_db.session.query.return_value.filter.return_value.first.return_value = mock_schedule \n\n        response = client.post(\"agentexecutions/schedule\", json=mock_patch_schedule_input)\n\n        assert response.status_code == 201\n        assert mock_schedule.start_time == datetime.strptime(mock_patch_schedule_input['start_time'], '%Y-%m-%d %H:%M:%S')\n        assert mock_schedule.recurrence_interval == mock_patch_schedule_input['recurrence_interval']\n        assert mock_schedule.expiry_date == datetime.strptime(mock_patch_schedule_input['expiry_date'], '%Y-%m-%d %H:%M:%S')\n        assert mock_schedule.expiry_runs == mock_patch_schedule_input['expiry_runs']\n\n# The agent isn't scheduled yet and we are scheduling it, we simply assert for a 201 status code and non-null schedule id.\ndef test_schedule_existing_agent_new_schedule(mock_patch_schedule_input, mock_schedule):\n    with patch('superagi.controllers.agent_execution.db') as mock_db:\n        mock_db.session.query.return_value.filter.return_value.first.return_value = mock_schedule\n\n        response = client.post(\"agentexecutions/schedule\", json=mock_patch_schedule_input)\n\n        assert response.status_code == 201\n        assert response.json()[\"schedule_id\"] is not None"}
{"type": "test_file", "path": "tests/unit_tests/agent/test_queue_step_handler.py", "content": "import pytest\nfrom unittest.mock import Mock, patch\nfrom superagi.agent.queue_step_handler import QueueStepHandler\n\n\n# To prevent having to patch each time, setup a pytest fixture\n@pytest.fixture\ndef queue_step_handler():\n    # Mock dependencies\n    session = Mock()\n    llm = Mock()\n    agent_id = 1\n    agent_execution_id = 1\n\n    # Instantiate your class with the mocked dependencies\n    return QueueStepHandler(session, llm, agent_id, agent_execution_id)\n\n\n@pytest.fixture\ndef step_tool():\n    step_tool = Mock()\n    step_tool.unique_id = \"unique_id\"\n    step_tool.input_instruction = \"input_instruction\"\n    return step_tool\n\n\ndef test_queue_identifier(queue_step_handler):\n    step_tool = Mock()\n    step_tool.unique_id = \"step_id\"\n    assert queue_step_handler._queue_identifier(step_tool) == \"step_id_1\"\n\n\n@patch(\"superagi.agent.queue_step_handler.AgentExecution\")  # Replace with your actual module path\n@patch(\"superagi.agent.queue_step_handler.AgentWorkflowStep\")\n@patch(\"superagi.agent.queue_step_handler.AgentWorkflowStepTool\")\n@patch(\"superagi.agent.queue_step_handler.TaskQueue\")\ndef test_execute_step(task_queue_mock, agent_execution_mock, workflow_step_mock, step_tool_mock, queue_step_handler):\n    agent_execution_mock.get_agent_execution_from_id.return_value = Mock(current_agent_step_id=\"step_id\")\n    workflow_step_mock.find_by_id.return_value = Mock(action_reference_id=\"action_id\")\n    step_tool_mock.find_by_id.return_value = Mock()\n    task_queue_mock.return_value.get_status.return_value = None  # Mock the get_status method on TaskQueue\n\n    # Here you can add assertions depending on what you expect\n    # For example if you expect the return value to be \"default\", you could do\n    assert queue_step_handler.execute_step() == \"default\"\n\n\n@patch(\"superagi.agent.queue_step_handler.TaskQueue\")\n@patch(\"superagi.agent.queue_step_handler.AgentExecutionFeed\")\ndef test_add_to_queue(task_queue_mock, agent_execution_feed_mock, queue_step_handler, step_tool):\n    # Setup mocks\n    queue_step_handler._process_input_instruction = Mock(return_value='{\"reply\": [\"task1\", \"task2\"]}')\n    queue_step_handler._process_reply = Mock()\n\n    # Call the method\n    queue_step_handler._add_to_queue(task_queue_mock, step_tool)\n\n    # Verify the calls\n    queue_step_handler._process_input_instruction.assert_called_once_with(step_tool)\n    queue_step_handler._process_reply.assert_called_once_with(task_queue_mock, '{\"reply\": [\"task1\", \"task2\"]}')\n\n\n@patch(\"superagi.agent.queue_step_handler.TaskQueue\")\n@patch(\"superagi.agent.queue_step_handler.AgentExecutionFeed\")\ndef test_consume_from_queue(task_queue_mock, agent_execution_feed_mock, queue_step_handler, step_tool):\n    # Setup mocks\n    task_queue_mock.get_tasks.return_value = ['task1', 'task2']\n    task_queue_mock.get_first_task.return_value = 'task1'\n    agent_execution_feed_instance = agent_execution_feed_mock.return_value\n\n    # Call the method\n    queue_step_handler._consume_from_queue(task_queue_mock)\n\n    # Verify the calls\n    queue_step_handler.session.commit.assert_called()  # Ensure session commits were called\n    queue_step_handler.session.add.assert_called()\n    task_queue_mock.complete_task.assert_called_once_with(\"PROCESSED\")\n"}
{"type": "test_file", "path": "tests/unit_tests/controllers/test_tool.py", "content": "from unittest.mock import patch\n\nimport pytest\nfrom fastapi.testclient import TestClient\n\nfrom main import app\nfrom superagi.models.organisation import Organisation\nfrom superagi.models.tool import Tool\nfrom superagi.models.toolkit import Toolkit\n\nclient = TestClient(app)\n\n\n@pytest.fixture\ndef mocks():\n    # Mock tool kit data for testing\n    user_organisation = Organisation(id=1)\n    toolkit_1 = Toolkit(\n        id=1,\n        name=\"toolkit_1\",\n        description=\"None\",\n        show_toolkit=None,\n        organisation_id=1\n    )\n    toolkit_2 = Toolkit(\n        id=1,\n        name=\"toolkit_2\",\n        description=\"None\",\n        show_toolkit=None,\n        organisation_id=1\n    )\n    user_toolkits = [toolkit_1, toolkit_2]\n    tool_1 = Tool(\n        id=1,\n        name=\"tool_1\",\n        description=\"Test Tool\",\n        folder_name=\"test folder\",\n        file_name=\"test file\",\n        toolkit_id=1\n    )\n    tool_2 = Tool(\n        id=1,\n        name=\"tool_2\",\n        description=\"Test Tool\",\n        folder_name=\"test folder\",\n        file_name=\"test file\",\n        toolkit_id=1\n    )\n    tool_3 = Tool(\n        id=1,\n        name=\"tool_3\",\n        description=\"Test Tool\",\n        folder_name=\"test folder\",\n        file_name=\"test file\",\n        toolkit_id=2\n    )\n    tools = [tool_1, tool_2, tool_3]\n    return user_organisation, user_toolkits, tools, toolkit_1, toolkit_2, tool_1, tool_2, tool_3\n\n\ndef test_get_tools_success(mocks):\n    # Unpack the fixture data\n    user_organisation, user_toolkits, tools, toolkit_1, toolkit_2, tool_1, tool_2, tool_3 = mocks\n\n    # Mock the database session and query functions\n    with patch('superagi.helper.auth.get_user_organisation') as mock_get_user_org, \\\n            patch('superagi.controllers.tool.db') as mock_db, \\\n            patch('superagi.helper.auth.db') as mock_auth_db:\n\n        # Mock the toolkit filtering\n        mock_db.session.query.return_value.filter.return_value.all.side_effect = [user_toolkits, [tool_1, tool_2],\n                                                                                  [tool_3]]\n\n        # Call the function\n        response = client.get(\"/tools/list\")\n\n        # Assertions\n        assert response.status_code == 200\n        assert response.json() == [{'id': 1, 'name': 'tool_1', 'description': 'Test Tool', 'folder_name': 'test folder',\n                                    'file_name': 'test file', 'toolkit_id': 1},\n                                   {'id': 1, 'name': 'tool_2', 'description': 'Test Tool', 'folder_name': 'test folder',\n                                    'file_name': 'test file', 'toolkit_id': 1},\n                                   {'id': 1, 'name': 'tool_3', 'description': 'Test Tool', 'folder_name': 'test folder',\n                                    'file_name': 'test file', 'toolkit_id': 2}]\n"}
{"type": "test_file", "path": "tests/unit_tests/controllers/api/test_agent.py", "content": "import pytest\nfrom fastapi.testclient import TestClient\nfrom fastapi import HTTPException\n\nimport superagi.config.config\nfrom unittest.mock import MagicMock, patch,Mock\nfrom main import app\nfrom unittest.mock import patch,create_autospec\nfrom sqlalchemy.orm import Session\nfrom superagi.controllers.api.agent import ExecutionStateChangeConfigIn,AgentConfigUpdateExtInput\nfrom superagi.models.agent import Agent\nfrom superagi.models.project import Project\n\nclient = TestClient(app)\n\n@pytest.fixture\ndef mock_api_key_get():\n    mock_api_key = \"your_mock_api_key\"\n    return mock_api_key\n@pytest.fixture\ndef mock_execution_state_change_input():\n    return {\n\n    }\n@pytest.fixture\ndef mock_run_id_config():\n    return {\n        \"run_ids\":[1,2]\n    }\n\n@pytest.fixture\ndef mock_agent_execution():\n    return {\n\n    }\n@pytest.fixture\ndef mock_run_id_config_empty():\n    return {\n        \"run_ids\":[]\n    }\n\n@pytest.fixture\ndef mock_run_id_config_invalid():\n    return {\n        \"run_ids\":[12310]\n    }\n@pytest.fixture\ndef mock_agent_config_update_ext_input():\n    return AgentConfigUpdateExtInput(\n        tools=[{\"name\":\"Image Generation Toolkit\"}],\n        schedule=None,\n        goal=[\"Test Goal\"],\n        instruction=[\"Test Instruction\"],\n        constraints=[\"Test Constraints\"],\n        iteration_interval=10,\n        model=\"Test Model\",\n        max_iterations=100,\n        agent_type=\"Test Agent Type\"\n    )\n\n@pytest.fixture\ndef mock_update_agent_config():\n    return {\n        \"name\": \"agent_3_UPDATED\",\n        \"description\": \"AI assistant to solve complex problems\",\n        \"goal\": [\"create a photo of a cat\"],\n        \"agent_type\": \"Dynamic Task Workflow\",\n        \"constraints\": [\n            \"~4000 word limit for short term memory.\",\n            \"Your long term memory is short, so immediately save important information to files.\",\n            \"If you are unsure how you previously did something or want to recall past events, thinking about similar events will help you remember.\",\n            \"No user assistance\",\n            \"Exclusively use the commands listed in double quotes e.g. \\\"command name\\\"\"\n        ],\n        \"instruction\": [\"Be accurate\"],\n        \"tools\":[\n            {\n                \"name\":\"Image Generation Toolkit\"\n            }\n        ],\n        \"iteration_interval\": 500,\n        \"model\": \"gpt-4\",\n        \"max_iterations\": 100\n    }\n# Define test cases\n\ndef test_update_agent_not_found(mock_update_agent_config,mock_api_key_get):\n    with patch('superagi.helper.auth.get_organisation_from_api_key') as mock_get_user_org, \\\n            patch('superagi.helper.auth.validate_api_key') as mock_validate_api_key, \\\n                patch('superagi.helper.auth.db') as mock_auth_db, \\\n                    patch('superagi.controllers.api.agent.db') as db_mock:\n\n        # Mock the session\n        mock_session = create_autospec(Session)\n        # # Configure session query methods to return None for agent\n        mock_session.query.return_value.filter.return_value.first.return_value = None\n        response = client.put(\n            \"/v1/agent/1\",\n            headers={\"X-API-Key\": mock_api_key_get},  # Provide the mock API key in headers\n            json=mock_update_agent_config\n        )\n        assert response.status_code == 404\n        assert response.text == '{\"detail\":\"Agent not found\"}'\n\n\ndef test_get_run_resources_no_run_ids(mock_run_id_config_empty,mock_api_key_get):\n    with patch('superagi.helper.auth.get_organisation_from_api_key') as mock_get_user_org, \\\n            patch('superagi.helper.auth.validate_api_key') as mock_validate_api_key, \\\n                patch('superagi.helper.auth.db') as mock_auth_db, \\\n                    patch('superagi.controllers.api.agent.db') as db_mock, \\\n                        patch('superagi.controllers.api.agent.get_config', return_value=\"S3\") as mock_get_config:\n\n        # Mock the session\n        mock_session = create_autospec(Session)\n        # # Configure session query methods to return None for agent\n        mock_session.query.return_value.filter.return_value.first.return_value = None\n        response = client.post(\n            \"v1/agent/resources/output\",\n            headers={\"X-API-Key\": mock_api_key_get},  # Provide the mock API key in headers\n            json=mock_run_id_config_empty\n        )\n        assert response.status_code == 404\n        assert response.text == '{\"detail\":\"No execution_id found\"}'\n\ndef test_get_run_resources_invalid_run_ids(mock_run_id_config_invalid,mock_api_key_get):\n    with patch('superagi.helper.auth.get_organisation_from_api_key') as mock_get_user_org, \\\n            patch('superagi.helper.auth.validate_api_key') as mock_validate_api_key, \\\n                patch('superagi.helper.auth.db') as mock_auth_db, \\\n                    patch('superagi.controllers.api.agent.db') as db_mock, \\\n                        patch('superagi.controllers.api.agent.get_config', return_value=\"S3\") as mock_get_config:\n\n        # Mock the session\n        mock_session = create_autospec(Session)\n        # # Configure session query methods to return None for agent\n        mock_session.query.return_value.filter.return_value.first.return_value = None\n        response = client.post(\n            \"v1/agent/resources/output\",\n            headers={\"X-API-Key\": mock_api_key_get},  # Provide the mock API key in headers\n            json=mock_run_id_config_invalid\n        )\n        assert response.status_code == 404\n        assert response.text == '{\"detail\":\"One or more run id(s) not found\"}'\n\ndef test_resume_agent_runs_agent_not_found(mock_execution_state_change_input,mock_api_key_get):\n    with patch('superagi.helper.auth.get_organisation_from_api_key') as mock_get_user_org, \\\n            patch('superagi.helper.auth.validate_api_key') as mock_validate_api_key, \\\n                patch('superagi.helper.auth.db') as mock_auth_db, \\\n                    patch('superagi.controllers.api.agent.db') as db_mock:\n\n        # Mock the session\n        mock_session = create_autospec(Session)\n        # # Configure session query methods to return None for agent\n        mock_session.query.return_value.filter.return_value.first.return_value = None\n        response = client.post(\n            \"/v1/agent/1/resume\",\n            headers={\"X-API-Key\": mock_api_key_get},  # Provide the mock API key in headers\n            json=mock_execution_state_change_input\n        )\n        assert response.status_code == 404\n        assert response.text == '{\"detail\":\"Agent not found\"}'\n\n\ndef test_pause_agent_runs_agent_not_found(mock_execution_state_change_input,mock_api_key_get):\n    with patch('superagi.helper.auth.get_organisation_from_api_key') as mock_get_user_org, \\\n            patch('superagi.helper.auth.validate_api_key') as mock_validate_api_key, \\\n                patch('superagi.helper.auth.db') as mock_auth_db, \\\n                    patch('superagi.controllers.api.agent.db') as db_mock:\n\n        # Mock the session\n        mock_session = create_autospec(Session)\n        # # Configure session query methods to return None for agent\n        mock_session.query.return_value.filter.return_value.first.return_value = None\n        response = client.post(\n            \"/v1/agent/1/pause\",\n            headers={\"X-API-Key\": mock_api_key_get},  # Provide the mock API key in headers\n            json=mock_execution_state_change_input\n        )\n        assert response.status_code == 404\n        assert response.text == '{\"detail\":\"Agent not found\"}'\n\ndef test_create_run_agent_not_found(mock_agent_execution,mock_api_key_get):\n    with patch('superagi.helper.auth.get_organisation_from_api_key') as mock_get_user_org, \\\n            patch('superagi.helper.auth.validate_api_key') as mock_validate_api_key, \\\n                patch('superagi.helper.auth.db') as mock_auth_db, \\\n                    patch('superagi.controllers.api.agent.db') as db_mock:\n\n        # Mock the session\n        mock_session = create_autospec(Session)\n        # # Configure session query methods to return None for agent\n        mock_session.query.return_value.filter.return_value.first.return_value = None\n        response = client.post(\n            \"/v1/agent/1/run\",\n            headers={\"X-API-Key\": mock_api_key_get},  # Provide the mock API key in headers\n            json=mock_agent_execution\n        )\n        assert response.status_code == 404\n        assert response.text == '{\"detail\":\"Agent not found\"}'\n\ndef test_create_run_project_not_matching_org(mock_agent_execution, mock_api_key_get):\n    with patch('superagi.helper.auth.get_organisation_from_api_key') as mock_get_user_org, \\\n            patch('superagi.helper.auth.validate_api_key') as mock_validate_api_key, \\\n            patch('superagi.helper.auth.db') as mock_auth_db, \\\n            patch('superagi.controllers.api.agent.db') as db_mock:\n\n        # Mock the session and configure query methods to return agent and project\n        mock_session = create_autospec(Session)\n        mock_agent = Agent(id=1, project_id=1, agent_workflow_id=1)\n        mock_session.query.return_value.filter.return_value.first.return_value = mock_agent\n        mock_project = Project(id=1, organisation_id=2)  # Different organisation ID\n        db_mock.Project.find_by_id.return_value = mock_project\n        db_mock.session.return_value.__enter__.return_value = mock_session\n\n        response = client.post(\n            \"/v1/agent/1/run\",\n            headers={\"X-API-Key\": mock_api_key_get},\n            json=mock_agent_execution\n        )\n\n        assert response.status_code == 404\n        assert response.text == '{\"detail\":\"Agent not found\"}'\n"}
{"type": "test_file", "path": "tests/unit_tests/agent/test_agent_workflow_step_wait_handler.py", "content": "from datetime import datetime\nfrom unittest.mock import MagicMock, patch\n\nimport pytest\n\nfrom superagi.models.agent_execution import AgentExecution\nfrom superagi.models.workflows.agent_workflow_step import AgentWorkflowStep\nfrom superagi.agent.agent_workflow_step_wait_handler import AgentWaitStepHandler\n\n\n# Mock datetime.now() for testing\n@pytest.fixture\ndef mock_datetime_now():\n    return datetime(2023, 9, 6, 12, 0, 0)\n\n\n@pytest.fixture(autouse=True)\ndef mock_datetime_now_fixture(monkeypatch, mock_datetime_now):\n    monkeypatch.setattr(\"superagi.agent.agent_workflow_step_wait_handler.datetime\",\n                        MagicMock(now=MagicMock(return_value=mock_datetime_now)))\n\n# Test cases\n@patch.object(AgentExecution, 'get_agent_execution_from_id')\n@patch.object(AgentWorkflowStep, 'find_by_id')\n@patch.object(AgentWorkflowStep, 'fetch_next_step')\ndef test_handle_next_step_complete(mock_fetch_next_step, mock_find_by_id, mock_get_agent_execution_from_id, mock_datetime_now_fixture):\n    mock_session = MagicMock()\n    mock_agent_execution = MagicMock(current_agent_step_id=1, status=\"WAIT_STEP\")\n\n    mock_get_agent_execution_from_id.return_value = mock_agent_execution\n    mock_find_by_id.return_value = MagicMock()\n\n    mock_next_step = MagicMock(id=2)\n    mock_next_step.__str__.return_value = \"COMPLETE\"\n    mock_fetch_next_step.return_value = mock_next_step\n\n    handler = AgentWaitStepHandler(mock_session, 1, 2)\n\n    handler.handle_next_step()\n\n    # Assertions\n    assert mock_agent_execution.current_agent_step_id == -1\n    assert mock_agent_execution.status == \"COMPLETED\"\n    mock_session.commit.assert_called_once()\n\n\n# Test cases\n@patch.object(AgentExecution, 'get_agent_execution_from_id')\n@patch.object(AgentWorkflowStep, 'find_by_id')\n@patch.object(AgentWorkflowStep, 'fetch_next_step')\ndef test_execute_step(mock_fetch_next_step, mock_find_by_id, mock_get_agent_execution_from_id):\n    mock_session = MagicMock()\n    mock_agent_execution = MagicMock(current_agent_step_id=1, status=\"WAIT_STEP\")\n    mock_step_wait = MagicMock(status=\"WAITING\")\n\n    mock_get_agent_execution_from_id.return_value = mock_agent_execution\n    mock_find_by_id.return_value = mock_step_wait\n    mock_fetch_next_step.return_value = MagicMock()\n\n    handler = AgentWaitStepHandler(mock_session, 1, 2)\n\n    handler.execute_step()\n\n    # Assertions\n    assert mock_step_wait.status == \"WAITING\"\n    assert mock_agent_execution.status == \"WAIT_STEP\"\n    mock_session.commit.assert_called_once()"}
{"type": "test_file", "path": "tests/unit_tests/controllers/test_toolkit.py", "content": "from unittest.mock import patch, call\n\nimport pytest\nfrom fastapi.testclient import TestClient\n\nfrom main import app\nfrom superagi.models.organisation import Organisation\nfrom superagi.models.tool import Tool\nfrom superagi.models.tool_config import ToolConfig\nfrom superagi.types.key_type import ToolConfigKeyType\nfrom superagi.models.toolkit import Toolkit\n\nclient = TestClient(app)\n\n\n@pytest.fixture\ndef mocks():\n    # Mock tool kit data for testing\n    user_organisation = Organisation(id=1)\n    toolkit_1 = Toolkit(\n        id=1,\n        name=\"toolkit_1\",\n        description=\"None\",\n        show_toolkit=None,\n        organisation_id=1\n    )\n    toolkit_2 = Toolkit(\n        id=1,\n        name=\"toolkit_2\",\n        description=\"None\",\n        show_toolkit=None,\n        organisation_id=1\n    )\n    user_toolkits = [toolkit_1, toolkit_2]\n    tool_1 = Tool(\n        id=1,\n        name=\"tool_1\",\n        description=\"Test Tool\",\n        folder_name=\"test folder\",\n        file_name=\"test file\",\n        toolkit_id=1\n    )\n    tool_2 = Tool(\n        id=1,\n        name=\"tool_2\",\n        description=\"Test Tool\",\n        folder_name=\"test folder\",\n        file_name=\"test file\",\n        toolkit_id=1\n    )\n    tool_3 = Tool(\n        id=1,\n        name=\"tool_3\",\n        description=\"Test Tool\",\n        folder_name=\"test folder\",\n        file_name=\"test file\",\n        toolkit_id=2\n    )\n    tools = [tool_1, tool_2, tool_3]\n    return user_organisation, user_toolkits, tools, toolkit_1, toolkit_2, tool_1, tool_2, tool_3\n\n\n@pytest.fixture\ndef mock_toolkit_details():\n    # Mock toolkit details data for testing\n    toolkit_details = {\n        \"name\": \"toolkit_1\",\n        \"description\": \"Test Toolkit\",\n        \"tool_code_link\": \"https://example.com/toolkit_1\",\n        \"show_toolkit\": None,\n        \"tools\": [\n            {\n                \"name\": \"tool_1\",\n                \"description\": \"Test Tool 1\",\n                \"folder_name\": \"test_folder_1\",\n                \"class_name\": \"TestTool1\",\n                \"file_name\": \"test_tool_1.py\"\n            },\n            {\n                \"name\": \"tool_2\",\n                \"description\": \"Test Tool 2\",\n                \"folder_name\": \"test_folder_2\",\n                \"class_name\": \"TestTool2\",\n                \"file_name\": \"test_tool_2.py\"\n            }\n        ],\n        \"configs\": [\n            {\n                \"key\": \"config_key_1\",\n                \"value\": \"config_value_1\",\n                'key_type': ToolConfigKeyType.STRING,\n                'is_secret': True,\n                'is_required': False\n            },\n            {\n                \"key\": \"config_key_2\",\n                \"value\": \"config_value_2\",\n                'key_type': ToolConfigKeyType.FILE,\n                'is_secret': True,\n                'is_required': False\n\n            }\n        ]\n    }\n    return toolkit_details\n\n\ndef test_handle_marketplace_operations_list(mocks):\n    # Unpack the fixture data\n    user_organisation, user_toolkits, tools, toolkit_1, toolkit_2, tool_1, tool_2, tool_3 = mocks\n\n    # Mock the database session and query functions\n    with patch('superagi.helper.auth.get_user_organisation') as mock_get_user_org, \\\n            patch('superagi.controllers.toolkit.db') as mock_db, \\\n            patch('superagi.models.toolkit.Toolkit.fetch_marketplace_list') as mock_fetch_marketplace_list, \\\n            patch('superagi.helper.auth.db') as mock_auth_db:\n        # Set up mock data\n        mock_db.session.query.return_value.filter.return_value.all.side_effect = [user_toolkits]\n        mock_fetch_marketplace_list.return_value = [toolkit_1.to_dict(), toolkit_2.to_dict()]\n\n        # Call the function\n        response = client.get(\"/toolkits/get/list\", params={\"page\": 0})\n\n        # Assertions\n        assert response.status_code == 200\n        assert response.json() == [\n            {\n                \"id\": 1,\n                \"name\": \"toolkit_1\",\n                \"description\": \"None\",\n                \"show_toolkit\": None,\n                \"organisation_id\": 1,\n                \"is_installed\": True\n            },\n            {\n                \"id\": 1,\n                \"name\": \"toolkit_2\",\n                \"description\": \"None\",\n                \"show_toolkit\": None,\n                \"organisation_id\": 1,\n                \"is_installed\": True\n            }\n        ]\n\n\ndef test_install_toolkit_from_marketplace(mock_toolkit_details):\n    # Mock the database session and query functions\n    with patch('superagi.helper.auth.get_user_organisation') as mock_get_user_org, \\\n            patch('superagi.models.toolkit.Toolkit.fetch_marketplace_detail') as mock_fetch_marketplace_detail, \\\n            patch('superagi.models.toolkit.Toolkit.add_or_update') as mock_add_or_update, \\\n            patch('superagi.models.tool.Tool.add_or_update') as mock_tool_add_or_update, \\\n            patch('superagi.controllers.toolkit.db') as mock_db, \\\n            patch('superagi.helper.auth.db') as mock_auth_db, \\\n            patch('superagi.models.tool_config.ToolConfig.add_or_update') as mock_tool_config_add_or_update:\n        # Set up mock data and behavior\n        mock_get_user_org.return_value = Organisation(id=1)\n        mock_fetch_marketplace_detail.return_value = mock_toolkit_details\n        mock_add_or_update.return_value = Toolkit(id=1, name=mock_toolkit_details['name'],\n                                                  description=mock_toolkit_details['description'])\n\n        # Call the function\n        response = client.get(\"/toolkits/get/install/toolkit_1\")\n\n        # Assertions\n        assert response.status_code == 200\n        assert response.json() == {\"message\": \"ToolKit installed successfully\"}\n\n        # Verify the function calls\n        mock_fetch_marketplace_detail.assert_called_once_with(search_str=\"details\", toolkit_name=\"toolkit_1\")\n"}
{"type": "test_file", "path": "tests/unit_tests/agent/test_agent_tool_step_handler.py", "content": "import json\nfrom unittest.mock import Mock, create_autospec, patch\n\nimport pytest\n\nfrom superagi.agent.agent_tool_step_handler import AgentToolStepHandler\nfrom superagi.agent.common_types import ToolExecutorResponse\nfrom superagi.agent.output_handler import ToolOutputHandler\nfrom superagi.agent.tool_builder import ToolBuilder\nfrom superagi.helper.token_counter import TokenCounter\nfrom superagi.models.agent import Agent\nfrom superagi.models.agent_config import AgentConfiguration\nfrom superagi.models.agent_execution import AgentExecution\nfrom superagi.models.agent_execution_config import AgentExecutionConfiguration\nfrom superagi.models.agent_execution_permission import AgentExecutionPermission\nfrom superagi.models.tool import Tool\nfrom superagi.models.workflows.agent_workflow_step import AgentWorkflowStep\nfrom superagi.models.workflows.agent_workflow_step_tool import AgentWorkflowStepTool\nfrom superagi.resource_manager.resource_summary import ResourceSummarizer\nfrom superagi.tools.code.write_code import CodingTool\n\n\n# Given\n@pytest.fixture\ndef handler():\n    mock_session = Mock()\n    llm = Mock()\n    agent_id = 1\n    agent_execution_id = 1\n\n    # Creating an instance of the class to test\n    handler = AgentToolStepHandler(mock_session, llm, agent_id, agent_execution_id, None)\n    return handler\n\n\ndef test_create_permission_request(handler):\n    # Arrange\n    execution = Mock()\n    step_tool = Mock()\n    step_tool.input_instruction = \"input_instruction\"\n    handler.session.commit = Mock()\n    handler.session.flush = Mock()\n\n    mock_permission = create_autospec(AgentExecutionPermission)\n    with patch('superagi.agent.agent_tool_step_handler.AgentExecutionPermission', return_value=mock_permission) as mock_cls:\n        # Act\n        handler._create_permission_request(execution, step_tool)\n\n        # Assert\n        mock_cls.assert_called_once_with(\n            agent_execution_id=handler.agent_execution_id,\n            status=\"PENDING\",\n            agent_id=handler.agent_id,\n            tool_name=\"WAIT_FOR_PERMISSION\",\n            question=step_tool.input_instruction,\n            assistant_reply=\"\"\n        )\n        handler.session.add.assert_called_once_with(mock_permission)\n        execution.permission_id = mock_permission.id\n        execution.status = \"WAITING_FOR_PERMISSION\"\n        assert handler.session.commit.call_count == 2\n        assert handler.session.flush.call_count == 1\n\n\n\ndef test_execute_step(handler):\n    # Arrange\n    execution = create_autospec(AgentExecution)\n    workflow_step = create_autospec(AgentWorkflowStep)\n    step_tool = create_autospec(AgentWorkflowStepTool)\n    agent_config = {}\n    agent_execution_config = {}\n\n    with patch.object(AgentExecution, 'get_agent_execution_from_id', return_value=execution), \\\n        patch.object(AgentWorkflowStep, 'find_by_id', return_value=workflow_step), \\\n        patch.object(AgentWorkflowStepTool, 'find_by_id', return_value=step_tool), \\\n        patch.object(Agent, 'fetch_configuration', return_value=agent_config), \\\n        patch.object(AgentExecutionConfiguration, 'fetch_configuration', return_value=agent_execution_config):\n\n        handler._handle_wait_for_permission = Mock(return_value=True)\n        handler._create_permission_request = Mock()\n        handler._process_input_instruction = Mock(return_value=\"{\\\"}\")\n        handler._build_tool_obj = Mock()\n        handler._process_output_instruction = Mock(return_value=\"step_response\")\n        handler._handle_next_step = Mock()\n\n        # Act\n        tool_output_handler = Mock(spec=ToolOutputHandler)\n        tool_output_handler.handle.return_value = ToolExecutorResponse(status=\"SUCCESS\", output=\"final_response\")\n\n        with patch('superagi.agent.agent_tool_step_handler.ToolOutputHandler', return_value=tool_output_handler):\n            # Act\n            handler.execute_step()\n\n            # Assert\n            handler._handle_wait_for_permission.assert_called_once()\n            handler._process_input_instruction.assert_called_once_with(agent_config, agent_execution_config, step_tool,\n                                                                       workflow_step)\n            handler._process_output_instruction.assert_called_once()\n\n\ndef test_handle_next_step_with_complete(handler):\n    # Arrange\n    next_step = \"COMPLETE\"\n    execution = create_autospec(AgentExecution)\n\n    with patch.object(AgentExecution, 'get_agent_execution_from_id', return_value=execution):\n        # Act\n        handler._handle_next_step(next_step)\n\n        # Assert\n        assert execution.current_agent_step_id == -1\n        assert execution.status == \"COMPLETED\"\n        handler.session.commit.assert_called_once()\n\n\ndef test_handle_next_step_with_next_step(handler):\n    # Arrange\n    next_step = create_autospec(AgentExecution)  # Mocking the next_step object\n    execution = create_autospec(AgentExecution)\n\n    with patch.object(AgentExecution, 'get_agent_execution_from_id', return_value=execution), \\\n        patch.object(AgentExecution, 'assign_next_step_id') as mock_assign_next_step_id:\n\n        # Act\n        handler._handle_next_step(next_step)\n\n        # Assert\n        mock_assign_next_step_id.assert_called_once_with(handler.session, handler.agent_execution_id, next_step.id)\n        handler.session.commit.assert_called_once()\n\n\ndef test_build_tool_obj(handler):\n    # Arrange\n    agent_config = {\"model\": \"model1\", \"resource_summary\": \"summary\"}\n    agent_execution_config = {}\n    tool_name = \"QueryResourceTool\"\n    model_api_key = {\"provider\":\"provider\",\"api_key\":\"apikey\"}\n    resource_summary = \"summary\"\n    tool = Tool()\n\n    with patch.object(AgentConfiguration, 'get_model_api_key', return_value=model_api_key), \\\n         patch.object(ToolBuilder, 'build_tool', return_value=tool), \\\n         patch.object(ToolBuilder, 'set_default_params_tool', return_value=tool), \\\n         patch.object(ResourceSummarizer, 'fetch_or_create_agent_resource_summary', return_value=resource_summary), \\\n         patch.object(handler.session, 'query', return_value=Mock(first=Mock(return_value=tool))):\n\n        # Act\n        result = handler._build_tool_obj(agent_config, agent_execution_config, tool_name)\n\n        # Assert\n        assert result == tool\n\n\ndef test_process_output_instruction(handler):\n    # Arrange\n    final_response = \"final_response\"\n    step_tool = AgentWorkflowStepTool()\n    workflow_step = AgentWorkflowStep()\n    mock_response = {\"content\": \"response_content\"}\n    mock_model = Mock()\n    current_tokens = 10\n    token_limit = 100\n\n    with patch.object(handler, '_build_tool_output_prompt', return_value=\"prompt\"), \\\n         patch.object(TokenCounter, 'count_message_tokens', return_value=current_tokens), \\\n         patch.object(TokenCounter, 'token_limit', return_value=token_limit), \\\n         patch.object(handler.llm, 'chat_completion', return_value=mock_response), \\\n         patch.object(AgentExecution, 'update_tokens'):\n\n        # Act\n        result = handler._process_output_instruction(final_response, step_tool, workflow_step)\n\n        # Assert\n        assert result == mock_response['content']\n\n\ndef test_build_tool_input_prompt(handler):\n    # Arrange\n    step_tool = AgentWorkflowStepTool()\n    step_tool.tool_name = \"CodingTool\"\n    step_tool.input_instruction = \"TestInstruction\"\n    tool = CodingTool()\n    # tool.name = \"TestTool\"\n    # tool.description = \"TestDescription\"\n    # tool.args = {\"arg1\": \"val1\"}\n    agent_execution_config = {\"goal\": [\"Goal1\", \"Goal2\"]}\n    mock_prompt = \"{goals}{tool_name}{instruction}{tool_schema}\"\n\n    with patch('superagi.agent.agent_tool_step_handler.PromptReader.read_agent_prompt', return_value=mock_prompt), \\\n            patch('superagi.agent.agent_tool_step_handler.AgentPromptBuilder.add_list_items_to_string', return_value=\"Goal1, Goal2\"):\n        # Act\n        result = handler._build_tool_input_prompt(step_tool, tool, agent_execution_config)\n\n        # Assert\n        result = result.replace(\"{goals}\", \"Goal1, Goal2\")\n        result = result.replace(\"{tool_name}\", step_tool.tool_name)\n        result = result.replace(\"{instruction}\", step_tool.input_instruction)\n        tool_schema = f\"\\\"{tool.name}\\\": {tool.description}, args json schema: {json.dumps(tool.args)}\"\n        result = result.replace(\"{tool_schema}\", tool_schema)\n\n        assert \"\"\"Goal1, Goal2CodingToolTestInstruction\"\"\" in result\n\n\ndef test_build_tool_output_prompt(handler):\n    # Arrange\n    step_tool = AgentWorkflowStepTool()\n    step_tool.tool_name = \"TestTool\"\n    step_tool.output_instruction = \"TestInstruction\"\n    tool_output = \"TestOutput\"\n    workflow_step = AgentWorkflowStep()\n    expected_prompt = \"TestOutputTestToolTestInstruction['option1', 'option2']\"\n    mock_prompt = \"{tool_output}{tool_name}{instruction}{output_options}\"\n    step_responses = [\"option1\", \"option2\", \"default\"]\n\n    with patch('superagi.agent.agent_tool_step_handler.PromptReader.read_agent_prompt', return_value=mock_prompt), \\\n            patch.object(handler, '_get_step_responses', return_value=step_responses):\n        # Act\n        result = handler._build_tool_output_prompt(step_tool, tool_output, workflow_step)\n\n        # Assert\n        expected_prompt = expected_prompt.replace(\"{tool_output}\", tool_output)\n        expected_prompt = expected_prompt.replace(\"{tool_name}\", step_tool.tool_name)\n        expected_prompt = expected_prompt.replace(\"{instruction}\", step_tool.output_instruction)\n        expected_prompt = expected_prompt.replace(\"{output_options}\", str(step_responses))\n\n        assert result == expected_prompt\n\n\ndef test_handle_wait_for_permission_approved(handler):\n    # Arrange\n    agent_execution = AgentExecution()\n    agent_execution.status = \"WAITING_FOR_PERMISSION\"\n    agent_execution.permission_id = 123\n    workflow_step = AgentWorkflowStep()\n    agent_execution_permission = AgentExecutionPermission()\n    agent_execution_permission.status = \"APPROVED\"\n    next_step = AgentWorkflowStep()\n\n    handler.session.query.return_value.filter.return_value.first.return_value = agent_execution_permission\n    handler._handle_next_step = Mock()\n    AgentWorkflowStep.fetch_next_step = Mock(return_value=next_step)\n\n    # Act\n    result = handler._handle_wait_for_permission(agent_execution, workflow_step)\n\n    # Assert\n    assert result == False\n    handler._handle_next_step.assert_called_once_with(next_step)\n    assert agent_execution.status == \"RUNNING\"\n    assert agent_execution.permission_id == -1\n\n\ndef test_handle_wait_for_permission_denied(handler):\n    # Arrange\n    agent_execution = AgentExecution()\n    agent_execution.status = \"WAITING_FOR_PERMISSION\"\n    agent_execution.permission_id = 123\n    workflow_step = AgentWorkflowStep()\n    agent_execution_permission = AgentExecutionPermission()\n    agent_execution_permission.status = \"DENIED\"\n    agent_execution_permission.user_feedback = \"User feedback\"\n    next_step = AgentWorkflowStep()\n\n    handler.session.query.return_value.filter.return_value.first.return_value = agent_execution_permission\n    handler._handle_next_step = Mock()\n    AgentWorkflowStep.fetch_next_step = Mock(return_value=next_step)\n\n    # Act\n    result = handler._handle_wait_for_permission(agent_execution, workflow_step)\n\n    # Assert\n    assert result == False\n    handler._handle_next_step.assert_called_once_with(next_step)\n    assert agent_execution.status == \"RUNNING\"\n    assert agent_execution.permission_id == -1\n"}
{"type": "test_file", "path": "tests/unit_tests/controllers/test_agent.py", "content": "from unittest.mock import patch, Mock\nfrom unittest import mock\nimport pytest\nfrom fastapi.testclient import TestClient\n\nfrom main import app\nfrom superagi.models.agent_schedule import AgentSchedule\nfrom superagi.models.agent_config import AgentConfiguration\nfrom superagi.models.agent import Agent\nfrom datetime import datetime, timedelta\nfrom pytz import timezone\n\nclient = TestClient(app)\n\n@pytest.fixture\ndef mock_patch_schedule_input():\n    return{\n        \"agent_id\": 1,\n        \"start_time\": \"2023-02-02 01:00:00\",\n        \"recurrence_interval\": \"2 Hours\",\n        \"expiry_date\": \"2023-12-30 01:00:00\",\n        \"expiry_runs\": -1\n    }\n\n@pytest.fixture\ndef mock_schedule():\n    # Mock schedule data for testing\n    return AgentSchedule(id=1, agent_id=1, status=\"SCHEDULED\")\n\n@pytest.fixture\ndef mock_agent_config():\n    return AgentConfiguration(key=\"user_timezone\", agent_id=1, value='GMT')\n\n@pytest.fixture\ndef mock_schedule_get():\n    return AgentSchedule(\n        id=1, \n        agent_id=1, \n        status=\"SCHEDULED\",\n        start_time= datetime(2022, 1, 1, 10, 30),\n        recurrence_interval=\"5 Minutes\",\n        expiry_date=datetime(2022, 1, 1, 10, 30) + timedelta(days=10),\n        expiry_runs=5 \n    )\n\n'''Test for Stopping Agent Scheduling'''\ndef test_stop_schedule_success(mock_schedule):\n    with patch('superagi.controllers.agent.db') as mock_db:\n        # Set up the database query result\n        mock_db.session.query.return_value.filter.return_value.first.return_value = mock_schedule \n\n        # Call the endpoint\n        response = client.post(\"agents/stop/schedule?agent_id=1\")\n\n        # Verify the HTTP response\n        assert response.status_code == 200\n\n        # Verify changes in the mock agent schedule\n        assert mock_schedule.status == \"STOPPED\"\n\n\ndef test_stop_schedule_not_found():\n    with patch('superagi.controllers.agent.db') as mock_db:\n        # Set up the database query result\n        mock_db.session.query.return_value.filter.return_value.first.return_value = None\n\n        # Call the endpoint\n        response = client.post(\"agents/stop/schedule?agent_id=1\")\n\n        # Verify the HTTP response\n        assert response.status_code == 404\n        assert response.json() == {\"detail\": \"Schedule not found\"}\n\n\n'''Test for editing agent schedule'''\ndef test_edit_schedule_success(mock_schedule, mock_patch_schedule_input):\n    with patch('superagi.controllers.agent.db') as mock_db:\n        # Set up the database query result\n        mock_db.session.query.return_value.filter.return_value.first.return_value = mock_schedule\n\n        # Call the endpoint\n        response = client.put(\"agents/edit/schedule\", json=mock_patch_schedule_input)\n\n        # Verify the HTTP response\n        assert response.status_code == 200\n        start_time = datetime.strptime(mock_patch_schedule_input[\"start_time\"], \"%Y-%m-%d %H:%M:%S\")\n        expiry_date = datetime.strptime(mock_patch_schedule_input[\"expiry_date\"], \"%Y-%m-%d %H:%M:%S\")\n\n        # Verify changes in the mock agent schedule\n        assert mock_schedule.start_time == start_time\n        assert mock_schedule.recurrence_interval == mock_patch_schedule_input[\"recurrence_interval\"]\n        assert mock_schedule.expiry_date == expiry_date\n        assert mock_schedule.expiry_runs == mock_patch_schedule_input[\"expiry_runs\"]\n\n\ndef test_edit_schedule_not_found(mock_patch_schedule_input):\n    with patch('superagi.controllers.agent.db') as mock_db:\n        # Set up the database query result\n        mock_db.session.query.return_value.filter.return_value.first.return_value = None\n\n        # Call the endpoint\n        response = client.put(\"agents/edit/schedule\", json=mock_patch_schedule_input)\n\n        # Verify the HTTP response\n        assert response.status_code == 404\n        assert response.json() == {\"detail\": \"Schedule not found\"}\n\n'''Test for getting agent schedule'''\ndef test_get_schedule_data_success(mock_schedule_get, mock_agent_config):\n    with patch('superagi.controllers.agent.db') as mock_db:\n        mock_db.session.query.return_value.filter.return_value.first.side_effect = [mock_schedule_get, mock_agent_config]\n        response = client.get(\"agents/get/schedule_data/1\")\n        assert response.status_code == 200\n\n        time_gmt = mock_schedule_get.start_time.astimezone(timezone('GMT'))\n\n        expected_data = {\n            \"current_datetime\": mock.ANY,\n            \"start_date\": time_gmt.strftime(\"%d %b %Y\"),\n            \"start_time\": time_gmt.strftime(\"%I:%M %p\"),\n            \"recurrence_interval\": mock_schedule_get.recurrence_interval,\n            \"expiry_date\": mock_schedule_get.expiry_date.astimezone(timezone('GMT')).strftime(\"%d/%m/%Y\"),\n            \"expiry_runs\": mock_schedule_get.expiry_runs,\n        }\n        assert response.json() == expected_data\n\n\ndef test_get_schedule_data_not_found():\n    with patch('superagi.controllers.agent.db') as mock_db:\n        # Set up the database query result\n        mock_db.session.query.return_value.filter.return_value.first.return_value = None\n\n        # Call the endpoint\n        response = client.get(\"agents/get/schedule_data/1\")\n\n        # Verify the HTTP response\n        assert response.status_code == 404\n        assert response.json() == {\"detail\": \"Agent Schedule not found\"}\n\n\n@pytest.fixture\ndef mock_agent_config_schedule():\n    return {\n        \"agent_config\": {\n            \"name\": \"SmartAGI\", \n            \"project_id\": 1,\n            \"description\": \"AI assistant to solve complex problems\",\n            \"goal\": [\"Share research on latest google news in fashion\"],\n            \"agent_workflow\": \"Don't Maintain Task Queue\",\n            \"constraints\": [\n                \"~4000 word limit for short term memory.\",\n                \"No user assistance\",\n                \"Exclusively use the commands listed in double quotes\"\n            ],\n            \"instruction\": [],\n            \"exit\": \"Exit strategy\",\n            \"iteration_interval\": 500,\n            \"model\": \"gpt-4\",\n            \"permission_type\": \"Type 1\",\n            \"LTM_DB\": \"Database Pinecone\",\n            \"toolkits\": [1],\n            \"tools\": [],\n            \"memory_window\": 10,\n            \"max_iterations\": 25,\n            \"user_timezone\": \"Asia/Kolkata\"\n        },\n        \"schedule\": {\n            \"start_time\": \"2023-07-04 11:13:00\",\n            \"expiry_runs\": -1,\n            \"recurrence_interval\": None,\n            \"expiry_date\": None\n        }\n    }\n\n@pytest.fixture\ndef mock_agent():\n    agent = Agent(id=1, name=\"SmartAGI\", project_id=1)\n    return agent\n\n\ndef test_create_and_schedule_agent_success(mock_agent_config_schedule, mock_agent, mock_schedule):\n    \n    with patch('superagi.models.agent.Agent') as AgentMock,\\\n         patch('superagi.controllers.agent.Project') as ProjectMock,\\\n         patch('superagi.controllers.agent.Tool') as ToolMock,\\\n         patch('superagi.controllers.agent.Toolkit') as ToolkitMock,\\\n         patch('superagi.controllers.agent.AgentSchedule') as AgentScheduleMock,\\\n         patch('superagi.controllers.agent.db') as db_mock:\n\n        project_mock = Mock()\n        ProjectMock.get.return_value = project_mock\n\n        # AgentMock.create_agent_with_config.return_value = mock_agent\n        AgentMock.return_value =  mock_agent\n\n        tool_mock = Mock()\n        ToolMock.get_invalid_tools.return_value = []\n\n        toolkit_mock = Mock()\n        ToolkitMock.fetch_tool_ids_from_toolkit.return_value = []\n        \n        agent_schedule_mock = Mock()\n        agent_schedule_mock.id = None  # id is None before commit\n        AgentScheduleMock.return_value = mock_schedule\n        \n        db_mock.session.query.return_value.get.return_value = project_mock\n        db_mock.session.add.return_value = None\n        db_mock.session.commit.side_effect = lambda: setattr(agent_schedule_mock, 'id', 1)  # id is set after commit\n        db_mock.session.query.return_value.get.return_value = project_mock\n\n        response = client.post(\"agents/schedule\", json=mock_agent_config_schedule)\n\n        assert response.status_code == 201\n        assert response.json() == {\n            \"id\": mock_agent.id,\n            \"name\": mock_agent.name,\n            \"contentType\": \"Agents\",\n            \"schedule_id\": 1\n        }\n\n\ndef test_create_and_schedule_agent_project_not_found(mock_agent_config_schedule):\n    with patch('superagi.controllers.agent.db') as mock_db:\n        # Set up the database query result\n        mock_db.session.query.return_value.get.return_value = None\n\n        # Call the endpoint\n        response = client.post(\"agents/schedule\", json=mock_agent_config_schedule)\n\n        # Verify the HTTP response\n        assert response.status_code == 404\n        assert response.json() == {\"detail\": \"Project not found\"}"}
{"type": "test_file", "path": "tests/tools/google_calendar/list_events_test.py", "content": "import unittest\nfrom datetime import datetime\nfrom unittest.mock import MagicMock, patch\nfrom pydantic import ValidationError\nfrom superagi.tools.google_calendar.list_calendar_events import ListCalendarEventsInput, ListCalendarEventsTool\nfrom superagi.helper.google_calendar_creds import GoogleCalendarCreds\nfrom superagi.helper.calendar_date import CalendarDate\n\nclass TestListCalendarEventsInput(unittest.TestCase):\n    \n    def test_valid_input(self):\n        input_data = {\n            \"start_time\": \"20:00:00\",\n            \"start_date\": \"2022-11-10\",\n            \"end_date\": \"2022-11-11\",\n            \"end_time\": \"22:00:00\",\n        }\n        try:\n            ListCalendarEventsInput(**input_data)\n            validation_passed = True\n        except ValidationError:\n            validation_passed = False\n        self.assertEqual(validation_passed, True)\n    \n    def test_invalid_input(self):\n        input_data = {\n            \"start_time\": \"invalid time\",\n            \"start_date\": \"invalid date\",\n            \"end_date\": \"another invalid date\",\n            \"end_time\": \"another invalid time\",\n        }\n        with self.assertRaises(ValidationError):\n            ListCalendarEventsInput(**input_data)\n\nclass TestListCalendarEventsTool(unittest.TestCase):\n    @patch.object(GoogleCalendarCreds, 'get_credentials')\n    @patch.object(CalendarDate, 'get_date_utc')\n    \n    def test_without_events(self, mock_get_date_utc, mock_get_credentials):\n        tool = ListCalendarEventsTool()\n        mock_get_credentials.return_value = {\n            \"success\": True,\n            \"service\": MagicMock()\n        }\n        mock_service = mock_get_credentials()[\"service\"]\n        mock_service.events().list().execute.return_value = {}\n        mock_get_date_utc.return_value = {\n            'start_datetime_utc': datetime.now().isoformat(),\n            'end_datetime_utc': datetime.now().isoformat()\n        }\n        result = tool._execute('20:00:00', '2022-11-10', '2022-11-11', '22:00:00')\n        self.assertEqual(result, \"No events found for the given date and time range.\")\n\nif __name__ == \"__main__\":\n    unittest.main()\n\n\n\n\n\n\n\n\n\n"}
{"type": "test_file", "path": "tests/unit_tests/controllers/test_models_controller.py", "content": "from unittest.mock import patch, MagicMock\nimport pytest\nfrom fastapi.testclient import TestClient\nfrom main import app\nfrom llama_cpp import Llama\nfrom llama_cpp import LlamaGrammar\nimport llama_cpp\n\nfrom superagi.helper.llm_loader import LLMLoader \n\nclient = TestClient(app)\n\n@patch('superagi.controllers.models_controller.db')\ndef test_store_api_keys_success(mock_get_db):\n    request = {\n        \"model_provider\": \"mock_provider\",\n        \"model_api_key\": \"mock_key\"\n    }\n    with patch('superagi.helper.auth.get_user_organisation') as mock_get_user_org, \\\n        patch('superagi.helper.auth.db') as mock_auth_db:\n\n        response = client.post(\"/models_controller/store_api_keys\", json=request)\n        assert response.status_code == 200\n\n@patch('superagi.controllers.models_controller.db')\ndef test_get_api_keys_success(mock_get_db):\n    with patch('superagi.helper.auth.get_user_organisation') as mock_get_user_org, \\\n        patch('superagi.helper.auth.db') as mock_auth_db:\n        response = client.get(\"/models_controller/get_api_keys\")\n        assert response.status_code == 200\n\n@patch('superagi.controllers.models_controller.db')\n@patch('superagi.controllers.models_controller.ModelsConfig.fetch_api_key', return_value = {})\ndef test_get_api_key_success(mock_fetch_api_key, mock_get_db):\n    params = {\n        \"model_provider\": \"model\"\n    }\n    with patch('superagi.helper.auth.get_user_organisation') as mock_get_user_org, \\\n        patch('superagi.helper.auth.db') as mock_auth_db:\n        response = client.get(\"/models_controller/get_api_key\", params=params)\n        assert response.status_code == 200\n\n@patch('superagi.controllers.models_controller.db')\ndef test_verify_end_point_success(mock_get_db):\n    with patch('superagi.helper.auth.db') as mock_auth_db:\n        response = client.get(\"/models_controller/verify_end_point?model_api_key=mock_key&end_point=mock_point&model_provider=mock_provider\")\n        assert response.status_code == 200\n\n@patch('superagi.controllers.models_controller.db')\ndef test_store_model_success(mock_get_db):\n    request = {\n        \"model_name\": \"mock_model\",\n        \"description\": \"mock_description\",\n        \"end_point\": \"mock_end_point\",\n        \"model_provider_id\": 1,\n        \"token_limit\": 10,\n        \"type\": \"mock_type\",\n        \"version\": \"mock_version\",\n        \"context_length\":4096\n    }\n    with patch('superagi.helper.auth.get_user_organisation') as mock_get_user_org, \\\n        patch('superagi.helper.auth.db') as mock_auth_db:\n        response = client.post(\"/models_controller/store_model\", json=request)\n        assert response.status_code == 200\n\n@patch('superagi.controllers.models_controller.db')\ndef test_fetch_models_success(mock_get_db):\n    with patch('superagi.helper.auth.get_user_organisation') as mock_get_user_org, \\\n        patch('superagi.helper.auth.db') as mock_auth_db:\n        response = client.get(\"/models_controller/fetch_models\")\n        assert response.status_code == 200\n\n@patch('superagi.controllers.models_controller.db')\ndef test_fetch_model_details_success(mock_get_db):\n    with patch('superagi.helper.auth.get_user_organisation') as mock_get_user_org, \\\n        patch('superagi.helper.auth.db') as mock_auth_db:\n        response = client.get(\"/models_controller/fetch_model/1\")\n        assert response.status_code == 200\n\n@patch('superagi.controllers.models_controller.db')\ndef test_fetch_data_success(mock_get_db):\n    request = {\n        \"model\": \"model\"\n    }\n    with patch('superagi.helper.auth.get_user_organisation') as mock_get_user_org, \\\n        patch('superagi.helper.auth.db') as mock_auth_db:\n        response = client.post(\"/models_controller/fetch_model_data\", json=request)\n        assert response.status_code == 200\n\n@patch('superagi.controllers.models_controller.db')\ndef test_get_marketplace_models_list_success(mock_get_db):\n    with patch('superagi.helper.auth.get_user_organisation') as mock_get_user_org, \\\n        patch('superagi.helper.auth.db') as mock_auth_db, \\\n        patch('superagi.controllers.models_controller.requests.get') as mock_get:\n\n        mock_response = MagicMock()\n        mock_response.status_code = 200\n        mock_get.return_value = mock_response\n\n        response = client.get(\"/models_controller/marketplace/list/0\")\n        assert response.status_code == 200\n\n@patch('superagi.controllers.models_controller.db')\ndef test_get_marketplace_models_list_success(mock_get_db):\n    with patch('superagi.helper.auth.get_user_organisation') as mock_get_user_org, \\\n        patch('superagi.helper.auth.db') as mock_auth_db:\n        response = client.get(\"/models_controller/marketplace/list/0\")\n        assert response.status_code == 200\n\ndef test_get_local_llm():\n    with(patch.object(LLMLoader, 'model', new_callable=MagicMock)) as mock_model:\n        with(patch.object(LLMLoader, 'grammar', new_callable=MagicMock)) as mock_grammar:\n\n            mock_model.create_chat_completion.return_value = {\"choices\": [{\"message\": {\"content\": \"Hello!\"}}]}\n\n            response = client.get(\"/models_controller/test_local_llm\")\n\n            assert response.status_code == 200"}
{"type": "test_file", "path": "tests/integration_tests/vector_embeddings/test_pinecone.py", "content": "import unittest\nfrom superagi.vector_embeddings.pinecone import Pinecone  \n\n\nclass TestPinecone(unittest.TestCase):\n\n    def setUp(self):\n        self.uuid = [\"id1\", \"id2\"]\n        self.embeds = [\"embed1\", \"embed2\"]\n        self.metadata = [\"metadata1\", \"metadata2\"]\n        self.pinecone_instance = Pinecone(self.uuid, self.embeds, self.metadata)\n\n    def test_init(self):\n        self.assertEqual(self.pinecone_instance.uuid, self.uuid)\n        self.assertEqual(self.pinecone_instance.embeds, self.embeds)\n        self.assertEqual(self.pinecone_instance.metadata, self.metadata)\n    \n    def test_get_vector_embeddings_from_chunks(self):\n        expected = {\n            'vectors': list(zip(self.uuid, self.embeds, self.metadata))\n        }\n        result = self.pinecone_instance.get_vector_embeddings_from_chunks()\n        self.assertEqual(result, expected)\n\n\nif __name__ == \"__main__\":\n    unittest.main()"}
{"type": "source_file", "path": "migrations/versions/598cfb37292a_adding_agent_templates.py", "content": "\"\"\"adding agent templates\n\nRevision ID: 598cfb37292a\nRevises: 2f97c068fab9\nCreate Date: 2023-06-05 12:44:30.982492\n\n\"\"\"\nfrom alembic import op\nimport sqlalchemy as sa\nfrom sqlalchemy.engine import Inspector\n\n# revision identifiers, used by Alembic.\nrevision = '598cfb37292a'\ndown_revision = '2cc1179834b0'\nbranch_labels = None\ndepends_on = None\n\ndef upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.create_table('agent_template_steps',\n                    sa.Column('created_at', sa.DateTime(), nullable=True),\n                    sa.Column('updated_at', sa.DateTime(), nullable=True),\n                    sa.Column('id', sa.Integer(), nullable=False),\n                    sa.Column('agent_template_id', sa.Integer(), nullable=True),\n                    sa.Column('unique_id', sa.String(), nullable=True),\n                    sa.Column('prompt', sa.Text(), nullable=True),\n                    sa.Column('variables', sa.Text(), nullable=True),\n                    sa.Column('output_type', sa.String(), nullable=True),\n                    sa.Column('step_type', sa.String(), nullable=True),\n                    sa.Column('next_step_id', sa.Integer(), nullable=True),\n                    sa.Column('history_enabled', sa.Boolean(), nullable=True),\n                    sa.Column('completion_prompt', sa.Text(), nullable=True),\n                    sa.PrimaryKeyConstraint('id')\n                    )\n\n    op.create_table('agent_templates',\n                    sa.Column('created_at', sa.DateTime(), nullable=True),\n                    sa.Column('updated_at', sa.DateTime(), nullable=True),\n                    sa.Column('id', sa.Integer(), nullable=False),\n                    sa.Column('name', sa.String(), nullable=True),\n                    sa.Column('description', sa.Text(), nullable=True),\n                    sa.PrimaryKeyConstraint('id')\n                    )\n\n    op.add_column('agent_executions', sa.Column('current_step_id', sa.Integer()))\n    op.add_column('agents', sa.Column('agent_template_id', sa.Integer()))\n    op.create_index(\"ix_agents_agnt_template_id\", \"agents\", ['agent_template_id'])\n    op.create_index(\"ix_aea_step_id\", \"agent_executions\", ['current_step_id'])\n\n    op.create_index(\"ix_ats_unique_id\", \"agent_template_steps\", ['unique_id'])\n    op.create_index(\"ix_at_name\", \"agent_templates\", ['name'])\n    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_column('agents', 'agent_template_id')\n    op.add_column('agent_executions', sa.Column('name', sa.VARCHAR(), autoincrement=False, nullable=True))\n    op.drop_column('agent_executions', 'current_step_id')\n    op.drop_table('agent_templates')\n    op.drop_table('agent_template_steps')\n    # ### end Alembic commands ###\n\n\n"}
{"type": "source_file", "path": "superagi/agent/agent_message_builder.py", "content": "import time\nfrom typing import Tuple, List\nfrom sqlalchemy import asc\n\nfrom superagi.config.config import get_config\nfrom superagi.helper.error_handler import ErrorHandler\nfrom superagi.helper.prompt_reader import PromptReader\nfrom superagi.helper.token_counter import TokenCounter\nfrom superagi.models.agent_execution import AgentExecution\nfrom superagi.models.agent_execution_feed import AgentExecutionFeed\nfrom superagi.types.common import BaseMessage\nfrom superagi.models.agent_execution_config import AgentExecutionConfiguration\nfrom superagi.models.agent import Agent\n\n\nclass AgentLlmMessageBuilder:\n    \"\"\"Agent message builder for LLM agent.\"\"\"\n    def __init__(self, session, llm, llm_model: str, agent_id: int, agent_execution_id: int):\n        self.session = session\n        self.llm = llm\n        self.llm_model = llm_model\n        self.agent_id = agent_id\n        self.agent_execution_id = agent_execution_id\n        self.organisation = Agent.find_org_by_agent_id(self.session, self.agent_id)\n\n    def build_agent_messages(self, prompt: str, agent_feeds: list, history_enabled=False,\n                             completion_prompt: str = None):\n        \"\"\" Build agent messages for LLM agent.\n\n        Args:\n            prompt (str): The prompt to be used for generating the agent messages.\n            agent_feeds (list): The list of agent feeds.\n            history_enabled (bool): Whether to use history or not.\n            completion_prompt (str): The completion prompt to be used for generating the agent messages.\n        \"\"\"\n        token_limit = TokenCounter(session=self.session, organisation_id=self.organisation.id).token_limit(self.llm_model)\n        max_output_token_limit = int(get_config(\"MAX_TOOL_TOKEN_LIMIT\", 800))\n        messages = [{\"role\": \"system\", \"content\": prompt}]\n        if history_enabled:\n            messages.append({\"role\": \"system\", \"content\": f\"The current time and date is {time.strftime('%c')}\"})\n            base_token_limit = TokenCounter.count_message_tokens(messages, self.llm_model)\n            full_message_history = [{'role': agent_feed.role, 'content': agent_feed.feed, 'chat_id': agent_feed.id}\n                                                for agent_feed in agent_feeds]\n            past_messages, current_messages = self._split_history(full_message_history,\n                                                              ((token_limit - base_token_limit - max_output_token_limit) // 4) * 3)\n            if past_messages:\n                ltm_summary = self._build_ltm_summary(past_messages=past_messages,\n                                                                   output_token_limit=(token_limit - base_token_limit - max_output_token_limit) // 4)\n                messages.append({\"role\": \"assistant\", \"content\": ltm_summary})\n\n            for history in current_messages:\n                messages.append({\"role\": history[\"role\"], \"content\": history[\"content\"]})\n            messages.append({\"role\": \"user\", \"content\": completion_prompt})\n\n        # insert initial agent feeds\n        self._add_initial_feeds(agent_feeds, messages)\n        return messages\n\n    def _split_history(self, history: List, pending_token_limit: int) -> Tuple[List[BaseMessage], List[BaseMessage]]:\n        hist_token_count = 0\n        i = len(history)\n        for message in reversed(history):\n            token_count = TokenCounter.count_message_tokens([{\"role\": message[\"role\"], \"content\": message[\"content\"]}],\n                                                            self.llm_model)\n            hist_token_count += token_count\n            if hist_token_count > pending_token_limit:\n                self._add_or_update_last_agent_feed_ltm_summary_id(str(history[i-1]['chat_id']))\n                return history[:i], history[i:]\n            i -= 1\n        return [], history\n\n    def _add_initial_feeds(self, agent_feeds: list, messages: list):\n        if agent_feeds:\n            return\n        for message in messages:\n            agent_execution_feed = AgentExecutionFeed(agent_execution_id=self.agent_execution_id,\n                                                      agent_id=self.agent_id,\n                                                      feed=message[\"content\"],\n                                                      role=message[\"role\"],\n                                                      feed_group_id=\"DEFAULT\")\n            self.session.add(agent_execution_feed)\n            self.session.commit()\n\n    def _add_or_update_last_agent_feed_ltm_summary_id(self, last_agent_feed_ltm_summary_id):\n        execution = AgentExecution(id=self.agent_execution_id)\n        agent_execution_configs = {\"last_agent_feed_ltm_summary_id\": last_agent_feed_ltm_summary_id}\n        AgentExecutionConfiguration.add_or_update_agent_execution_config(self.session, execution,\n                                                                         agent_execution_configs)\n\n\n    def _build_ltm_summary(self, past_messages, output_token_limit) -> str:\n        ltm_prompt = self._build_prompt_for_ltm_summary(past_messages=past_messages,\n                                                        token_limit=output_token_limit)\n\n        summary = AgentExecutionConfiguration.fetch_value(self.session, self.agent_execution_id, \"ltm_summary\")\n        previous_ltm_summary = summary.value if summary is not None else \"\"\n\n        ltm_summary_base_token_limit = 10\n        if ((TokenCounter.count_text_tokens(ltm_prompt) + ltm_summary_base_token_limit + output_token_limit)\n            - TokenCounter(session=self.session, organisation_id=self.organisation.id).token_limit(self.llm_model)) > 0:\n            last_agent_feed_ltm_summary_id = AgentExecutionConfiguration.fetch_value(self.session,\n                                                       self.agent_execution_id, \"last_agent_feed_ltm_summary_id\")\n            last_agent_feed_ltm_summary_id = (\n                int(last_agent_feed_ltm_summary_id.value)\n                if last_agent_feed_ltm_summary_id is not None and last_agent_feed_ltm_summary_id.value is not None\n                else 0\n            )\n            past_messages = self.session.query(AgentExecutionFeed.role, AgentExecutionFeed.feed,\n                                               AgentExecutionFeed.id) \\\n                .filter(AgentExecutionFeed.agent_execution_id == self.agent_execution_id,\n                        AgentExecutionFeed.id > last_agent_feed_ltm_summary_id) \\\n                .order_by(asc(AgentExecutionFeed.created_at)) \\\n                .all()\n\n            past_messages = [\n                {'role': past_message.role, 'content': past_message.feed, 'chat_id': past_message.id}\n                for past_message in past_messages]\n\n            ltm_prompt = self._build_prompt_for_recursive_ltm_summary_using_previous_ltm_summary(\n                previous_ltm_summary=previous_ltm_summary, past_messages=past_messages, token_limit=output_token_limit)\n\n        msgs = [{\"role\": \"system\", \"content\": \"You are GPT Prompt writer\"},\n                {\"role\": \"assistant\", \"content\": ltm_prompt}]\n        ltm_summary = self.llm.chat_completion(msgs)\n\n        if 'error' in ltm_summary and ltm_summary['message'] is not None:\n            ErrorHandler.handle_openai_errors(self.session, self.agent_id, self.agent_execution_id, ltm_summary['message'])\n\n        execution = AgentExecution(id=self.agent_execution_id)\n        agent_execution_configs = {\"ltm_summary\": ltm_summary[\"content\"]}\n        AgentExecutionConfiguration.add_or_update_agent_execution_config(session=self.session, execution=execution,\n                                                                 agent_execution_configs=agent_execution_configs)\n\n        return ltm_summary[\"content\"]\n\n    def _build_prompt_for_ltm_summary(self, past_messages: List[BaseMessage], token_limit: int):\n        ltm_summary_prompt = PromptReader.read_agent_prompt(__file__, \"agent_summary.txt\")\n\n        past_messages_prompt = \"\"\n        for past_message in past_messages:\n            past_messages_prompt += past_message[\"role\"] + \": \" + past_message[\"content\"] + \"\\n\"\n        ltm_summary_prompt = ltm_summary_prompt.replace(\"{past_messages}\", past_messages_prompt)\n\n        ltm_summary_prompt = ltm_summary_prompt.replace(\"{char_limit}\", str(token_limit*4))\n\n        return ltm_summary_prompt\n\n    def _build_prompt_for_recursive_ltm_summary_using_previous_ltm_summary(self, previous_ltm_summary: str,\n                                                                    past_messages: List[BaseMessage], token_limit: int):\n        ltm_summary_prompt = PromptReader.read_agent_prompt(__file__, \"agent_recursive_summary.txt\")\n\n        ltm_summary_prompt = ltm_summary_prompt.replace(\"{previous_ltm_summary}\", previous_ltm_summary)\n\n        past_messages_prompt = \"\"\n        for past_message in past_messages:\n            past_messages_prompt += past_message[\"role\"] + \": \" + past_message[\"content\"] + \"\\n\"\n        ltm_summary_prompt = ltm_summary_prompt.replace(\"{past_messages}\", past_messages_prompt)\n\n        ltm_summary_prompt = ltm_summary_prompt.replace(\"{char_limit}\", str(token_limit*4))\n\n        return ltm_summary_prompt\n"}
{"type": "source_file", "path": "migrations/versions/40affbf3022b_add_filter_colume_in_webhooks.py", "content": "\"\"\"add filter colume in webhooks\n\nRevision ID: 40affbf3022b\nRevises: 5d5f801f28e7\nCreate Date: 2023-08-28 12:30:35.171176\n\n\"\"\"\nfrom alembic import op\nimport sqlalchemy as sa\n\n\n# revision identifiers, used by Alembic.\nrevision = '40affbf3022b'\ndown_revision = '5d5f801f28e7'\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.add_column('webhooks', sa.Column('filters', sa.JSON(), nullable=True))\n    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_column('webhooks', 'filters')\n    # ### end Alembic commands ###\n"}
{"type": "source_file", "path": "superagi/agent/agent_prompt_template.py", "content": "import re\n\nfrom pydantic.types import List\n\nfrom superagi.helper.prompt_reader import PromptReader\n\nFINISH_NAME = \"finish\"\n\n\nclass AgentPromptTemplate:\n\n    @staticmethod\n    def add_list_items_to_string(items: List[str]) -> str:\n        list_string = \"\"\n        for i, item in enumerate(items):\n            list_string += f\"{i + 1}. {item}\\n\"\n        return list_string\n\n    @classmethod\n    def clean_prompt(cls, prompt):\n        prompt = re.sub('[ \\t]+', ' ', prompt)\n        return prompt.strip()\n\n    @classmethod\n    def get_super_agi_single_prompt(cls):\n        super_agi_prompt = PromptReader.read_agent_prompt(__file__, \"superagi.txt\")\n\n        return {\"prompt\": super_agi_prompt, \"variables\": [\"goals\", \"instructions\", \"constraints\", \"tools\"]}\n\n    @classmethod\n    def start_task_based(cls):\n        super_agi_prompt = PromptReader.read_agent_prompt(__file__, \"initialize_tasks.txt\")\n\n        return {\"prompt\": AgentPromptTemplate.clean_prompt(super_agi_prompt), \"variables\": [\"goals\", \"instructions\"]}\n        # super_agi_prompt = super_agi_prompt.replace(\"{goals}\", AgentPromptBuilder.add_list_items_to_string(goals))\n\n    @classmethod\n    def analyse_task(cls):\n        constraints = [\n            'Exclusively use the tools listed in double quotes e.g. \"tool name\"'\n        ]\n        super_agi_prompt = PromptReader.read_agent_prompt(__file__, \"analyse_task.txt\")\n        super_agi_prompt = AgentPromptTemplate.clean_prompt(super_agi_prompt) \\\n            .replace(\"{constraints}\", AgentPromptTemplate.add_list_items_to_string(constraints))\n        return {\"prompt\": super_agi_prompt, \"variables\": [\"goals\", \"instructions\", \"tools\", \"current_task\"]}\n\n    @classmethod\n    def create_tasks(cls):\n        # just executed task `{last_task}` and got the result `{last_task_result}`\n        super_agi_prompt = PromptReader.read_agent_prompt(__file__, \"create_tasks.txt\")\n        return {\"prompt\": AgentPromptTemplate.clean_prompt(super_agi_prompt),\n                \"variables\": [\"goals\", \"instructions\", \"last_task\", \"last_task_result\", \"pending_tasks\"]}\n\n    @classmethod\n    def prioritize_tasks(cls):\n        # just executed task `{last_task}` and got the result `{last_task_result}`\n        super_agi_prompt = PromptReader.read_agent_prompt(__file__, \"prioritize_tasks.txt\")\n        return {\"prompt\": AgentPromptTemplate.clean_prompt(super_agi_prompt),\n                \"variables\": [\"goals\", \"instructions\", \"last_task\", \"last_task_result\", \"pending_tasks\"]}\n"}
{"type": "source_file", "path": "migrations/versions/be1d922bf2ad_create_call_logs_table.py", "content": "\"\"\"create call logs table\n\nRevision ID: be1d922bf2ad\nRevises: 2fbd6472112c\nCreate Date: 2023-08-08 08:42:37.148178\n\n\"\"\"\nfrom alembic import op\nimport sqlalchemy as sa\n\n\n# revision identifiers, used by Alembic.\nrevision = 'be1d922bf2ad'\ndown_revision = '520aa6776347'\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.create_table('call_logs',\n    sa.Column('id', sa.Integer(), nullable=False),\n    sa.Column('agent_execution_name', sa.String(), nullable=False),\n    sa.Column('agent_id', sa.Integer(), nullable=False),\n    sa.Column('tokens_consumed', sa.Integer(), nullable=False),\n    sa.Column('tool_used', sa.String(), nullable=False),\n    sa.Column('model', sa.String(), nullable=True),\n    sa.Column('org_id', sa.Integer(), nullable=False),\n    sa.Column('created_at', sa.DateTime(), nullable=True),\n    sa.Column('updated_at', sa.DateTime(), nullable=True),\n    sa.PrimaryKeyConstraint('id')\n    )\n    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_table('call_logs')\n    # ### end Alembic commands ###\n"}
{"type": "source_file", "path": "superagi/agent/agent_iteration_step_handler.py", "content": "from datetime import datetime\nimport json\nfrom sqlalchemy import asc\nfrom sqlalchemy.sql.operators import and_\nimport logging\nimport superagi\nfrom superagi.agent.agent_message_builder import AgentLlmMessageBuilder\nfrom superagi.agent.agent_prompt_builder import AgentPromptBuilder\nfrom superagi.agent.output_handler import ToolOutputHandler, get_output_handler\nfrom superagi.agent.task_queue import TaskQueue\nfrom superagi.agent.tool_builder import ToolBuilder\nfrom superagi.apm.event_handler import EventHandler\nfrom superagi.config.config import get_config\nfrom superagi.helper.error_handler import ErrorHandler\nfrom superagi.helper.token_counter import TokenCounter\nfrom superagi.lib.logger import logger\nfrom superagi.models.agent import Agent\nfrom superagi.models.agent_config import AgentConfiguration\nfrom superagi.models.agent_execution import AgentExecution\nfrom superagi.models.agent_execution_config import AgentExecutionConfiguration\nfrom superagi.models.agent_execution_feed import AgentExecutionFeed\nfrom superagi.models.agent_execution_permission import AgentExecutionPermission\nfrom superagi.models.organisation import Organisation\nfrom superagi.models.tool import Tool\nfrom superagi.models.workflows.agent_workflow import AgentWorkflow\nfrom superagi.models.workflows.agent_workflow_step import AgentWorkflowStep\nfrom superagi.models.workflows.iteration_workflow import IterationWorkflow\nfrom superagi.models.workflows.iteration_workflow_step import IterationWorkflowStep\nfrom superagi.resource_manager.resource_summary import ResourceSummarizer\nfrom superagi.tools.resource.query_resource import QueryResourceTool\nfrom superagi.tools.thinking.tools import ThinkingTool\nfrom superagi.apm.call_log_helper import CallLogHelper\n\n\nclass AgentIterationStepHandler:\n    \"\"\" Handles iteration workflow steps in the agent workflow.\"\"\"\n    def __init__(self, session, llm, agent_id: int, agent_execution_id: int, memory=None):\n        self.session = session\n        self.llm = llm\n        self.agent_execution_id = agent_execution_id\n        self.agent_id = agent_id\n        self.memory = memory\n        self.organisation = Agent.find_org_by_agent_id(self.session, agent_id=self.agent_id)\n        self.task_queue = TaskQueue(str(self.agent_execution_id))\n\n    def execute_step(self):\n        agent_config = Agent.fetch_configuration(self.session, self.agent_id)\n        execution = AgentExecution.get_agent_execution_from_id(self.session, self.agent_execution_id)\n        iteration_workflow_step = IterationWorkflowStep.find_by_id(self.session, execution.iteration_workflow_step_id)\n        agent_execution_config = AgentExecutionConfiguration.fetch_configuration(self.session, self.agent_execution_id)\n        if not self._handle_wait_for_permission(execution, agent_config, agent_execution_config,\n                                                iteration_workflow_step):\n            return\n\n        workflow_step = AgentWorkflowStep.find_by_id(self.session, execution.current_agent_step_id)\n        organisation = Agent.find_org_by_agent_id(self.session, agent_id=self.agent_id)\n        iteration_workflow = IterationWorkflow.find_by_id(self.session, workflow_step.action_reference_id)\n        agent_feeds = AgentExecutionFeed.fetch_agent_execution_feeds(self.session, self.agent_execution_id)\n        if not agent_feeds:\n            self.task_queue.clear_tasks()\n\n        agent_tools = self._build_tools(agent_config, agent_execution_config)\n        prompt = self._build_agent_prompt(iteration_workflow=iteration_workflow,\n                                          agent_config=agent_config,\n                                          agent_execution_config=agent_execution_config,\n                                          prompt=iteration_workflow_step.prompt,\n                                          agent_tools=agent_tools)\n\n        messages = AgentLlmMessageBuilder(self.session, self.llm, self.llm.get_model(), self.agent_id, self.agent_execution_id) \\\n            .build_agent_messages(prompt, agent_feeds, history_enabled=iteration_workflow_step.history_enabled,\n                                  completion_prompt=iteration_workflow_step.completion_prompt)\n\n        logger.debug(\"Prompt messages:\", messages)\n        current_tokens = TokenCounter.count_message_tokens(messages = messages, model = self.llm.get_model())\n        response = self.llm.chat_completion(messages, TokenCounter(session=self.session, organisation_id=organisation.id).token_limit(self.llm.get_model()) - current_tokens)\n\n        if 'error' in response and response['message'] is not None:\n            ErrorHandler.handle_openai_errors(self.session, self.agent_id, self.agent_execution_id, response['message'])\n            \n        if 'content' not in response or response['content'] is None:\n            raise RuntimeError(f\"Failed to get response from llm\")\n\n        total_tokens = current_tokens + TokenCounter.count_message_tokens(response['content'], self.llm.get_model())\n        AgentExecution.update_tokens(self.session, self.agent_execution_id, total_tokens)\n        try:\n            content = json.loads(response['content'])\n            tool = content.get('tool', {})\n            tool_name = tool.get('name', '') if tool else ''\n        except json.JSONDecodeError:\n            print(\"Decoding JSON has failed\")\n            tool_name = ''\n\n        CallLogHelper(session=self.session, organisation_id=organisation.id).create_call_log(execution.name,\n                                                                                             agent_config['agent_id'], total_tokens, tool_name, agent_config['model'])\n\n        assistant_reply = response['content']\n        output_handler = get_output_handler(iteration_workflow_step.output_type,\n                                            agent_execution_id=self.agent_execution_id,\n                                            agent_config=agent_config,memory=self.memory, agent_tools=agent_tools)\n        response = output_handler.handle(self.session, assistant_reply)\n        if response.status == \"COMPLETE\":\n            execution.status = \"COMPLETED\"\n            self.session.commit()\n\n            self._update_agent_execution_next_step(execution, iteration_workflow_step.next_step_id, \"COMPLETE\")\n            EventHandler(session=self.session).create_event('run_completed',\n                                                            {'agent_execution_id': execution.id,\n                                                             'name': execution.name,\n                                                             'tokens_consumed': execution.num_of_tokens,\n                                                             \"calls\": execution.num_of_calls},\n                                                            execution.agent_id, organisation.id)\n        elif response.status == \"WAITING_FOR_PERMISSION\":\n            execution.status = \"WAITING_FOR_PERMISSION\"\n            execution.permission_id = response.permission_id\n            self.session.commit()\n        else:\n            # moving to next step of iteration or workflow\n            self._update_agent_execution_next_step(execution, iteration_workflow_step.next_step_id)\n            logger.info(f\"Starting next job for agent execution id: {self.agent_execution_id}\")\n\n        self.session.flush()\n\n    def _update_agent_execution_next_step(self, execution, next_step_id, step_response: str = \"default\"):\n        if next_step_id == -1:\n            next_step = AgentWorkflowStep.fetch_next_step(self.session, execution.current_agent_step_id, step_response)\n            if str(next_step) == \"COMPLETE\":\n                execution.current_agent_step_id = -1\n                execution.status = \"COMPLETED\"\n            else:\n                AgentExecution.assign_next_step_id(self.session, self.agent_execution_id, next_step.id)\n        else:\n            execution.iteration_workflow_step_id = next_step_id\n        self.session.commit()\n\n    def _build_agent_prompt(self, iteration_workflow: IterationWorkflow, agent_config: dict,\n                            agent_execution_config: dict,\n                            prompt: str, agent_tools: list):\n        max_token_limit = int(get_config(\"MAX_TOOL_TOKEN_LIMIT\", 600))\n        prompt = AgentPromptBuilder.replace_main_variables(prompt, agent_execution_config[\"goal\"],\n                                                           agent_execution_config[\"instruction\"],\n                                                           agent_config[\"constraints\"], agent_tools,\n                                                           (not iteration_workflow.has_task_queue))\n        if iteration_workflow.has_task_queue:\n            response = self.task_queue.get_last_task_details()\n            last_task, last_task_result = (response[\"task\"], response[\"response\"]) if response is not None else (\"\", \"\")\n            current_task = self.task_queue.get_first_task() or \"\"\n            token_limit = TokenCounter(session=self.session, organisation_id=self.organisation.id).token_limit() - max_token_limit\n            prompt = AgentPromptBuilder.replace_task_based_variables(prompt, current_task, last_task, last_task_result,\n                                                                     self.task_queue.get_tasks(),\n                                                                     self.task_queue.get_completed_tasks(), token_limit)\n        return prompt\n\n    def _build_tools(self, agent_config: dict, agent_execution_config: dict):\n        agent_tools = [ThinkingTool()]\n\n        config_data = AgentConfiguration.get_model_api_key(self.session, self.agent_id, agent_config[\"model\"])\n        model_api_key = config_data['api_key']\n        tool_builder = ToolBuilder(self.session, self.agent_id, self.agent_execution_id)\n        resource_summary = ResourceSummarizer(session=self.session, agent_id=self.agent_id, model=agent_config['model']).fetch_or_create_agent_resource_summary(default_summary=agent_config.get(\"resource_summary\"))\n        if resource_summary is not None:\n            agent_tools.append(QueryResourceTool())\n        user_tools = self.session.query(Tool).filter(\n            and_(Tool.id.in_(agent_execution_config[\"tools\"]), Tool.file_name is not None)).all()\n        for tool in user_tools:\n            agent_tools.append(tool_builder.build_tool(tool))\n        agent_tools = [tool_builder.set_default_params_tool(tool, agent_config, agent_execution_config,\n                                                            model_api_key, resource_summary,self.memory) for tool in agent_tools]\n        return agent_tools\n\n    def _handle_wait_for_permission(self, agent_execution, agent_config: dict, agent_execution_config: dict,\n                                    iteration_workflow_step: IterationWorkflowStep):\n        \"\"\"\n        Handles the wait for permission when the agent execution is waiting for permission.\n\n        Args:\n            agent_execution (AgentExecution): The agent execution.\n            agent_config (dict): The agent configuration.\n            agent_execution_config (dict): The agent execution configuration.\n            iteration_workflow_step (IterationWorkflowStep): The iteration workflow step.\n\n        Raises:\n            Returns permission success or failure\n        \"\"\"\n        if agent_execution.status != \"WAITING_FOR_PERMISSION\":\n            return True\n        agent_execution_permission = self.session.query(AgentExecutionPermission).filter(\n            AgentExecutionPermission.id == agent_execution.permission_id).first()\n        if agent_execution_permission.status == \"PENDING\":\n            logger.error(\"handle_wait_for_permission: Permission is still pending\")\n            return False\n        if agent_execution_permission.status == \"APPROVED\":\n            agent_tools = self._build_tools(agent_config, agent_execution_config)\n            tool_output_handler = ToolOutputHandler(self.agent_execution_id, agent_config, agent_tools,self.memory)\n            tool_result = tool_output_handler.handle_tool_response(self.session,\n                                                                   agent_execution_permission.assistant_reply)\n            result = tool_result.result\n        else:\n            result = f\"User denied the permission to run the tool {agent_execution_permission.tool_name}\" \\\n                     f\"{' and has given the following feedback : ' + agent_execution_permission.user_feedback if agent_execution_permission.user_feedback else ''}\"\n\n        agent_execution_feed = AgentExecutionFeed(agent_execution_id=agent_execution_permission.agent_execution_id,\n                                                  agent_id=agent_execution_permission.agent_id,\n                                                  feed=agent_execution_permission.assistant_reply,\n                                                  role=\"assistant\",\n                                                  feed_group_id=agent_execution.current_feed_group_id)\n        self.session.add(agent_execution_feed)\n        agent_execution_feed1 = AgentExecutionFeed(agent_execution_id=agent_execution_permission.agent_execution_id,\n                                                  agent_id=agent_execution_permission.agent_id,\n                                                  feed=result, role=\"user\",\n                                                  feed_group_id=agent_execution.current_feed_group_id)\n        self.session.add(agent_execution_feed1)\n        agent_execution.status = \"RUNNING\"\n        execution = AgentExecution.find_by_id(self.session, agent_execution_permission.agent_execution_id)\n        self._update_agent_execution_next_step(execution, iteration_workflow_step.next_step_id)\n        self.session.commit()\n\n\n        return True\n"}
{"type": "source_file", "path": "migrations/versions/7a3e336c0fba_added_tools_related_models.py", "content": "\"\"\"added_tools_related_models\n\nRevision ID: 7a3e336c0fba\nRevises: 516ecc1c723d\nCreate Date: 2023-06-18 11:05:35.801505\n\n\"\"\"\nfrom alembic import op\nimport sqlalchemy as sa\n\n\n# revision identifiers, used by Alembic.\nrevision = '7a3e336c0fba'\ndown_revision = '1d54db311055'\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.create_table('toolkits',\n    sa.Column('created_at', sa.DateTime(), nullable=True),\n    sa.Column('updated_at', sa.DateTime(), nullable=True),\n    sa.Column('id', sa.Integer(), nullable=False),\n    sa.Column('name', sa.String(), nullable=True),\n    sa.Column('description', sa.String(), nullable=True),\n    sa.Column('show_toolkit', sa.Boolean(), nullable=True),\n    sa.Column('organisation_id', sa.Integer(), nullable=True),\n    sa.Column('tool_code_link', sa.String(), nullable=True),\n    sa.PrimaryKeyConstraint('id')\n    )\n\n    op.add_column('tool_configs', sa.Column('toolkit_id', sa.Integer(), nullable=True))\n    op.drop_column('tool_configs', 'name')\n    op.drop_column('tool_configs', 'agent_id')\n    op.add_column('tools', sa.Column('description', sa.String(), nullable=True))\n    op.add_column('tools', sa.Column('toolkit_id', sa.Integer(), nullable=True))\n    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_column('tools', 'toolkit_id')\n    op.drop_column('tools', 'description')\n    op.add_column('tool_configs', sa.Column('agent_id', sa.INTEGER(), autoincrement=False, nullable=True))\n    op.add_column('tool_configs', sa.Column('name', sa.VARCHAR(), autoincrement=False, nullable=True))\n    op.drop_column('tool_configs', 'toolkit_id')\n    op.drop_table('toolkits')\n    # ### end Alembic commands ###\n"}
{"type": "source_file", "path": "migrations/versions/fe234ea6e9bc_modify_agent_workflow_tables.py", "content": "\"\"\"update agent workflow tables\n\nRevision ID: fe234ea6e9bc\nRevises: d8315244ea43\nCreate Date: 2023-07-18 16:46:29.305378\n\n\"\"\"\nfrom alembic import op\nimport sqlalchemy as sa\n\n\n# revision identifiers, used by Alembic.\nrevision = 'fe234ea6e9bc'\ndown_revision = 'd8315244ea43'\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    op.rename_table('agent_workflows', 'iteration_workflows')\n    op.rename_table('agent_workflow_steps', 'iteration_workflow_steps')\n\n    with op.batch_alter_table('iteration_workflow_steps') as bop:\n        bop.alter_column('agent_workflow_id', new_column_name='iteration_workflow_id')\n\n    with op.batch_alter_table('agent_executions') as bop:\n        bop.alter_column('current_step_id', new_column_name='current_agent_step_id')\n\n\n    op.add_column('agent_executions', sa.Column('iteration_workflow_step_id', sa.Integer(), nullable=True))\n    op.add_column('iteration_workflows',\n                  sa.Column('has_task_queue', sa.Boolean(), nullable=True, server_default=sa.false()))\n\n\ndef downgrade() -> None:\n    op.rename_table('iteration_workflows', 'agent_workflows')\n    op.rename_table('iteration_workflow_steps', 'agent_workflow_steps')\n    op.drop_column('agent_executions', 'iteration_workflow_step_id')\n    op.drop_column('agent_workflows', 'has_task_queue')\n"}
{"type": "source_file", "path": "migrations/versions/2f97c068fab9_resource_modified.py", "content": "\"\"\"Resource Modified\n\nRevision ID: 2f97c068fab9\nRevises: a91808a89623\nCreate Date: 2023-06-02 13:13:21.670935\n\n\"\"\"\nfrom alembic import op\nimport sqlalchemy as sa\n\n\n# revision identifiers, used by Alembic.\nrevision = '2f97c068fab9'\ndown_revision = 'a91808a89623'\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.add_column('resources', sa.Column('agent_id', sa.Integer(), nullable=True))\n    op.drop_column('resources', 'project_id')\n    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.add_column('resources', sa.Column('project_id', sa.INTEGER(), autoincrement=False, nullable=True))\n    op.drop_column('resources', 'agent_id')\n    # ### end Alembic commands ###\n"}
{"type": "source_file", "path": "superagi/agent/output_handler.py", "content": "import json\nfrom superagi.agent.common_types import TaskExecutorResponse, ToolExecutorResponse\nfrom superagi.agent.output_parser import AgentSchemaOutputParser\nfrom superagi.agent.task_queue import TaskQueue\nfrom superagi.agent.tool_executor import ToolExecutor\nfrom superagi.helper.json_cleaner import JsonCleaner\nfrom superagi.lib.logger import logger\nfrom langchain.text_splitter import TokenTextSplitter\nfrom superagi.models.agent import Agent\nfrom superagi.models.agent_execution import AgentExecution\nfrom superagi.models.agent_execution_feed import AgentExecutionFeed\nfrom superagi.vector_store.base import VectorStore\nimport numpy as np\n\nfrom superagi.models.agent_execution_permission import AgentExecutionPermission\n\n\nclass ToolOutputHandler:\n    \"\"\"Handles the tool output response from the thinking step\"\"\"\n    def __init__(self,\n                 agent_execution_id: int,\n                 agent_config: dict,\n                 tools: list,\n                 memory:VectorStore=None,\n                 output_parser=AgentSchemaOutputParser()):\n        self.agent_execution_id = agent_execution_id\n        self.task_queue = TaskQueue(str(agent_execution_id))\n        self.agent_config = agent_config\n        self.tools = tools\n        self.output_parser = output_parser\n        self.memory=memory\n\n    def handle(self, session, assistant_reply):\n        \"\"\"Handles the tool output response from the thinking step.\n        Step takes care of permission control as well at tool level.\n\n        Args:\n            session (Session): The database session.\n            assistant_reply (str): The assistant reply.\n        \"\"\"\n        response = self._check_permission_in_restricted_mode(session, assistant_reply)\n        if response.is_permission_required:\n            return response\n\n        tool_response = self.handle_tool_response(session, assistant_reply)\n        # print(tool_response)\n\n        agent_execution = AgentExecution.find_by_id(session, self.agent_execution_id)\n        agent_execution_feed = AgentExecutionFeed(agent_execution_id=self.agent_execution_id,\n                                                  agent_id=self.agent_config[\"agent_id\"],\n                                                  feed=assistant_reply,\n                                                  role=\"assistant\",\n                                                  feed_group_id=agent_execution.current_feed_group_id)\n        session.add(agent_execution_feed)\n        tool_response_feed = AgentExecutionFeed(agent_execution_id=self.agent_execution_id,\n                                                agent_id=self.agent_config[\"agent_id\"],\n                                                feed=tool_response.result,\n                                                role=\"system\",\n                                                feed_group_id=agent_execution.current_feed_group_id)\n        session.add(tool_response_feed)\n        session.commit()\n        if not tool_response.retry:\n            tool_response = self._check_for_completion(tool_response)\n\n        self.add_text_to_memory(assistant_reply, tool_response.result)\n        return tool_response\n\n    def add_text_to_memory(self, assistant_reply,tool_response_result):\n        \"\"\"\n        Adds the text generated by the assistant and tool response to the memory.\n\n        Args:\n            assistant_reply (str): The assistant reply.\n            tool_response_result (str): The tool response.\n\n        Returns:\n            None\n        \"\"\"\n        if self.memory is not None:\n            try:\n                data = json.loads(assistant_reply)\n                task_description = data['thoughts']['text']\n                final_tool_response = tool_response_result\n                prompt = task_description + final_tool_response\n                text_splitter = TokenTextSplitter(chunk_size=1024, chunk_overlap=10)\n                chunk_response = text_splitter.split_text(prompt)\n                metadata = {\"agent_execution_id\": self.agent_execution_id}\n                metadatas = []\n                for _ in chunk_response:\n                    metadatas.append(metadata)\n\n                self.memory.add_texts(chunk_response, metadatas)\n            except Exception as exception:\n                logger.error(f\"Exception: {exception}\")\n        \n     \n\n    def handle_tool_response(self, session, assistant_reply):\n        \"\"\"Only handle processing of tool response\"\"\"\n        action = self.output_parser.parse(assistant_reply)\n        agent = session.query(Agent).filter(Agent.id == self.agent_config[\"agent_id\"]).first()\n        organisation = agent.get_agent_organisation(session)\n        tool_executor = ToolExecutor(organisation_id=organisation.id, agent_id=agent.id, tools=self.tools, agent_execution_id=self.agent_execution_id)\n        return tool_executor.execute(session, action.name, action.args)\n\n    def _check_permission_in_restricted_mode(self, session, assistant_reply: str):\n        action = self.output_parser.parse(assistant_reply)\n        tools = {t.name: t for t in self.tools}\n\n        excluded_tools = [ToolExecutor.FINISH, '', None]\n\n        if self.agent_config[\"permission_type\"].upper() == \"RESTRICTED\" and action.name not in excluded_tools and \\\n                tools.get(action.name) and tools[action.name].permission_required:\n            new_agent_execution_permission = AgentExecutionPermission(\n                agent_execution_id=self.agent_execution_id,\n                status=\"PENDING\",\n                agent_id=self.agent_config[\"agent_id\"],\n                tool_name=action.name,\n                assistant_reply=assistant_reply)\n\n            session.add(new_agent_execution_permission)\n            session.commit()\n            return ToolExecutorResponse(is_permission_required=True, status=\"WAITING_FOR_PERMISSION\",\n                                        permission_id=new_agent_execution_permission.id)\n        return ToolExecutorResponse(status=\"PENDING\", is_permission_required=False)\n\n    def _check_for_completion(self, tool_response):\n        self.task_queue.complete_task(tool_response.result)\n        current_tasks = self.task_queue.get_tasks()\n        if self.task_queue.get_completed_tasks() and len(current_tasks) == 0:\n            tool_response.status = \"COMPLETE\"\n        if current_tasks and tool_response.status == \"COMPLETE\":\n            tool_response.status = \"PENDING\"\n        return tool_response\n\n\nclass TaskOutputHandler:\n    \"\"\"Handles the task output from the LLM. Output is mostly in the array of tasks and\n    handler adds every task to the task queue.\n    \"\"\"\n\n    def __init__(self, agent_execution_id: int, agent_config: dict):\n        self.agent_execution_id = agent_execution_id\n        self.task_queue = TaskQueue(str(agent_execution_id))\n        self.agent_config = agent_config\n\n    def handle(self, session, assistant_reply):\n        assistant_reply = JsonCleaner.extract_json_array_section(assistant_reply)\n        tasks = eval(assistant_reply)\n        tasks = np.array(tasks).flatten().tolist()\n        for task in reversed(tasks):\n            self.task_queue.add_task(task)\n        if len(tasks) > 0:\n            logger.info(\"Adding task to queue: \" + str(tasks))\n        agent_execution = AgentExecution.find_by_id(session, self.agent_execution_id)\n        for task in tasks:\n            agent_execution_feed = AgentExecutionFeed(agent_execution_id=self.agent_execution_id,\n                                                      agent_id=self.agent_config[\"agent_id\"],\n                                                      feed=\"New Task Added: \" + task,\n                                                      role=\"system\",\n                                                      feed_group_id=agent_execution.current_feed_group_id)\n            session.add(agent_execution_feed)\n        status = \"COMPLETE\" if len(self.task_queue.get_tasks()) == 0 else \"PENDING\"\n        session.commit()\n        return TaskExecutorResponse(status=status, retry=False)\n\n\nclass ReplaceTaskOutputHandler:\n    \"\"\"Handles the replace/prioritize task output type.\n    Output is mostly in the array of tasks and handler adds every task to the task queue.\n    \"\"\"\n\n    def __init__(self, agent_execution_id: int, agent_config: dict):\n        self.agent_execution_id = agent_execution_id\n        self.task_queue = TaskQueue(str(agent_execution_id))\n        self.agent_config = agent_config\n\n    def handle(self, session, assistant_reply):\n        assistant_reply = JsonCleaner.extract_json_array_section(assistant_reply)\n        tasks = eval(assistant_reply)\n        self.task_queue.clear_tasks()\n        for task in reversed(tasks):\n            self.task_queue.add_task(task)\n        if len(tasks) > 0:\n            logger.info(\"Tasks reprioritized in order: \" + str(tasks))\n        status = \"COMPLETE\" if len(self.task_queue.get_tasks()) == 0 else \"PENDING\"\n        session.commit()\n        return TaskExecutorResponse(status=status, retry=False)\n\n\ndef get_output_handler(output_type: str, agent_execution_id: int, agent_config: dict, agent_tools: list = [],memory=None):\n    if output_type == \"tools\":\n        return ToolOutputHandler(agent_execution_id, agent_config, agent_tools,memory=memory)\n    elif output_type == \"replace_tasks\":\n        return ReplaceTaskOutputHandler(agent_execution_id, agent_config)\n    elif output_type == \"tasks\":\n        return TaskOutputHandler(agent_execution_id, agent_config)\n    return ToolOutputHandler(agent_execution_id, agent_config, agent_tools,memory=memory)\n"}
{"type": "source_file", "path": "cli2.py", "content": "import os\r\nimport sys\r\nimport subprocess\r\nfrom time import sleep\r\nimport shutil\r\nfrom sys import platform\r\nfrom multiprocessing import Process\r\nfrom superagi.lib.logger import logger\r\n\r\n\r\ndef check_command(command, message):\r\n    if not shutil.which(command):\r\n        logger.info(message)\r\n        sys.exit(1)\r\n\r\n\r\ndef run_npm_commands(shell=False):\r\n    os.chdir(\"gui\")\r\n    try:\r\n        subprocess.run([\"npm\", \"install\"], check=True, shell=shell)\r\n    except subprocess.CalledProcessError:\r\n        logger.error(f\"Error during '{' '.join(sys.exc_info()[1].cmd)}'. Exiting.\")\r\n        sys.exit(1)\r\n    os.chdir(\"..\")\r\n\r\n\r\ndef run_server(shell=False,a_name=None,a_description=None,goals=None):\r\n    tgwui_process = Process(target=subprocess.run, args=([\"python\", \"test.py\",\"--name\",a_name,\"--description\",a_description,\"--goals\"]+goals,), kwargs={\"shell\": shell})\r\n    api_process = Process(target=subprocess.run, args=([\"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"],), kwargs={\"shell\": shell})\r\n    celery_process = Process(target=subprocess.run, args=([\"celery\", \"-A\", \"celery_app\", \"worker\", \"--loglevel=info\"],), kwargs={\"shell\": shell})\r\n    ui_process = Process(target=subprocess.run, args=([\"python\", \"test.py\",\"--name\",a_name,\"--description\",a_description,\"--goals\"]+goals,), kwargs={\"shell\": shell})\r\n    api_process.start()\r\n    celery_process.start()\r\n    ui_process.start()\r\n\r\n    return api_process, ui_process, celery_process\r\n\r\n\r\ndef cleanup(api_process, ui_process, celery_process):\r\n    logger.info(\"Shutting down processes...\")\r\n    api_process.terminate()\r\n    ui_process.terminate()\r\n    celery_process.terminate()\r\n    logger.info(\"Processes terminated. Exiting.\")\r\n    sys.exit(1)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    check_command(\"node\", \"Node.js is not installed. Please install it and try again.\")\r\n    check_command(\"npm\", \"npm is not installed. Please install npm to proceed.\")\r\n    check_command(\"uvicorn\", \"uvicorn is not installed. Please install uvicorn to proceed.\")\r\n\r\n    agent_name = input(\"Enter an agent name: \")\r\n    agent_description = input(\"Enter an agent description: \")\r\n    goals = []\r\n    while True:\r\n        goal = input(\"Enter a goal (or 'q' to quit): \")\r\n        if goal == 'q':\r\n            break\r\n        goals.append(goal)\r\n    isWindows = False\r\n    if platform == \"win32\" or platform == \"cygwin\":\r\n        isWindows = True\r\n    run_npm_commands(shell=isWindows)\r\n\r\n    try:\r\n        api_process, ui_process, celery_process = run_server(isWindows, agent_name, agent_description, goals)\r\n        while True:\r\n            try:\r\n                sleep(30)\r\n            except KeyboardInterrupt:\r\n                cleanup(api_process, ui_process, celery_process)\r\n    except Exception as e:\r\n        cleanup(api_process, ui_process, celery_process)"}
{"type": "source_file", "path": "migrations/versions/c4f2f6ba602a_agent_workflow_wait_step.py", "content": "\"\"\"agent_workflow_wait_step\n\nRevision ID: c4f2f6ba602a\nRevises: 40affbf3022b\nCreate Date: 2023-09-04 05:34:10.195248\n\n\"\"\"\nfrom alembic import op\nimport sqlalchemy as sa\n\n\n# revision identifiers, used by Alembic.\nrevision = 'c4f2f6ba602a'\ndown_revision = '40affbf3022b'\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.create_table('agent_workflow_step_waits',\n    sa.Column('id', sa.Integer(), nullable=False),\n    sa.Column('name', sa.String(), nullable=True),\n    sa.Column('description', sa.String(), nullable=True),\n    sa.Column('unique_id', sa.String(), nullable=True),\n    sa.Column('delay', sa.Integer(), nullable=True),\n    sa.Column('wait_begin_time', sa.DateTime(), nullable=True),\n    sa.Column('status', sa.String(), nullable=True),\n    sa.Column('created_at', sa.DateTime(), nullable=True),\n    sa.Column('updated_at', sa.DateTime(), nullable=True),\n    sa.PrimaryKeyConstraint('id')\n    )\n    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_table('agent_workflow_step_waits')\n    # ### end Alembic commands ###\n"}
{"type": "source_file", "path": "superagi/agent/agent_prompt_builder.py", "content": "import json\nimport re\n\nfrom pydantic.types import List\n\nfrom superagi.helper.token_counter import TokenCounter\nfrom superagi.tools.base_tool import BaseTool\n\nFINISH_NAME = \"finish\"\n\n\nclass AgentPromptBuilder:\n    \"\"\"Agent prompt builder for LLM agent.\"\"\"\n\n    @staticmethod\n    def add_list_items_to_string(items: List[str]) -> str:\n        list_string = \"\"\n        for i, item in enumerate(items):\n            list_string += f\"{i + 1}. {item}\\n\"\n        return list_string\n\n\n    @classmethod\n    def add_tools_to_prompt(cls, tools: List[BaseTool], add_finish: bool = True) -> str:\n        \"\"\"Add tools to the prompt.\n\n        Args:\n            tools (List[BaseTool]): The list of tools.\n            add_finish (bool): Whether to add finish tool or not.\n        \"\"\"\n        final_string = \"\"\n        print(tools)\n        for i, item in enumerate(tools):\n            final_string += f\"{i + 1}. {cls._generate_tool_string(item)}\\n\"\n        finish_description = (\n            \"use this to signal that you have finished all your objectives\"\n        )\n        finish_args = (\n            '\"response\": \"final response to let '\n            'people know you have finished your objectives\"'\n        )\n        finish_string = (\n            f\"{len(tools) + 1}. \\\"{FINISH_NAME}\\\": \"\n            f\"{finish_description}, args: {finish_args}\"\n        )\n        if add_finish:\n            final_string = final_string + finish_string + \"\\n\\n\"\n        else:\n            final_string = final_string + \"\\n\"\n\n        return final_string\n\n    @classmethod\n    def _generate_tool_string(cls, tool: BaseTool) -> str:\n        output = f\"\\\"{tool.name}\\\": {tool.description}\"\n        # print(tool.args)\n        output += f\", args json schema: {json.dumps(tool.args)}\"\n        return output\n    \n    @classmethod\n    def clean_prompt(cls, prompt):\n        prompt = re.sub('[ \\t]+', ' ', prompt)\n        return prompt.strip()\n\n    @classmethod\n    def replace_main_variables(cls, super_agi_prompt: str, goals: List[str], instructions: List[str], constraints: List[str],\n                               tools: List[BaseTool], add_finish_tool: bool = True):\n        \"\"\"Replace the main variables in the super agi prompt.\n\n        Args:\n            super_agi_prompt (str): The super agi prompt.\n            goals (List[str]): The list of goals.\n            instructions (List[str]): The list of instructions.\n            constraints (List[str]): The list of constraints.\n            tools (List[BaseTool]): The list of tools.\n            add_finish_tool (bool): Whether to add finish tool or not.\n        \"\"\"\n        super_agi_prompt = super_agi_prompt.replace(\"{goals}\", AgentPromptBuilder.add_list_items_to_string(goals))\n        if len(instructions) > 0 and len(instructions[0]) > 0:\n            task_str = \"INSTRUCTION(Follow these instruction to decide the flow of execution and decide the next steps for achieving the task):\"\n            super_agi_prompt = super_agi_prompt.replace(\"{instructions}\", \"INSTRUCTION: \" + '\\n' +  AgentPromptBuilder.add_list_items_to_string(instructions))\n            super_agi_prompt = super_agi_prompt.replace(\"{task_instructions}\", task_str + '\\n' +  AgentPromptBuilder.add_list_items_to_string(instructions))\n        else:\n            super_agi_prompt = super_agi_prompt.replace(\"{instructions}\", '')\n        super_agi_prompt = super_agi_prompt.replace(\"{task_instructions}\", \"\")\n        super_agi_prompt = super_agi_prompt.replace(\"{constraints}\",\n                                                    AgentPromptBuilder.add_list_items_to_string(constraints))\n\n\n        # logger.info(tools)\n        tools_string = AgentPromptBuilder.add_tools_to_prompt(tools, add_finish_tool)\n        super_agi_prompt = super_agi_prompt.replace(\"{tools}\", tools_string)\n        return super_agi_prompt\n\n    @classmethod\n    def replace_task_based_variables(cls, super_agi_prompt: str, current_task: str, last_task: str,\n                                     last_task_result: str, pending_tasks: List[str], completed_tasks: list, token_limit: int):\n        \"\"\"Replace the task based variables in the super agi prompt.\n\n        Args:\n            super_agi_prompt (str): The super agi prompt.\n            current_task (str): The current task.\n            last_task (str): The last task.\n            last_task_result (str): The last task result.\n            pending_tasks (List[str]): The list of pending tasks.\n            completed_tasks (list): The list of completed tasks.\n            token_limit (int): The token limit.\n        \"\"\"\n        if \"{current_task}\" in super_agi_prompt:\n            super_agi_prompt = super_agi_prompt.replace(\"{current_task}\", current_task)\n        if \"{last_task}\" in super_agi_prompt:\n            super_agi_prompt = super_agi_prompt.replace(\"{last_task}\", last_task)\n        if \"{last_task_result}\" in super_agi_prompt:\n            super_agi_prompt = super_agi_prompt.replace(\"{last_task_result}\", last_task_result)\n        if \"{pending_tasks}\" in super_agi_prompt:\n            super_agi_prompt = super_agi_prompt.replace(\"{pending_tasks}\", str(pending_tasks))\n\n        completed_tasks.reverse()\n        if \"{completed_tasks}\" in super_agi_prompt:\n            completed_tasks_arr = []\n            for task in completed_tasks:\n                completed_tasks_arr.append(task['task'])\n            super_agi_prompt = super_agi_prompt.replace(\"{completed_tasks}\", str(completed_tasks_arr))\n\n        base_token_limit = TokenCounter.count_message_tokens([{\"role\": \"user\", \"content\": super_agi_prompt}])\n        pending_tokens = token_limit - base_token_limit\n        final_output = \"\"\n        if \"{task_history}\" in super_agi_prompt:\n            for task in reversed(completed_tasks[-10:]):\n                final_output = f\"Task: {task['task']}\\nResult: {task['response']}\\n\" + final_output\n                token_count = TokenCounter.count_message_tokens([{\"role\": \"user\", \"content\": final_output}])\n                # giving buffer of 100 tokens\n                if token_count > min(600, pending_tokens):\n                    break\n            super_agi_prompt = super_agi_prompt.replace(\"{task_history}\", \"\\n\" + final_output + \"\\n\")\n        return super_agi_prompt\n"}
{"type": "source_file", "path": "migrations/versions/1d54db311055_add_permissions.py", "content": "\"\"\"add permissions\n\nRevision ID: 1d54db311055\nRevises: 3356a2f89a33\nCreate Date: 2023-06-14 11:05:59.678961\n\n\"\"\"\nfrom alembic import op\nimport sqlalchemy as sa\n\n\n# revision identifiers, used by Alembic.\nrevision = '1d54db311055'\ndown_revision = '516ecc1c723d'\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.create_table('agent_execution_permissions',\n    sa.Column('created_at', sa.DateTime(), nullable=True),\n    sa.Column('updated_at', sa.DateTime(), nullable=True),\n    sa.Column('id', sa.Integer(), nullable=False),\n    sa.Column('agent_execution_id', sa.Integer(), nullable=True),\n    sa.Column('agent_id', sa.Integer(), nullable=True),\n    sa.Column('status', sa.String(), nullable=True),\n    sa.Column('tool_name', sa.String(), nullable=True),\n    sa.Column('user_feedback', sa.Text(), nullable=True),\n    sa.Column('assistant_reply', sa.Text(), nullable=True),\n    sa.PrimaryKeyConstraint('id')\n    )\n    op.add_column('agent_executions', sa.Column('permission_id', sa.Integer(), nullable=True))\n    # index on agent_execution_id\n    op.create_index(op.f('ix_agent_execution_permissions_agent_execution_id')\n                    , 'agent_execution_permissions', ['agent_execution_id'], unique=False)\n\n    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_column('agent_executions', 'permission_id')\n    op.drop_table('agent_execution_permissions')\n    # ### end Alembic commands ###\n"}
{"type": "source_file", "path": "migrations/versions/3356a2f89a33_added_configurations_table.py", "content": "\"\"\"added_configurations_table\n\nRevision ID: 3356a2f89a33\nRevises: 35e47f20475b\nCreate Date: 2023-06-06 10:51:15.111738\n\n\"\"\"\nfrom alembic import op\nimport sqlalchemy as sa\n\n\n# revision identifiers, used by Alembic.\nrevision = '3356a2f89a33'\ndown_revision = '35e47f20475b'\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.create_table('configurations',\n    sa.Column('created_at', sa.DateTime(), nullable=True),\n    sa.Column('updated_at', sa.DateTime(), nullable=True),\n    sa.Column('id', sa.Integer(), autoincrement=True, nullable=False),\n    sa.Column('organisation_id', sa.Integer(), nullable=True),\n    sa.Column('key', sa.String(), nullable=True),\n    sa.Column('value', sa.Text(), nullable=True),\n    sa.PrimaryKeyConstraint('id')\n    )\n    op.drop_index('ix_aea_step_id', table_name='agent_executions')\n    op.drop_index('ix_ats_unique_id', table_name='agent_template_steps')\n    op.drop_index('ix_at_name', table_name='agent_templates')\n    op.drop_index('ix_agents_agnt_template_id', table_name='agents')\n    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.create_index('ix_agents_agnt_template_id', 'agents', ['agent_template_id'], unique=False)\n    op.create_index('ix_at_name', 'agent_templates', ['name'], unique=False)\n    op.create_index('ix_ats_unique_id', 'agent_template_steps', ['unique_id'], unique=False)\n    op.create_index('ix_aea_step_id', 'agent_executions', ['current_step_id'], unique=False)\n    op.drop_table('configurations')\n    # ### end Alembic commands ###\n"}
{"type": "source_file", "path": "migrations/versions/8962bed0d809_creating_agent_templates.py", "content": "\"\"\"creating agent templates\n\nRevision ID: 8962bed0d809\nRevises: d9b3436197eb\nCreate Date: 2023-06-10 15:40:08.942612\n\n\"\"\"\nfrom alembic import op\nimport sqlalchemy as sa\n\n\n# revision identifiers, used by Alembic.\nrevision = '8962bed0d809'\ndown_revision = 'd9b3436197eb'\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    op.create_table('agent_templates',\n                    sa.Column('created_at', sa.DateTime(), nullable=True),\n                    sa.Column('updated_at', sa.DateTime(), nullable=True),\n                    sa.Column('id', sa.Integer(), nullable=False),\n                    sa.Column('name', sa.String(), nullable=True),\n                    sa.Column('description', sa.Text(), nullable=True),\n                    sa.Column('organisation_id', sa.Integer(), nullable=True),\n                    sa.Column('agent_workflow_id', sa.Integer(), nullable=True),\n                    sa.PrimaryKeyConstraint('id')\n                    )\n\n    op.create_table('agent_template_configs',\n                    sa.Column('created_at', sa.DateTime(), nullable=True),\n                    sa.Column('updated_at', sa.DateTime(), nullable=True),\n                    sa.Column('id', sa.Integer(), nullable=False),\n                    sa.Column('agent_template_id', sa.Integer(), nullable=True),\n                    sa.Column('key', sa.String(), nullable=True),\n                    sa.Column('value', sa.Text(), nullable=True),\n                    sa.PrimaryKeyConstraint('id')\n                    )\n    op.create_index(\"ix_atc_agnt_template_id_key\", \"agent_template_configs\", ['agent_template_id', 'key'])\n    op.create_index(\"ix_agt_agnt_organisation_id\", \"agent_templates\", ['organisation_id'])\n    op.create_index(\"ix_agt_agnt_workflow_id\", \"agent_templates\", ['agent_workflow_id'])\n    op.create_index(\"ix_agt_agnt_name\", \"agent_templates\", ['name'])\n\n\ndef downgrade() -> None:\n    op.drop_table('agent_template_configs')\n    op.drop_table('agent_templates')\n"}
{"type": "source_file", "path": "superagi/agent/queue_step_handler.py", "content": "import time\n\nimport numpy as np\n\nfrom superagi.agent.agent_message_builder import AgentLlmMessageBuilder\nfrom superagi.agent.task_queue import TaskQueue\nfrom superagi.helper.error_handler import ErrorHandler\nfrom superagi.helper.json_cleaner import JsonCleaner\nfrom superagi.helper.prompt_reader import PromptReader\nfrom superagi.helper.token_counter import TokenCounter\nfrom superagi.lib.logger import logger\nfrom superagi.models.agent_execution import AgentExecution\nfrom superagi.models.agent_execution_feed import AgentExecutionFeed\nfrom superagi.models.workflows.agent_workflow_step import AgentWorkflowStep\nfrom superagi.models.workflows.agent_workflow_step_tool import AgentWorkflowStepTool\nfrom superagi.models.agent import Agent\nfrom superagi.types.queue_status import QueueStatus\n\n\nclass QueueStepHandler:\n    \"\"\"Handles the queue step of the agent workflow\"\"\"\n    def __init__(self, session, llm, agent_id: int, agent_execution_id: int):\n        self.session = session\n        self.llm = llm\n        self.agent_execution_id = agent_execution_id\n        self.agent_id = agent_id\n        self.organisation = Agent.find_org_by_agent_id(self.session, agent_id=self.agent_id)\n\n    def _queue_identifier(self, step_tool):\n        return step_tool.unique_id + \"_\" + str(self.agent_execution_id)\n\n    def _build_task_queue(self, step_tool):\n        return TaskQueue(self._queue_identifier(step_tool))\n\n    def execute_step(self):\n        execution = AgentExecution.get_agent_execution_from_id(self.session, self.agent_execution_id)\n        workflow_step = AgentWorkflowStep.find_by_id(self.session, execution.current_agent_step_id)\n        step_tool = AgentWorkflowStepTool.find_by_id(self.session, workflow_step.action_reference_id)\n        task_queue = self._build_task_queue(step_tool)\n\n        if not task_queue.get_status() or task_queue.get_status() == QueueStatus.COMPLETE.value:\n            task_queue.set_status(QueueStatus.INITIATED.value)\n\n        if task_queue.get_status() == QueueStatus.INITIATED.value:\n            self._add_to_queue(task_queue, step_tool)\n            execution.current_feed_group_id = \"DEFAULT\"\n            task_queue.set_status(QueueStatus.PROCESSING.value)\n\n        if not task_queue.get_tasks():\n            task_queue.set_status(QueueStatus.COMPLETE.value)\n            return \"COMPLETE\"\n        self._consume_from_queue(task_queue)\n        return \"default\"\n\n    def _add_to_queue(self, task_queue: TaskQueue, step_tool: AgentWorkflowStepTool):\n        assistant_reply = self._process_input_instruction(step_tool)\n        self._process_reply(task_queue, assistant_reply)\n\n    def _consume_from_queue(self, task_queue: TaskQueue):\n        tasks = task_queue.get_tasks()\n        agent_execution = AgentExecution.find_by_id(self.session, self.agent_execution_id)\n        if tasks:\n            task = task_queue.get_first_task()\n            # generating the new feed group id\n            agent_execution.current_feed_group_id = \"GROUP_\" + str(int(time.time()))\n            self.session.commit()\n            task_response_feed = AgentExecutionFeed(agent_execution_id=self.agent_execution_id,\n                                                    agent_id=self.agent_id,\n                                                    feed=\"Input: \" + task,\n                                                    role=\"assistant\",\n                                                    feed_group_id=agent_execution.current_feed_group_id)\n            self.session.add(task_response_feed)\n            self.session.commit()\n            task_queue.complete_task(\"PROCESSED\")\n\n    def _process_reply(self, task_queue: TaskQueue, assistant_reply: str):\n        assistant_reply = JsonCleaner.extract_json_array_section(assistant_reply)\n        print(\"Queue reply:\", assistant_reply)\n        task_array = np.array(eval(assistant_reply)).flatten().tolist()\n        for task in task_array:\n            task_queue.add_task(str(task))\n            logger.info(\"RAMRAM: Added task to queue: \", task)\n\n    def _process_input_instruction(self, step_tool):\n        prompt = self._build_queue_input_prompt(step_tool)\n        logger.info(\"Prompt: \", prompt)\n        agent_feeds = AgentExecutionFeed.fetch_agent_execution_feeds(self.session, self.agent_execution_id)\n        print(\".........//////////////..........2\")\n        messages = AgentLlmMessageBuilder(self.session, self.llm, self.llm.get_model(), self.agent_id, self.agent_execution_id) \\\n            .build_agent_messages(prompt, agent_feeds, history_enabled=step_tool.history_enabled,\n                                  completion_prompt=step_tool.completion_prompt)\n        current_tokens = TokenCounter.count_message_tokens(messages, self.llm.get_model())\n        response = self.llm.chat_completion(messages, TokenCounter(session=self.session, organisation_id=self.organisation.id).token_limit(self.llm.get_model()) - current_tokens)\n        \n        if 'error' in response and response['message'] is not None:\n            ErrorHandler.handle_openai_errors(self.session, self.agent_id, self.agent_execution_id, response['message'])\n            \n        if 'content' not in response or response['content'] is None:\n            raise RuntimeError(f\"Failed to get response from llm\")\n        total_tokens = current_tokens + TokenCounter.count_message_tokens(response, self.llm.get_model())\n        AgentExecution.update_tokens(self.session, self.agent_execution_id, total_tokens)\n        assistant_reply = response['content']\n        return assistant_reply\n\n    def _build_queue_input_prompt(self, step_tool: AgentWorkflowStepTool):\n        queue_input_prompt = PromptReader.read_agent_prompt(__file__, \"agent_queue_input.txt\")\n        queue_input_prompt = queue_input_prompt.replace(\"{instruction}\", step_tool.input_instruction)\n\n        return queue_input_prompt\n"}
{"type": "source_file", "path": "migrations/versions/c5c19944c90c_create_oauth_tokens.py", "content": "\"\"\"Create Oauth Tokens\n\nRevision ID: c5c19944c90c\nRevises: 7a3e336c0fba\nCreate Date: 2023-06-30 07:26:29.180784\n\n\"\"\"\nfrom alembic import op\nimport sqlalchemy as sa\n\n\n# revision identifiers, used by Alembic.\nrevision = 'c5c19944c90c'\ndown_revision = '7a3e336c0fba'\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.create_table('oauth_tokens',\n    sa.Column('created_at', sa.DateTime(), nullable=True),\n    sa.Column('updated_at', sa.DateTime(), nullable=True),\n    sa.Column('id', sa.Integer(), autoincrement=True, nullable=False),\n    sa.Column('user_id', sa.Integer(), nullable=True),\n    sa.Column('organisation_id', sa.Integer(), nullable=True),\n    sa.Column('toolkit_id', sa.Integer(), nullable=True),\n    sa.Column('key', sa.String(), nullable=True),\n    sa.Column('value', sa.Text(), nullable=True),\n    sa.PrimaryKeyConstraint('id')\n    )\n    op.drop_index('ix_agent_execution_permissions_agent_execution_id', table_name='agent_execution_permissions')\n    op.drop_index('ix_atc_agnt_template_id_key', table_name='agent_template_configs')\n    op.drop_index('ix_agt_agnt_name', table_name='agent_templates')\n    op.drop_index('ix_agt_agnt_organisation_id', table_name='agent_templates')\n    op.drop_index('ix_agt_agnt_workflow_id', table_name='agent_templates')\n    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.create_index('ix_agt_agnt_workflow_id', 'agent_templates', ['agent_workflow_id'], unique=False)\n    op.create_index('ix_agt_agnt_organisation_id', 'agent_templates', ['organisation_id'], unique=False)\n    op.create_index('ix_agt_agnt_name', 'agent_templates', ['name'], unique=False)\n    op.create_index('ix_atc_agnt_template_id_key', 'agent_template_configs', ['agent_template_id', 'key'], unique=False)\n    op.create_index('ix_agent_execution_permissions_agent_execution_id', 'agent_execution_permissions', ['agent_execution_id'], unique=False)\n    op.drop_table('oauth_tokens')\n    # ### end Alembic commands ###\n"}
{"type": "source_file", "path": "migrations/versions/c02f3d759bf3_add_summary_to_resource.py", "content": "\"\"\"add summary to resource\n\nRevision ID: c02f3d759bf3\nRevises: 1d54db311055\nCreate Date: 2023-06-27 05:07:29.016704\n\n\"\"\"\nfrom alembic import op\nimport sqlalchemy as sa\n\n\n# revision identifiers, used by Alembic.\nrevision = 'c02f3d759bf3'\ndown_revision = 'c5c19944c90c'\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ##\n    op.add_column('resources', sa.Column('summary', sa.Text(), nullable=True))\n    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_column('resources', 'summary')\n    # ### end Alembic commands ###\n"}
{"type": "source_file", "path": "migrations/versions/516ecc1c723d_adding_marketplace_template_id_to_agent_.py", "content": "\"\"\"adding marketplace_template_id to agent tempaltes\n\nRevision ID: 516ecc1c723d\nRevises: 8962bed0d809\nCreate Date: 2023-06-13 17:10:06.262764\n\n\"\"\"\nfrom alembic import op\nimport sqlalchemy as sa\n\n\n# revision identifiers, used by Alembic.\nrevision = '516ecc1c723d'\ndown_revision = '8962bed0d809'\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    op.add_column('agent_templates', sa.Column('marketplace_template_id', sa.Integer(), nullable=True))\n\n\ndef downgrade() -> None:\n    op.drop_column('agent_templates', sa.Column('marketplace_template_id', sa.Integer(), nullable=True))\n"}
{"type": "source_file", "path": "main.py", "content": "import requests\nfrom fastapi import FastAPI, HTTPException, Depends, Request, status, Query\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom fastapi.responses import JSONResponse\nfrom fastapi.responses import RedirectResponse\nfrom fastapi_jwt_auth import AuthJWT\nfrom fastapi_jwt_auth.exceptions import AuthJWTException\nfrom fastapi_sqlalchemy import DBSessionMiddleware, db\nfrom pydantic import BaseModel\nfrom sqlalchemy import create_engine\nfrom sqlalchemy.orm import sessionmaker\n\nimport superagi\nfrom datetime import timedelta, datetime\nfrom superagi.agent.workflow_seed import IterationWorkflowSeed, AgentWorkflowSeed\nfrom superagi.config.config import get_config\nfrom superagi.controllers.agent import router as agent_router\nfrom superagi.controllers.agent_execution import router as agent_execution_router\nfrom superagi.controllers.agent_execution_feed import router as agent_execution_feed_router\nfrom superagi.controllers.agent_execution_permission import router as agent_execution_permission_router\nfrom superagi.controllers.agent_template import router as agent_template_router\nfrom superagi.controllers.agent_workflow import router as agent_workflow_router\nfrom superagi.controllers.budget import router as budget_router\nfrom superagi.controllers.config import router as config_router\nfrom superagi.controllers.organisation import router as organisation_router\nfrom superagi.controllers.project import router as project_router\nfrom superagi.controllers.twitter_oauth import router as twitter_oauth_router\nfrom superagi.controllers.google_oauth import router as google_oauth_router\nfrom superagi.controllers.resources import router as resources_router\nfrom superagi.controllers.tool import router as tool_router\nfrom superagi.controllers.tool_config import router as tool_config_router\nfrom superagi.controllers.toolkit import router as toolkit_router\nfrom superagi.controllers.user import router as user_router\nfrom superagi.controllers.agent_execution_config import router as agent_execution_config\nfrom superagi.controllers.analytics import router as analytics_router\nfrom superagi.controllers.models_controller import router as models_controller_router\nfrom superagi.controllers.knowledges import router as knowledges_router\nfrom superagi.controllers.knowledge_configs import router as knowledge_configs_router\nfrom superagi.controllers.vector_dbs import router as vector_dbs_router\nfrom superagi.controllers.vector_db_indices import router as vector_db_indices_router\nfrom superagi.controllers.marketplace_stats import router as marketplace_stats_router\nfrom superagi.controllers.api_key import router as api_key_router\nfrom superagi.controllers.api.agent import router as api_agent_router\nfrom superagi.controllers.webhook import router as web_hook_router\nfrom superagi.helper.tool_helper import register_toolkits, register_marketplace_toolkits\nfrom superagi.lib.logger import logger\nfrom superagi.llms.google_palm import GooglePalm\nfrom superagi.llms.llm_model_factory import build_model_with_api_key\nfrom superagi.llms.openai import OpenAi\nfrom superagi.llms.replicate import Replicate\nfrom superagi.llms.hugging_face import HuggingFace\nfrom superagi.models.agent_template import AgentTemplate\nfrom superagi.models.models_config import ModelsConfig\nfrom superagi.models.organisation import Organisation\nfrom superagi.models.types.login_request import LoginRequest\nfrom superagi.models.types.validate_llm_api_key_request import ValidateAPIKeyRequest\nfrom superagi.models.user import User\nfrom superagi.models.workflows.agent_workflow import AgentWorkflow\nfrom superagi.models.workflows.iteration_workflow import IterationWorkflow\nfrom superagi.models.workflows.iteration_workflow_step import IterationWorkflowStep\nfrom urllib.parse import urlparse\napp = FastAPI()\n\ndb_host = get_config('DB_HOST', 'super__postgres')\ndb_url = get_config('DB_URL', None)\ndb_username = get_config('DB_USERNAME')\ndb_password = get_config('DB_PASSWORD')\ndb_name = get_config('DB_NAME')\nenv = get_config('ENV', \"DEV\")\n\nif db_url is None:\n    if db_username is None:\n        db_url = f'postgresql://{db_host}/{db_name}'\n    else:\n        db_url = f'postgresql://{db_username}:{db_password}@{db_host}/{db_name}'\nelse:\n    db_url = urlparse(db_url)\n    db_url = db_url.scheme + \"://\" + db_url.netloc + db_url.path\n\nengine = create_engine(db_url,\n                       pool_size=20,  # Maximum number of database connections in the pool\n                       max_overflow=50,  # Maximum number of connections that can be created beyond the pool_size\n                       pool_timeout=30,  # Timeout value in seconds for acquiring a connection from the pool\n                       pool_recycle=1800,  # Recycle connections after this number of seconds (optional)\n                       pool_pre_ping=False,  # Enable connection health checks (optional)\n                       )\n\n# app.add_middleware(DBSessionMiddleware, db_url=f'postgresql://{db_username}:{db_password}@localhost/{db_name}')\napp.add_middleware(DBSessionMiddleware, db_url=db_url)\n\n# Configure CORS middleware\norigins = [\n    # Add more origins if needed\n    \"*\",  # Allow all origins\n]\n\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\n# Creating requrired tables -- Now handled using migrations\n# DBBaseModel.metadata.create_all(bind=engine, checkfirst=True)\n# DBBaseModel.metadata.drop_all(bind=engine,checkfirst=True)\n\n\napp.include_router(user_router, prefix=\"/users\")\napp.include_router(tool_router, prefix=\"/tools\")\napp.include_router(organisation_router, prefix=\"/organisations\")\napp.include_router(project_router, prefix=\"/projects\")\napp.include_router(budget_router, prefix=\"/budgets\")\napp.include_router(agent_router, prefix=\"/agents\")\napp.include_router(agent_execution_router, prefix=\"/agentexecutions\")\napp.include_router(agent_execution_feed_router, prefix=\"/agentexecutionfeeds\")\napp.include_router(agent_execution_permission_router, prefix=\"/agentexecutionpermissions\")\napp.include_router(resources_router, prefix=\"/resources\")\napp.include_router(config_router, prefix=\"/configs\")\napp.include_router(toolkit_router, prefix=\"/toolkits\")\napp.include_router(tool_config_router, prefix=\"/tool_configs\")\napp.include_router(config_router, prefix=\"/configs\")\napp.include_router(agent_template_router, prefix=\"/agent_templates\")\napp.include_router(agent_workflow_router, prefix=\"/agent_workflows\")\napp.include_router(twitter_oauth_router, prefix=\"/twitter\")\napp.include_router(agent_execution_config, prefix=\"/agent_executions_configs\")\napp.include_router(analytics_router, prefix=\"/analytics\")\napp.include_router(models_controller_router, prefix=\"/models_controller\")\napp.include_router(google_oauth_router, prefix=\"/google\")\napp.include_router(knowledges_router, prefix=\"/knowledges\")\napp.include_router(knowledge_configs_router, prefix=\"/knowledge_configs\")\napp.include_router(vector_dbs_router, prefix=\"/vector_dbs\")\napp.include_router(vector_db_indices_router, prefix=\"/vector_db_indices\")\napp.include_router(marketplace_stats_router, prefix=\"/marketplace\")\napp.include_router(api_key_router, prefix=\"/api-keys\")\napp.include_router(api_agent_router,prefix=\"/v1/agent\")\napp.include_router(web_hook_router,prefix=\"/webhook\")\n\n# in production you can use Settings management\n# from pydantic to get secret key from .env\nclass Settings(BaseModel):\n    # jwt_secret = get_config(\"JWT_SECRET_KEY\")\n    authjwt_secret_key: str = superagi.config.config.get_config(\"JWT_SECRET_KEY\")\n\n\ndef create_access_token(email, Authorize: AuthJWT = Depends()):\n    expiry_time_hours = superagi.config.config.get_config(\"JWT_EXPIRY\")\n    if type(expiry_time_hours) == str:\n        expiry_time_hours = int(expiry_time_hours)\n    if expiry_time_hours is None:\n        expiry_time_hours = 200\n    expires = timedelta(hours=expiry_time_hours)\n    access_token = Authorize.create_access_token(subject=email, expires_time=expires)\n    return access_token\n\n\n# callback to get your configuration\n@AuthJWT.load_config\ndef get_config():\n    return Settings()\n\n\n# exception handler for authjwt\n# in production, you can tweak performance using orjson response\n@app.exception_handler(AuthJWTException)\ndef authjwt_exception_handler(request: Request, exc: AuthJWTException):\n    return JSONResponse(\n        status_code=exc.status_code,\n        content={\"detail\": exc.message}\n    )\n\n\ndef replace_old_iteration_workflows(session):\n    templates = session.query(AgentTemplate).all()\n    for template in templates:\n        iter_workflow = IterationWorkflow.find_by_id(session, template.agent_workflow_id)\n        if not iter_workflow:\n            continue\n        if iter_workflow.name == \"Fixed Task Queue\":\n            agent_workflow = AgentWorkflow.find_by_name(session, \"Fixed Task Workflow\")\n            template.agent_workflow_id = agent_workflow.id\n            session.commit()\n\n        if iter_workflow.name == \"Maintain Task Queue\":\n            agent_workflow = AgentWorkflow.find_by_name(session, \"Dynamic Task Workflow\")\n            template.agent_workflow_id = agent_workflow.id\n            session.commit()\n\n        if iter_workflow.name == \"Don't Maintain Task Queue\" or iter_workflow.name == \"Goal Based Agent\":\n            agent_workflow = AgentWorkflow.find_by_name(session, \"Goal Based Workflow\")\n            template.agent_workflow_id = agent_workflow.id\n            session.commit()\n\n@app.on_event(\"startup\")\nasync def startup_event():\n    # Perform startup tasks here\n    logger.info(\"Running Startup tasks\")\n    Session = sessionmaker(bind=engine)\n    session = Session()\n    default_user = session.query(User).filter(User.email == \"super6@agi.com\").first()\n    logger.info(default_user)\n    if default_user is not None:\n        organisation = session.query(Organisation).filter_by(id=default_user.organisation_id).first()\n        logger.info(organisation)\n        register_toolkits(session, organisation)\n\n    def register_toolkit_for_all_organisation():\n        organizations = session.query(Organisation).all()\n        for organization in organizations:\n            register_toolkits(session, organization)\n        logger.info(\"Successfully registered local toolkits for all Organisations!\")\n\n    def register_toolkit_for_master_organisation():\n        marketplace_organisation_id = superagi.config.config.get_config(\"MARKETPLACE_ORGANISATION_ID\")\n        marketplace_organisation = session.query(Organisation).filter(\n            Organisation.id == marketplace_organisation_id).first()\n        if marketplace_organisation is not None:\n            register_marketplace_toolkits(session, marketplace_organisation)\n\n    IterationWorkflowSeed.build_single_step_agent(session)\n    IterationWorkflowSeed.build_task_based_agents(session)\n    IterationWorkflowSeed.build_action_based_agents(session)\n    IterationWorkflowSeed.build_initialize_task_workflow(session)\n\n    AgentWorkflowSeed.build_goal_based_agent(session)\n    AgentWorkflowSeed.build_task_based_agent(session)\n    AgentWorkflowSeed.build_fixed_task_based_agent(session)\n    AgentWorkflowSeed.build_sales_workflow(session)\n    AgentWorkflowSeed.build_recruitment_workflow(session)\n    AgentWorkflowSeed.build_coding_workflow(session)\n\n    # NOTE: remove old workflows. Need to remove this changes later\n    workflows = [\"Sales Engagement Workflow\", \"Recruitment Workflow\", \"SuperCoder\", \"Goal Based Workflow\",\n     \"Dynamic Task Workflow\", \"Fixed Task Workflow\"]\n    workflows = session.query(AgentWorkflow).filter(AgentWorkflow.name.not_in(workflows))\n    for workflow in workflows:\n        session.delete(workflow)\n\n    # AgentWorkflowSeed.doc_search_and_code(session)\n    # AgentWorkflowSeed.build_research_email_workflow(session)\n    replace_old_iteration_workflows(session)\n\n    if env != \"PROD\":\n        register_toolkit_for_all_organisation()\n    else:\n        register_toolkit_for_master_organisation()\n    session.close()\n\n\n@app.post('/login')\ndef login(request: LoginRequest, Authorize: AuthJWT = Depends()):\n    \"\"\"Login API for email and password based login\"\"\"\n\n    email_to_find = request.email\n    user: User = db.session.query(User).filter(User.email == email_to_find).first()\n\n    if user == None or request.email != user.email or request.password != user.password:\n        raise HTTPException(status_code=401, detail=\"Bad username or password\")\n\n    # subject identifier for who this token is for example id or username from database\n    access_token = create_access_token(user.email, Authorize)\n    return {\"access_token\": access_token}\n\n\n# def get_jwt_from_payload(user_email: str,Authorize: AuthJWT = Depends()):\n#     access_token = Authorize.create_access_token(subject=user_email)\n#     return access_token\n\n@app.get('/github-login')\ndef github_login():\n    \"\"\"GitHub login\"\"\"\n\n    github_client_id = \"\"\n    return RedirectResponse(f'https://github.com/login/oauth/authorize?scope=user:email&client_id={github_client_id}')\n\n\n@app.get('/github-auth')\ndef github_auth_handler(code: str = Query(...), Authorize: AuthJWT = Depends()):\n    \"\"\"GitHub login callback\"\"\"\n\n    github_token_url = 'https://github.com/login/oauth/access_token'\n    github_client_id = superagi.config.config.get_config(\"GITHUB_CLIENT_ID\")\n    github_client_secret = superagi.config.config.get_config(\"GITHUB_CLIENT_SECRET\")\n\n    frontend_url = superagi.config.config.get_config(\"FRONTEND_URL\", \"http://localhost:3000\")\n    params = {\n        'client_id': github_client_id,\n        'client_secret': github_client_secret,\n        'code': code\n    }\n    headers = {\n        'Accept': 'application/json'\n    }\n    response = requests.post(github_token_url, params=params, headers=headers)\n    if response.ok:\n        data = response.json()\n        access_token = data.get('access_token')\n        github_api_url = 'https://api.github.com/user'\n        headers = {\n            'Authorization': f'Bearer {access_token}'\n        }\n        response = requests.get(github_api_url, headers=headers)\n        if response.ok:\n            user_data = response.json()\n            user_email = user_data[\"email\"]\n            if user_email is None:\n                user_email = user_data[\"login\"] + \"@github.com\"\n            db_user: User = db.session.query(User).filter(User.email == user_email).first()\n            if db_user is not None:\n                jwt_token = create_access_token(user_email, Authorize)\n                redirect_url_success = f\"{frontend_url}?access_token={jwt_token}&first_time_login={False}\"\n                return RedirectResponse(url=redirect_url_success)\n\n            user = User(name=user_data[\"name\"], email=user_email)\n            db.session.add(user)\n            db.session.commit()\n            jwt_token = create_access_token(user_email, Authorize)\n            redirect_url_success = f\"{frontend_url}?access_token={jwt_token}&first_time_login={True}\"\n            return RedirectResponse(url=redirect_url_success)\n        else:\n            redirect_url_failure = \"https://superagi.com/\"\n            return RedirectResponse(url=redirect_url_failure)\n    else:\n        redirect_url_failure = \"https://superagi.com/\"\n        return RedirectResponse(url=redirect_url_failure)\n\n\n@app.get('/user')\ndef user(Authorize: AuthJWT = Depends()):\n    \"\"\"API to get current logged in User\"\"\"\n\n    Authorize.jwt_required()\n    current_user = Authorize.get_jwt_subject()\n    return {\"user\": current_user}\n\n\n@app.get(\"/validate-access-token\")\nasync def root(Authorize: AuthJWT = Depends()):\n    \"\"\"API to validate access token\"\"\"\n\n    try:\n        Authorize.jwt_required()\n        current_user_email = Authorize.get_jwt_subject()\n        current_user = db.session.query(User).filter(User.email == current_user_email).first()\n        return current_user\n    except:\n        raise HTTPException(status_code=status.HTTP_401_UNAUTHORIZED, detail=\"Invalid token\")\n\n\n@app.post(\"/validate-llm-api-key\")\nasync def validate_llm_api_key(request: ValidateAPIKeyRequest, Authorize: AuthJWT = Depends()):\n    \"\"\"API to validate LLM API Key\"\"\"\n    source = request.model_source\n    api_key = request.model_api_key\n    model = build_model_with_api_key(source, api_key)\n    valid_api_key = model.verify_access_key() if model is not None else False\n    if valid_api_key:\n        return {\"message\": \"Valid API Key\", \"status\": \"success\"}\n    else:\n        return {\"message\": \"Invalid API Key\", \"status\": \"failed\"}\n\n\n@app.get(\"/validate-open-ai-key/{open_ai_key}\")\nasync def root(open_ai_key: str, Authorize: AuthJWT = Depends()):\n    \"\"\"API to validate Open AI Key\"\"\"\n\n    try:\n        llm = OpenAi(api_key=open_ai_key)\n        response = llm.chat_completion([{\"role\": \"system\", \"content\": \"Hey!\"}])\n    except:\n        raise HTTPException(status_code=status.HTTP_401_UNAUTHORIZED, detail=\"Invalid API Key\")\n\n\n# #Unprotected route\n@app.get(\"/hello/{name}\")\nasync def say_hello(name: str, Authorize: AuthJWT = Depends()):\n    Authorize.jwt_required()\n    return {\"message\": f\"Hello {name}\"}\n\n@app.get('/get/github_client_id')\ndef github_client_id():\n    \"\"\"Get GitHub Client ID\"\"\"\n\n    git_hub_client_id = superagi.config.config.get_config(\"GITHUB_CLIENT_ID\")\n    if git_hub_client_id:\n        git_hub_client_id = git_hub_client_id.strip()\n    return {\"github_client_id\": git_hub_client_id}\n\n# # __________________TO RUN____________________________\n# # uvicorn main:app --host 0.0.0.0 --port 8001 --reload\n\n"}
{"type": "source_file", "path": "migrations/versions/cac478732572_delete_agent_feature.py", "content": "\"\"\"delete_agent_feature\n\nRevision ID: cac478732572\nRevises: e39295ec089c\nCreate Date: 2023-07-13 17:18:42.003412\n\n\"\"\"\nfrom alembic import op\nimport sqlalchemy as sa\n\n# revision identifiers, used by Alembic.\nrevision = 'cac478732572'\ndown_revision = 'e39295ec089c'\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    op.add_column('agents', sa.Column('is_deleted', sa.Boolean(), nullable=True, server_default=sa.false()))\n\n\ndef downgrade() -> None:\n    op.drop_column('agents', 'is_deleted')\n"}
{"type": "source_file", "path": "migrations/versions/71e3980d55f5_knowledge_and_vector_dbs.py", "content": "\"\"\"Knowledge and Vector dbs\n\nRevision ID: 71e3980d55f5\nRevises: cac478732572\nCreate Date: 2023-07-26 07:18:06.492832\n\n\"\"\"\nfrom alembic import op\nimport sqlalchemy as sa\n\n\n# revision identifiers, used by Alembic.\nrevision = '71e3980d55f5'\ndown_revision = 'cac478732572'\nbranch_labels = None\ndepends_on = None\n\ndef upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.create_table('knowledge_configs',\n    sa.Column('id', sa.Integer(), autoincrement=True, nullable=False),\n    sa.Column('knowledge_id', sa.Integer(), nullable=False),\n    sa.Column('key', sa.String(), nullable=True),\n    sa.Column('value', sa.Text(), nullable=True),\n    sa.Column('created_at', sa.DateTime(), nullable=True),\n    sa.Column('updated_at', sa.DateTime(), nullable=True),\n    sa.PrimaryKeyConstraint('id')\n    )\n    op.create_table('knowledges',\n    sa.Column('id', sa.Integer(), autoincrement=True, nullable=False),\n    sa.Column('name', sa.String(), nullable=False),\n    sa.Column('description', sa.String(), nullable=True),\n    sa.Column('vector_db_index_id', sa.Integer(), nullable=True),\n    sa.Column('organisation_id', sa.Integer(), nullable=True),\n    sa.Column('contributed_by', sa.String(), nullable=True),\n    sa.Column('created_at', sa.DateTime(), nullable=True),\n    sa.Column('updated_at', sa.DateTime(), nullable=True),\n    sa.PrimaryKeyConstraint('id')\n    )\n    op.create_table('marketplace_stats',\n    sa.Column('id', sa.Integer(), autoincrement=True, nullable=False),\n    sa.Column('reference_id', sa.Integer(), nullable=True),\n    sa.Column('reference_name', sa.String(), nullable=True),\n    sa.Column('key', sa.String(), nullable=True),\n    sa.Column('value', sa.Integer(), nullable=True),\n    sa.Column('created_at', sa.DateTime(), nullable=True),\n    sa.Column('updated_at', sa.DateTime(), nullable=True),\n    sa.PrimaryKeyConstraint('id')\n    )\n    op.create_table('vector_db_configs',\n    sa.Column('id', sa.Integer(), autoincrement=True, nullable=False),\n    sa.Column('vector_db_id', sa.Integer(), nullable=False),\n    sa.Column('key', sa.String(), nullable=True),\n    sa.Column('value', sa.Text(), nullable=True),\n    sa.Column('created_at', sa.DateTime(), nullable=True),\n    sa.Column('updated_at', sa.DateTime(), nullable=True),\n    sa.PrimaryKeyConstraint('id')\n    )\n    op.create_table('vector_db_indices',\n    sa.Column('id', sa.Integer(), autoincrement=True, nullable=False),\n    sa.Column('name', sa.String(), nullable=False),\n    sa.Column('vector_db_id', sa.Integer(), nullable=True),\n    sa.Column('dimensions', sa.Integer(), nullable=True),\n    sa.Column('state', sa.String(), nullable=True),\n    sa.Column('created_at', sa.DateTime(), nullable=True),\n    sa.Column('updated_at', sa.DateTime(), nullable=True),\n    sa.PrimaryKeyConstraint('id')\n    )\n    op.create_table('vector_dbs',\n    sa.Column('id', sa.Integer(), autoincrement=True, nullable=False),\n    sa.Column('name', sa.String(), nullable=False),\n    sa.Column('db_type', sa.String(), nullable=True),\n    sa.Column('organisation_id', sa.Integer(), nullable=True),\n    sa.Column('created_at', sa.DateTime(), nullable=True),\n    sa.Column('updated_at', sa.DateTime(), nullable=True),\n    sa.PrimaryKeyConstraint('id')\n    )\n    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_table('vector_dbs')\n    op.drop_table('vector_db_indices')\n    op.drop_table('vector_db_configs')\n    op.drop_table('knowledges')\n    op.drop_table('knowledge_configs')"}
{"type": "source_file", "path": "migrations/versions/5184645e9f12_add_question_to_agent_execution_.py", "content": "\"\"\"add question to agent execution permission\n\nRevision ID: 5184645e9f12\nRevises: 9419b3340af7\nCreate Date: 2023-07-21 08:16:14.702389\n\n\"\"\"\nfrom alembic import op\nimport sqlalchemy as sa\n\n\n# revision identifiers, used by Alembic.\nrevision = '5184645e9f12'\ndown_revision = '9419b3340af7'\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    op.add_column('agent_execution_permissions', sa.Column('question', sa.Text(), nullable=True))\n\n\ndef downgrade() -> None:\n    op.drop_column('agent_execution_permissions', \"question\")\n"}
{"type": "source_file", "path": "migrations/versions/661ec8a4c32e_open_ai_error_handling.py", "content": "\"\"\"open_ai_error_handling\n\nRevision ID: 661ec8a4c32e\nRevises: 40affbf3022b\nCreate Date: 2023-09-07 10:41:07.462436\n\n\"\"\"\nfrom alembic import op\nimport sqlalchemy as sa\n\n\n# revision identifiers, used by Alembic.\nrevision = '661ec8a4c32e'\ndown_revision = 'c4f2f6ba602a'\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.add_column('agent_execution_feeds', sa.Column('error_message', sa.String(), nullable=True))\n    op.add_column('agent_executions', sa.Column('last_shown_error_id', sa.Integer(), nullable=True))\n    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_column('agent_executions', 'last_shown_error_id')\n    op.drop_column('agent_execution_feeds', 'error_message')\n    # ### end Alembic commands ###\n"}
{"type": "source_file", "path": "migrations/versions/5d5f801f28e7_create_model_table.py", "content": "\"\"\"create model table\n\nRevision ID: 5d5f801f28e7\nRevises: 520aa6776347\nCreate Date: 2023-08-07 05:36:29.791610\n\n\"\"\"\nfrom alembic import op\nimport sqlalchemy as sa\n\n\n# revision identifiers, used by Alembic.\nrevision = '5d5f801f28e7'\ndown_revision = 'be1d922bf2ad'\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.create_table('models',\n    sa.Column('id', sa.Integer(), nullable=False),\n    sa.Column('model_name', sa.String(), nullable=False),\n    sa.Column('description', sa.String(), nullable=True),\n    sa.Column('end_point', sa.String(), nullable=False),\n    sa.Column('model_provider_id', sa.Integer(), nullable=False),\n    sa.Column('token_limit', sa.Integer(), nullable=False),\n    sa.Column('type', sa.String(), nullable=False),\n    sa.Column('version', sa.String(), nullable=False),\n    sa.Column('org_id', sa.Integer(), nullable=False),\n    sa.Column('model_features', sa.String(), nullable=False),\n    sa.Column('created_at', sa.DateTime(), nullable=True),\n    sa.Column('updated_at', sa.DateTime(), nullable=True),\n    sa.PrimaryKeyConstraint('id')\n    )\n    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_table('models')\n    # ### end Alembic commands ###\n"}
{"type": "source_file", "path": "superagi/agent/__init__.py", "content": ""}
{"type": "source_file", "path": "migrations/versions/d9b3436197eb_renaming_templates.py", "content": "\"\"\"renaming templates\n\nRevision ID: d9b3436197eb\nRevises: 3356a2f89a33\nCreate Date: 2023-06-10 09:28:28.262705\n\n\"\"\"\nfrom alembic import op\nimport sqlalchemy as sa\n\n\n# revision identifiers, used by Alembic.\nrevision = 'd9b3436197eb'\ndown_revision = '3356a2f89a33'\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    op.rename_table('agent_templates', 'agent_workflows')\n    op.rename_table('agent_template_steps', 'agent_workflow_steps')\n    with op.batch_alter_table('agent_workflow_steps') as bop:\n        bop.alter_column('agent_template_id', new_column_name='agent_workflow_id')\n    with op.batch_alter_table('agents') as bop:\n        bop.alter_column('agent_template_id', new_column_name='agent_workflow_id')\n\n\ndef downgrade() -> None:\n    op.rename_table('agent_workflows', 'agent_templates')\n    op.rename_table('agent_workflow_steps', 'agent_template_steps')\n    with op.batch_alter_table('agent_templates') as bop:\n        bop.alter_column('agent_workflow_id', new_column_name='agent_template_id')\n    with op.batch_alter_table('agents') as bop:\n        bop.alter_column('agent_workflow_id', new_column_name='agent_template_id')\n"}
{"type": "source_file", "path": "migrations/versions/3867bb00a495_added_first_login_source.py", "content": "\"\"\"added_first_login_source\n\nRevision ID: 3867bb00a495\nRevises: 661ec8a4c32e\nCreate Date: 2023-09-15 02:06:24.006555\n\n\"\"\"\nfrom alembic import op\nimport sqlalchemy as sa\n\n\n# revision identifiers, used by Alembic.\nrevision = '3867bb00a495'\ndown_revision = '661ec8a4c32e'\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.add_column('users', sa.Column('first_login_source', sa.String(), nullable=True))\n    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_column('users', 'first_login_source')\n    # ### end Alembic commands ###\n"}
{"type": "source_file", "path": "superagi/agent/agent_tool_step_handler.py", "content": "import json\n\nfrom superagi.agent.task_queue import TaskQueue\nfrom superagi.agent.agent_message_builder import AgentLlmMessageBuilder\nfrom superagi.agent.agent_prompt_builder import AgentPromptBuilder\nfrom superagi.agent.output_handler import ToolOutputHandler\nfrom superagi.agent.output_parser import AgentSchemaToolOutputParser\nfrom superagi.agent.queue_step_handler import QueueStepHandler\nfrom superagi.agent.tool_builder import ToolBuilder\nfrom superagi.helper.error_handler import ErrorHandler\nfrom superagi.helper.prompt_reader import PromptReader\nfrom superagi.helper.token_counter import TokenCounter\nfrom superagi.lib.logger import logger\nfrom superagi.models.agent import Agent\nfrom superagi.models.agent_config import AgentConfiguration\nfrom superagi.models.agent_execution import AgentExecution\nfrom superagi.models.agent_execution_config import AgentExecutionConfiguration\nfrom superagi.models.agent_execution_feed import AgentExecutionFeed\nfrom superagi.models.agent_execution_permission import AgentExecutionPermission\nfrom superagi.models.tool import Tool\nfrom superagi.models.toolkit import Toolkit\nfrom superagi.models.workflows.agent_workflow_step import AgentWorkflowStep\nfrom superagi.models.workflows.agent_workflow_step_tool import AgentWorkflowStepTool\nfrom superagi.resource_manager.resource_summary import ResourceSummarizer\nfrom superagi.tools.base_tool import BaseTool\nfrom sqlalchemy import and_\n\nclass AgentToolStepHandler:\n    \"\"\"Handles the tools steps in the agent workflow\"\"\"\n    def __init__(self, session, llm, agent_id: int, agent_execution_id: int, memory=None):\n        self.session = session\n        self.llm = llm\n        self.agent_execution_id = agent_execution_id\n        self.agent_id = agent_id\n        self.memory = memory\n        self.task_queue = TaskQueue(str(self.agent_execution_id))\n        self.organisation = Agent.find_org_by_agent_id(self.session, self.agent_id)\n\n    def execute_step(self):\n        execution = AgentExecution.get_agent_execution_from_id(self.session, self.agent_execution_id)\n        workflow_step = AgentWorkflowStep.find_by_id(self.session, execution.current_agent_step_id)\n        step_tool = AgentWorkflowStepTool.find_by_id(self.session, workflow_step.action_reference_id)\n        agent_config = Agent.fetch_configuration(self.session, self.agent_id)\n        agent_execution_config = AgentExecutionConfiguration.fetch_configuration(self.session, self.agent_execution_id)\n        # print(agent_execution_config)\n\n        if not self._handle_wait_for_permission(execution, workflow_step):\n            return\n\n        if step_tool.tool_name == \"TASK_QUEUE\":\n            step_response = QueueStepHandler(self.session, self.llm, self.agent_id, self.agent_execution_id).execute_step()\n            next_step = AgentWorkflowStep.fetch_next_step(self.session, workflow_step.id, step_response)\n            self._handle_next_step(next_step)\n            return\n\n        if step_tool.tool_name == \"WAIT_FOR_PERMISSION\":\n            self._create_permission_request(execution, step_tool)\n            return\n\n        assistant_reply = self._process_input_instruction(agent_config, agent_execution_config, step_tool,\n                                                          workflow_step)\n        tool_obj = self._build_tool_obj(agent_config, agent_execution_config, step_tool.tool_name)\n        tool_output_handler = ToolOutputHandler(self.agent_execution_id, agent_config, [tool_obj],self.memory,\n                                                output_parser=AgentSchemaToolOutputParser())\n        final_response = tool_output_handler.handle(self.session, assistant_reply)\n        step_response = \"default\"\n        if step_tool.output_instruction:\n            step_response = self._process_output_instruction(final_response.result, step_tool, workflow_step)\n\n        next_step = AgentWorkflowStep.fetch_next_step(self.session, workflow_step.id, step_response)\n        self._handle_next_step(next_step)\n        self.session.flush()\n\n    def _create_permission_request(self, execution, step_tool: AgentWorkflowStepTool):\n        new_agent_execution_permission = AgentExecutionPermission(\n            agent_execution_id=self.agent_execution_id,\n            status=\"PENDING\",\n            agent_id=self.agent_id,\n            tool_name=\"WAIT_FOR_PERMISSION\",\n            question=step_tool.input_instruction,\n            assistant_reply=\"\")\n        self.session.add(new_agent_execution_permission)\n        self.session.commit()\n        self.session.flush()\n        execution.permission_id = new_agent_execution_permission.id\n        execution.status = \"WAITING_FOR_PERMISSION\"\n        self.session.commit()\n\n    def _handle_next_step(self, next_step):\n        if str(next_step) == \"COMPLETE\":\n            agent_execution = AgentExecution.get_agent_execution_from_id(self.session, self.agent_execution_id)\n            agent_execution.current_agent_step_id = -1\n            agent_execution.status = \"COMPLETED\"\n        else:\n            AgentExecution.assign_next_step_id(self.session, self.agent_execution_id, next_step.id)\n        self.session.commit()\n\n    def _process_input_instruction(self, agent_config, agent_execution_config, step_tool, workflow_step):\n        tool_obj = self._build_tool_obj(agent_config, agent_execution_config, step_tool.tool_name)\n        prompt = self._build_tool_input_prompt(step_tool, tool_obj, agent_execution_config)\n        logger.info(\"Prompt: \", prompt)\n        agent_feeds = AgentExecutionFeed.fetch_agent_execution_feeds(self.session, self.agent_execution_id)\n        messages = AgentLlmMessageBuilder(self.session, self.llm, self.llm.get_model(), self.agent_id, self.agent_execution_id) \\\n            .build_agent_messages(prompt, agent_feeds, history_enabled=step_tool.history_enabled,\n                                  completion_prompt=step_tool.completion_prompt)\n        # print(messages)\n        current_tokens = TokenCounter.count_message_tokens(messages, self.llm.get_model())\n        response = self.llm.chat_completion(messages, TokenCounter(session=self.session, organisation_id=self.organisation.id).token_limit(self.llm.get_model()) - current_tokens)\n\n        if 'error' in response and response['message'] is not None:\n            ErrorHandler.handle_openai_errors(self.session, self.agent_id, self.agent_execution_id, response['message'])\n        # ModelsHelper(session=self.session, organisation_id=organisation.id).create_call_log(execution.name,agent_config['agent_id'],response['response'].usage.total_tokens,json.loads(response['content'])['tool']['name'],agent_config['model'])\n        if 'content' not in response or response['content'] is None:\n            raise RuntimeError(f\"Failed to get response from llm\")\n        total_tokens = current_tokens + TokenCounter.count_message_tokens(response, self.llm.get_model())\n        AgentExecution.update_tokens(self.session, self.agent_execution_id, total_tokens)\n        assistant_reply = response['content']\n        return assistant_reply\n\n    def _build_tool_obj(self, agent_config, agent_execution_config, tool_name: str):\n        model_api_key = AgentConfiguration.get_model_api_key(self.session, self.agent_id, agent_config[\"model\"])['api_key']\n        tool_builder = ToolBuilder(self.session, self.agent_id, self.agent_execution_id)\n        resource_summary = \"\"\n        if tool_name == \"QueryResourceTool\":\n            resource_summary = ResourceSummarizer(session=self.session,\n                                                  agent_id=self.agent_id,\n                                                  model=agent_config[\"model\"]).fetch_or_create_agent_resource_summary(\n                default_summary=agent_config.get(\"resource_summary\"))\n\n        organisation = Agent.find_org_by_agent_id(self.session, self.agent_id)\n        tool = self.session.query(Tool).join(Toolkit, and_(Tool.toolkit_id == Toolkit.id, Toolkit.organisation_id == organisation.id, Tool.name == tool_name)).first()\n        tool_obj = tool_builder.build_tool(tool)\n        tool_obj = tool_builder.set_default_params_tool(tool_obj, agent_config, agent_execution_config, model_api_key,\n                                                        resource_summary,self.memory)\n        return tool_obj\n\n    def _process_output_instruction(self, final_response: str, step_tool: AgentWorkflowStepTool,\n                                    workflow_step: AgentWorkflowStep):\n        prompt = self._build_tool_output_prompt(step_tool, final_response, workflow_step)\n        messages = [{\"role\": \"system\", \"content\": prompt}]\n        current_tokens = TokenCounter.count_message_tokens(messages, self.llm.get_model())\n        response = self.llm.chat_completion(messages,\n                                            TokenCounter(session=self.session, organisation_id=self.organisation.id).token_limit(self.llm.get_model()) - current_tokens)\n\n        if 'error' in response and response['message'] is not None:\n            ErrorHandler.handle_openai_errors(self.session, self.agent_id, self.agent_execution_id, response['message'])\n            \n        if 'content' not in response or response['content'] is None:\n            raise RuntimeError(f\"ToolWorkflowStepHandler: Failed to get output response from llm\")\n        total_tokens = current_tokens + TokenCounter.count_message_tokens(response, self.llm.get_model())\n        AgentExecution.update_tokens(self.session, self.agent_execution_id, total_tokens)\n        step_response = response['content']\n        step_response = step_response.replace(\"'\", \"\").replace(\"\\\"\", \"\")\n        return step_response\n\n    def _build_tool_input_prompt(self, step_tool: AgentWorkflowStepTool, tool: BaseTool, agent_execution_config: dict):\n        super_agi_prompt = PromptReader.read_agent_prompt(__file__, \"agent_tool_input.txt\")\n        super_agi_prompt = super_agi_prompt.replace(\"{goals}\", AgentPromptBuilder.add_list_items_to_string(\n            agent_execution_config[\"goal\"]))\n        super_agi_prompt = super_agi_prompt.replace(\"{tool_name}\", step_tool.tool_name)\n        super_agi_prompt = super_agi_prompt.replace(\"{instruction}\", step_tool.input_instruction)\n\n        tool_schema = f\"\\\"{tool.name}\\\": {tool.description}, args json schema: {json.dumps(tool.args)}\"\n        super_agi_prompt = super_agi_prompt.replace(\"{tool_schema}\", tool_schema)\n        return super_agi_prompt\n\n    def _get_step_responses(self, workflow_step: AgentWorkflowStep):\n        return [step[\"step_response\"] for step in workflow_step.next_steps]\n\n    def _build_tool_output_prompt(self, step_tool: AgentWorkflowStepTool, tool_output: str,\n                                  workflow_step: AgentWorkflowStep):\n        super_agi_prompt = PromptReader.read_agent_prompt(__file__, \"agent_tool_output.txt\")\n        super_agi_prompt = super_agi_prompt.replace(\"{tool_output}\", tool_output)\n        super_agi_prompt = super_agi_prompt.replace(\"{tool_name}\", step_tool.tool_name)\n        super_agi_prompt = super_agi_prompt.replace(\"{instruction}\", step_tool.output_instruction)\n\n        step_responses = self._get_step_responses(workflow_step)\n        if \"default\" in step_responses:\n            step_responses.remove(\"default\")\n        super_agi_prompt = super_agi_prompt.replace(\"{output_options}\", str(step_responses))\n        return super_agi_prompt\n\n    def _handle_wait_for_permission(self, agent_execution, workflow_step: AgentWorkflowStep):\n        \"\"\"\n        Handles the wait for permission when the agent execution is waiting for permission.\n\n        Args:\n            agent_execution (AgentExecution): The agent execution.\n            workflow_step (AgentWorkflowStep): The workflow step.\n\n        Raises:\n            Returns permission success or failure\n        \"\"\"\n        if agent_execution.status != \"WAITING_FOR_PERMISSION\":\n            return True\n        agent_execution_permission = self.session.query(AgentExecutionPermission).filter(\n            AgentExecutionPermission.id == agent_execution.permission_id).first()\n        if agent_execution_permission.status == \"PENDING\":\n            logger.error(\"handle_wait_for_permission: Permission is still pending\")\n            return False\n        if agent_execution_permission.status == \"APPROVED\":\n            next_step = AgentWorkflowStep.fetch_next_step(self.session, workflow_step.id, \"YES\")\n        else:\n            next_step = AgentWorkflowStep.fetch_next_step(self.session, workflow_step.id, \"NO\")\n            result = f\"{' User has given the following feedback : ' + agent_execution_permission.user_feedback if agent_execution_permission.user_feedback else ''}\"\n\n\n            agent_execution_feed = AgentExecutionFeed(agent_execution_id=agent_execution_permission.agent_execution_id,\n                                                      agent_id=agent_execution_permission.agent_id,\n                                                      feed=result, role=\"user\",\n                                                      feed_group_id=agent_execution.current_feed_group_id)\n            self.session.add(agent_execution_feed)\n\n        agent_execution.status = \"RUNNING\"\n        agent_execution.permission_id = -1\n        self.session.commit()\n        self._handle_next_step(next_step)\n        self.session.commit()\n        return False\n"}
{"type": "source_file", "path": "migrations/versions/9419b3340af7_create_agent_workflow.py", "content": "\"\"\"create agent workflow\n\nRevision ID: 9419b3340af7\nRevises: fe234ea6e9bc\nCreate Date: 2023-07-18 16:46:03.497943\n\n\"\"\"\nfrom alembic import op\nimport sqlalchemy as sa\nfrom sqlalchemy.dialects import postgresql\n\n# revision identifiers, used by Alembic.\nrevision = '9419b3340af7'\ndown_revision = 'fe234ea6e9bc'\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    op.create_table('agent_workflows',\n                    sa.Column('id', sa.Integer(), nullable=False),\n                    sa.Column('name', sa.String(), nullable=True),\n                    sa.Column('description', sa.Text(), nullable=True),\n                    sa.Column('organisation_id', sa.Integer(), nullable=True),\n                    sa.Column('created_at', sa.DateTime(), nullable=True),\n                    sa.Column('updated_at', sa.DateTime(), nullable=True),\n                    sa.PrimaryKeyConstraint('id')\n                   )\n\n    op.create_table('agent_workflow_steps',\n                    sa.Column('id', sa.Integer(), nullable=False),\n                    sa.Column('step_type', sa.String(), nullable=False),\n                    sa.Column('agent_workflow_id', sa.Integer(), nullable=True),\n                    sa.Column('action_reference_id', sa.Integer(), nullable=True),\n                    sa.Column('action_type', sa.String(), nullable=True),\n                    sa.Column('unique_id', sa.String(), nullable=False),\n                    sa.Column('next_steps', postgresql.JSONB(astext_type=sa.Text()), nullable=True),\n                    sa.Column('created_at', sa.DateTime(), nullable=True),\n                    sa.Column('updated_at', sa.DateTime(), nullable=True),\n                    sa.PrimaryKeyConstraint('id')\n                    )\n\n    op.create_table('agent_workflow_step_tools',\n                    sa.Column('id', sa.Integer(), nullable=False),\n                    sa.Column('unique_id', sa.String(), nullable=True),\n                    sa.Column('tool_name', sa.String(), nullable=True),\n                    sa.Column('input_instruction', sa.Text(), nullable=True),\n                    sa.Column('output_instruction', sa.Text(), nullable=True),\n                    sa.Column('history_enabled', sa.Boolean(), nullable=True),\n                    sa.Column('completion_prompt', sa.Text(), nullable=True),\n                    sa.Column('created_at', sa.DateTime(), nullable=True),\n                    sa.Column('updated_at', sa.DateTime(), nullable=True),\n                    sa.PrimaryKeyConstraint('id')\n                    )\n\ndef downgrade() -> None:\n    op.drop_table('agent_workflows')\n    op.drop_table('agent_workflow_steps')\n    op.drop_table('agent_workflow_step_tools')\n"}
{"type": "source_file", "path": "migrations/versions/e39295ec089c_creating_events.py", "content": "\"\"\"creating events\n\nRevision ID: e39295ec089c\nRevises: 7a3e336c0fba\nCreate Date: 2023-06-30 12:23:12.269999\n\n\"\"\"\nfrom alembic import op\nimport sqlalchemy as sa\nfrom sqlalchemy.dialects import postgresql\n\n# revision identifiers, used by Alembic.\nrevision = 'e39295ec089c'\ndown_revision = '467e85d5e1cd'\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    op.create_table('events',\n    sa.Column('created_at', sa.DateTime(), nullable=True),\n    sa.Column('updated_at', sa.DateTime(), nullable=True),\n    sa.Column('id', sa.Integer(), nullable=False),\n    sa.Column('event_name', sa.String(), nullable=False),\n    sa.Column('event_value', sa.Integer(), nullable=False),\n    sa.Column('event_property', postgresql.JSONB(astext_type=sa.Text()), nullable=True),\n    sa.Column('agent_id', sa.Integer(), nullable=True),\n    sa.Column('org_id', sa.Integer(), nullable=True),\n    sa.PrimaryKeyConstraint('id')\n    )\n\n    # Use naming convention similar to the reference code for the index creation\n    op.create_index(op.f('ix_events_agent_id'), 'events', ['agent_id'], unique=False)\n    op.create_index(op.f('ix_events_org_id'), 'events', ['org_id'], unique=False)\n    op.create_index(op.f('ix_events_event_property'), 'events', ['event_property'], unique=False)\n\ndef downgrade() -> None:\n    op.drop_index(op.f('ix_events_event_property'), table_name='events')\n    op.drop_index(op.f('ix_events_org_id'), table_name='events')\n    op.drop_index(op.f('ix_events_agent_id'), table_name='events')\n    op.drop_table('events')"}
{"type": "source_file", "path": "migrations/versions/9270eb5a8475_local_llms.py", "content": "\"\"\"local_llms\n\nRevision ID: 9270eb5a8475\nRevises: 3867bb00a495\nCreate Date: 2023-10-04 09:26:33.865424\n\n\"\"\"\nfrom alembic import op\nimport sqlalchemy as sa\n\n\n# revision identifiers, used by Alembic.\nrevision = '9270eb5a8475'\ndown_revision = '3867bb00a495'\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.add_column('models', sa.Column('context_length', sa.Integer(), nullable=True))\n    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_column('models', 'context_length')\n    # ### end Alembic commands ###\n"}
{"type": "source_file", "path": "superagi/agent/common_types.py", "content": "from pydantic import BaseModel\n\n\nclass ToolExecutorResponse(BaseModel):\n    status: str\n    result: str = None\n    retry: bool = False\n    is_permission_required: bool = False\n    permission_id: int = None\n\n\nclass TaskExecutorResponse(BaseModel):\n    status: str\n    retry: bool\n"}
{"type": "source_file", "path": "migrations/versions/2fbd6472112c_add_feed_group_id_to_execution_and_feed.py", "content": "\"\"\"add feed group id to execution and feed\n\nRevision ID: 2fbd6472112c\nRevises: 5184645e9f12\nCreate Date: 2023-08-01 17:09:16.183863\n\n\"\"\"\nfrom alembic import op\nimport sqlalchemy as sa\n\n# revision identifiers, used by Alembic.\nrevision = '2fbd6472112c'\ndown_revision = '5184645e9f12'\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    op.add_column('agent_executions',\n                  sa.Column('current_feed_group_id', sa.String(), nullable=True, server_default=\"DEFAULT\"))\n    op.add_column('agent_execution_feeds', sa.Column('feed_group_id', sa.String(), nullable=True))\n\n\ndef downgrade() -> None:\n    op.drop_column('agent_executions', 'current_feed_group_id')\n    op.drop_column('agent_execution_feeds', 'feed_group_id')\n"}
{"type": "source_file", "path": "migrations/versions/446884dcae58_add_api_key_and_web_hook.py", "content": "\"\"\"add api_key and web_hook\n\nRevision ID: 446884dcae58\nRevises: 71e3980d55f5\nCreate Date: 2023-07-29 10:55:21.714245\n\n\"\"\"\nfrom alembic import op\nimport sqlalchemy as sa\n\n\n# revision identifiers, used by Alembic.\nrevision = '446884dcae58'\ndown_revision = '2fbd6472112c'\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.create_table('api_keys',\n    sa.Column('id', sa.Integer(), nullable=False),\n    sa.Column('org_id', sa.Integer(), nullable=True),\n    sa.Column('name', sa.String(), nullable=True),\n    sa.Column('key', sa.String(), nullable=True),\n    sa.Column('created_at', sa.DateTime(), nullable=True),\n    sa.Column('updated_at', sa.DateTime(), nullable=True),\n    sa.Column('is_expired',sa.Boolean(),nullable=True,default=False),\n    sa.PrimaryKeyConstraint('id')\n    )\n    op.create_table('webhooks',\n    sa.Column('id', sa.Integer(), nullable=False),\n    sa.Column('name', sa.String(), nullable=True),\n    sa.Column('org_id', sa.Integer(), nullable=True),\n    sa.Column('url', sa.String(), nullable=True),\n    sa.Column('headers', sa.JSON(), nullable=True),\n    sa.Column('created_at', sa.DateTime(), nullable=True),\n    sa.Column('updated_at', sa.DateTime(), nullable=True),\n    sa.Column('is_deleted',sa.Boolean(),nullable=True),\n    sa.PrimaryKeyConstraint('id')\n    )\n    op.create_table('webhook_events',\n    sa.Column('id', sa.Integer(), nullable=False),\n    sa.Column('agent_id', sa.Integer(), nullable=True),\n    sa.Column('run_id', sa.Integer(), nullable=True),\n    sa.Column('event', sa.String(), nullable=True),\n    sa.Column('status', sa.String(), nullable=True),\n    sa.Column('errors', sa.Text(), nullable=True),\n    sa.Column('created_at', sa.DateTime(), nullable=True),\n    sa.Column('updated_at', sa.DateTime(), nullable=True),\n    sa.PrimaryKeyConstraint('id')\n    )\n\n    #add index *********************\n    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    \n    op.drop_table('webhooks')\n    op.drop_table('api_keys')\n    op.drop_table('webhook_events')\n\n    # ### end Alembic commands ###\n"}
{"type": "source_file", "path": "migrations/versions/d8315244ea43_updated_tool_configs.py", "content": "\"\"\"updated_tool_configs\n\nRevision ID: d8315244ea43\nRevises: 71e3980d55f5\nCreate Date: 2023-08-01 11:11:32.725355\n\n\"\"\"\nfrom alembic import op\nimport sqlalchemy as sa\n\n\n# revision identifiers, used by Alembic.\nrevision = 'd8315244ea43'\ndown_revision = '71e3980d55f5'\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.add_column('tool_configs', sa.Column('key_type', sa.String(), nullable=True))\n    op.add_column('tool_configs', sa.Column('is_secret', sa.Boolean(), nullable=True))\n    op.add_column('tool_configs', sa.Column('is_required', sa.Boolean(), nullable=True))\n    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_column('tool_configs', 'is_required')\n    op.drop_column('tool_configs', 'is_secret')\n    op.drop_column('tool_configs', 'key_type')\n    # ### end Alembic commands ###\n"}
{"type": "source_file", "path": "migrations/versions/520aa6776347_create_models_config.py", "content": "\"\"\"create models config\n\nRevision ID: 520aa6776347\nRevises: 71e3980d55f5\nCreate Date: 2023-08-01 07:48:13.724938\n\n\"\"\"\nfrom alembic import op\nimport sqlalchemy as sa\n\n\n# revision identifiers, used by Alembic.\nrevision = '520aa6776347'\ndown_revision = '446884dcae58'\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.create_table('models_config',\n    sa.Column('id', sa.Integer(), nullable=False),\n    sa.Column('provider', sa.String(), nullable=False),\n    sa.Column('api_key', sa.String(), nullable=False),\n    sa.Column('org_id', sa.Integer(), nullable=False),\n    sa.Column('created_at', sa.DateTime(), nullable=True),\n    sa.Column('updated_at', sa.DateTime(), nullable=True),\n    sa.PrimaryKeyConstraint('id')\n    )\n    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_table('models_config')\n    # ### end Alembic commands ###\n"}
{"type": "source_file", "path": "run_gui.py", "content": "import os\nimport sys\nimport subprocess\nfrom time import sleep\nimport shutil\nfrom superagi.lib.logger import logger\n\ndef check_command(command, message):\n    if not shutil.which(command):\n        logger.info(message)\n        sys.exit(1)\n\ndef run_npm_commands():\n    os.chdir(\"gui\")\n    try:\n        subprocess.run([\"npm\", \"install\"], check=True)\n    except subprocess.CalledProcessError:\n        logger.error(f\"Error during '{' '.join(sys.exc_info()[1].cmd)}'. Exiting.\")\n        sys.exit(1)\n    os.chdir(\"..\")\n\ndef run_server():\n    api_process = subprocess.Popen([\"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"])\n    os.chdir(\"gui\")\n    ui_process = subprocess.Popen([\"npm\", \"run\", \"dev\"])\n    os.chdir(\"..\")\n    return api_process, ui_process\n\ndef cleanup(api_process, ui_process):\n    logger.info(\"Shutting down processes...\")\n    api_process.terminate()\n    ui_process.terminate()\n    logger.info(\"Processes terminated. Exiting.\")\n    sys.exit(1)\n\nif __name__ == \"__main__\":\n    check_command(\"node\", \"Node.js is not installed. Please install it and try again.\")\n    check_command(\"npm\", \"npm is not installed. Please install npm to proceed.\")\n    check_command(\"uvicorn\", \"uvicorn is not installed. Please install uvicorn to proceed.\")\n\n    run_npm_commands()\n\n    try:\n        api_process, ui_process = run_server()\n        while True:\n            try:\n                sleep(30)\n            except KeyboardInterrupt:\n                cleanup(api_process, ui_process)\n    except Exception as e:\n        cleanup(api_process, ui_process)"}
{"type": "source_file", "path": "migrations/versions/2cc1179834b0_agent_executions_modified.py", "content": "\"\"\"agent_executions_modified\n\nRevision ID: 2cc1179834b0\nRevises: 2f97c068fab9\nCreate Date: 2023-06-02 21:01:43.303961\n\n\"\"\"\nfrom alembic import op\nimport sqlalchemy as sa\n\n\n# revision identifiers, used by Alembic.\nrevision = '2cc1179834b0'\ndown_revision = '2f97c068fab9'\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.add_column('agent_executions', sa.Column('calls', sa.Integer(), nullable=True))\n    op.add_column('agent_executions', sa.Column('tokens', sa.Integer(), nullable=True))\n    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_column('agent_executions', 'tokens')\n    op.drop_column('agent_executions', 'calls')\n    # ### end Alembic commands ###\n"}
{"type": "source_file", "path": "migrations/versions/44b0d6f2d1b3_init_models.py", "content": "\"\"\"init models\n\nRevision ID: 44b0d6f2d1b3\nRevises: \nCreate Date: 2023-06-01 11:55:35.195423\n\n\"\"\"\nfrom alembic import op\nimport sqlalchemy as sa\n\n\n# revision identifiers, used by Alembic.\nrevision = '44b0d6f2d1b3'\ndown_revision = None\nbranch_labels = None\ndepends_on = None\n\nfrom sqlalchemy.engine.reflection import Inspector\n\nconn = op.get_bind()\ninspector = Inspector.from_engine(conn)\ntables = inspector.get_table_names()\n\ndef upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    if 'agent_configurations' not in tables:\n        op.create_table('agent_configurations',\n        sa.Column('created_at', sa.DateTime(), nullable=True),\n        sa.Column('updated_at', sa.DateTime(), nullable=True),\n        sa.Column('id', sa.Integer(), autoincrement=True, nullable=False),\n        sa.Column('agent_id', sa.Integer(), nullable=True),\n        sa.Column('key', sa.String(), nullable=True),\n        sa.Column('value', sa.Text(), nullable=True),\n        sa.PrimaryKeyConstraint('id')\n        )\n    if 'agent_execution_feeds' not in tables:\n        op.create_table('agent_execution_feeds',\n        sa.Column('created_at', sa.DateTime(), nullable=True),\n        sa.Column('updated_at', sa.DateTime(), nullable=True),\n        sa.Column('id', sa.Integer(), nullable=False),\n        sa.Column('agent_execution_id', sa.Integer(), nullable=True),\n        sa.Column('agent_id', sa.Integer(), nullable=True),\n        sa.Column('feed', sa.Text(), nullable=True),\n        sa.Column('role', sa.String(), nullable=True),\n        sa.PrimaryKeyConstraint('id')\n        )\n    if 'agent_executions' not in tables:\n        op.create_table('agent_executions',\n        sa.Column('created_at', sa.DateTime(), nullable=True),\n        sa.Column('updated_at', sa.DateTime(), nullable=True),\n        sa.Column('id', sa.Integer(), nullable=False),\n        sa.Column('status', sa.String(), nullable=True),\n        sa.Column('agent_id', sa.Integer(), nullable=True),\n        sa.Column('last_execution_time', sa.DateTime(), nullable=True),\n        sa.PrimaryKeyConstraint('id')\n        )\n    if 'agents' not in tables:\n        op.create_table('agents',\n        sa.Column('created_at', sa.DateTime(), nullable=True),\n        sa.Column('updated_at', sa.DateTime(), nullable=True),\n        sa.Column('id', sa.Integer(), autoincrement=True, nullable=False),\n        sa.Column('name', sa.String(), nullable=True),\n        sa.Column('project_id', sa.Integer(), nullable=True),\n        sa.Column('description', sa.String(), nullable=True),\n        sa.PrimaryKeyConstraint('id')\n        )\n    if 'budgets' not in tables:\n        op.create_table('budgets',\n        sa.Column('created_at', sa.DateTime(), nullable=True),\n        sa.Column('updated_at', sa.DateTime(), nullable=True),\n        sa.Column('id', sa.Integer(), nullable=False),\n        sa.Column('budget', sa.Float(), nullable=True),\n        sa.Column('cycle', sa.String(), nullable=True),\n        sa.PrimaryKeyConstraint('id')\n        )\n    if 'organisations' not in tables:\n        op.create_table('organisations',\n        sa.Column('created_at', sa.DateTime(), nullable=True),\n        sa.Column('updated_at', sa.DateTime(), nullable=True),\n        sa.Column('id', sa.Integer(), nullable=False),\n        sa.Column('name', sa.String(), nullable=True),\n        sa.Column('description', sa.String(), nullable=True),\n        sa.PrimaryKeyConstraint('id')\n        )\n    if 'projects' not in tables:\n        op.create_table('projects',\n        sa.Column('created_at', sa.DateTime(), nullable=True),\n        sa.Column('updated_at', sa.DateTime(), nullable=True),\n        sa.Column('id', sa.Integer(), nullable=False),\n        sa.Column('name', sa.String(), nullable=True),\n        sa.Column('organisation_id', sa.Integer(), nullable=True),\n        sa.Column('description', sa.String(), nullable=True),\n        sa.PrimaryKeyConstraint('id')\n        )\n    if 'tool_configs' not in tables:\n        op.create_table('tool_configs',\n        sa.Column('created_at', sa.DateTime(), nullable=True),\n        sa.Column('updated_at', sa.DateTime(), nullable=True),\n        sa.Column('id', sa.Integer(), nullable=False),\n        sa.Column('name', sa.String(), nullable=True),\n        sa.Column('key', sa.String(), nullable=True),\n        sa.Column('value', sa.String(), nullable=True),\n        sa.Column('agent_id', sa.Integer(), nullable=True),\n        sa.PrimaryKeyConstraint('id')\n        )\n    if 'tools' not in tables:\n        op.create_table('tools',\n        sa.Column('created_at', sa.DateTime(), nullable=True),\n        sa.Column('updated_at', sa.DateTime(), nullable=True),\n        sa.Column('id', sa.Integer(), autoincrement=True, nullable=False),\n        sa.Column('name', sa.String(), nullable=True),\n        sa.Column('folder_name', sa.String(), nullable=True),\n        sa.Column('class_name', sa.String(), nullable=True),\n        sa.Column('file_name', sa.String(), nullable=True),\n        sa.PrimaryKeyConstraint('id')\n        )\n    if 'users' not in tables:\n        op.create_table('users',\n        sa.Column('created_at', sa.DateTime(), nullable=True),\n        sa.Column('updated_at', sa.DateTime(), nullable=True),\n        sa.Column('id', sa.Integer(), autoincrement=True, nullable=False),\n        sa.Column('name', sa.String(), nullable=True),\n        sa.Column('email', sa.String(), nullable=True),\n        sa.Column('password', sa.String(), nullable=True),\n        sa.Column('organisation_id', sa.Integer(), nullable=True),\n        sa.PrimaryKeyConstraint('id'),\n        sa.UniqueConstraint('email')\n        )\n    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_table('users')\n    op.drop_table('tools')\n    op.drop_table('tool_configs')\n    op.drop_table('projects')\n    op.drop_table('organisations')\n    op.drop_table('budgets')\n    op.drop_table('agents')\n    op.drop_table('agent_executions')\n    op.drop_table('agent_execution_feeds')\n    op.drop_table('agent_configurations')\n    # ### end Alembic commands ###\n"}
{"type": "source_file", "path": "superagi/agent/agent_workflow_step_wait_handler.py", "content": "from datetime import datetime\n\nfrom superagi.agent.types.agent_execution_status import AgentExecutionStatus\nfrom superagi.lib.logger import logger\nfrom superagi.models.agent_execution import AgentExecution\nfrom superagi.models.workflows.agent_workflow_step import AgentWorkflowStep\nfrom superagi.models.workflows.agent_workflow_step_wait import AgentWorkflowStepWait\nfrom superagi.agent.types.wait_step_status import AgentWorkflowStepWaitStatus\n\nclass AgentWaitStepHandler:\n    \"\"\"Handle Agent Wait Step in the agent workflow.\"\"\"\n\n    def __init__(self, session, agent_id, agent_execution_id):\n        self.session = session\n        self.agent_id = agent_id\n        self.agent_execution_id = agent_execution_id\n\n    def execute_step(self):\n        \"\"\"Execute the agent wait step.\"\"\"\n\n        logger.info(\"Executing Wait Step\")\n        execution = AgentExecution.get_agent_execution_from_id(self.session, self.agent_execution_id)\n        workflow_step = AgentWorkflowStep.find_by_id(self.session, execution.current_agent_step_id)\n        step_wait = AgentWorkflowStepWait.find_by_id(self.session, workflow_step.action_reference_id)\n        if step_wait is not None:\n            step_wait.wait_begin_time = datetime.now()\n            step_wait.status = AgentWorkflowStepWaitStatus.WAITING.value\n            execution.status = AgentExecutionStatus.WAIT_STEP.value\n\n            self.session.commit()\n\n    def handle_next_step(self):\n        \"\"\"Handle next step of agent workflow in case of wait step.\"\"\"\n\n        execution = AgentExecution.get_agent_execution_from_id(self.session, self.agent_execution_id)\n        workflow_step = AgentWorkflowStep.find_by_id(self.session, execution.current_agent_step_id)\n        step_response = \"default\"\n        next_step = AgentWorkflowStep.fetch_next_step(self.session, workflow_step.id, step_response)\n        if str(next_step) == \"COMPLETE\":\n            agent_execution = AgentExecution.get_agent_execution_from_id(self.session, self.agent_execution_id)\n            agent_execution.current_agent_step_id = -1\n            agent_execution.status = \"COMPLETED\"\n        else:\n            AgentExecution.assign_next_step_id(self.session, self.agent_execution_id, next_step.id)\n        self.session.commit()\n"}
{"type": "source_file", "path": "migrations/versions/ba60b12ae109_create_agent_scheduler.py", "content": "\"\"\"create_agent_scheduler\n\nRevision ID: ba60b12ae109\nRevises: 83424de1347e\nCreate Date: 2023-07-04 10:58:37.991063\n\n\"\"\"\nfrom alembic import op\nimport sqlalchemy as sa\n\n\n# revision identifiers, used by Alembic.\nrevision = 'ba60b12ae109'\ndown_revision = '83424de1347e'\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.create_table('agent_schedule',\n    sa.Column('created_at', sa.DateTime(), nullable=True),\n    sa.Column('updated_at', sa.DateTime(), nullable=True),\n    sa.Column('id', sa.Integer(), nullable=False),\n    sa.Column('agent_id', sa.Integer(), nullable=True),\n    sa.Column('start_time', sa.DateTime(), nullable=True),\n    sa.Column('next_scheduled_time', sa.DateTime(), nullable=True),\n    sa.Column('recurrence_interval', sa.String(), nullable=True),\n    sa.Column('expiry_date', sa.DateTime(), nullable=True),\n    sa.Column('expiry_runs', sa.Integer(), nullable=True),\n    sa.Column('current_runs', sa.Integer(), nullable=True),\n    sa.Column('status', sa.String(), nullable=True),\n    sa.PrimaryKeyConstraint('id')\n    )\n    op.create_index(op.f('ix_agent_schedule_expiry_date'), 'agent_schedule', ['expiry_date'], unique=False)\n    op.create_index(op.f('ix_agent_schedule_status'), 'agent_schedule', ['status'], unique=False)\n    op.create_index(op.f('ix_agent_schedule_agent_id'), 'agent_schedule', ['agent_id'], unique=False)\n    # ### end Alembic commands ###\n\n    \ndef downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_index(op.f('ix_agent_schedule_agent_id'), table_name='agent_schedule')\n    op.drop_index(op.f('ix_agent_schedule_status'), table_name='agent_schedule')\n    op.drop_index(op.f('ix_agent_schedule_expiry_date'), table_name='agent_schedule')\n    op.drop_table('agent_schedule')\n    # ### end Alembic commands ###\n"}
{"type": "source_file", "path": "superagi/__init__.py", "content": ""}
{"type": "source_file", "path": "superagi/agent/output_parser.py", "content": "import json\nfrom abc import ABC, abstractmethod\nfrom typing import Dict, NamedTuple, List\nimport re\nimport ast\nimport json\nfrom superagi.helper.json_cleaner import JsonCleaner\nfrom superagi.lib.logger import logger\n\n\nclass AgentGPTAction(NamedTuple):\n    name: str\n    args: Dict\n\n\nclass AgentTasks(NamedTuple):\n    tasks: List[str] = []\n    error: str = \"\"\n\n\nclass BaseOutputParser(ABC):\n    @abstractmethod\n    def parse(self, text: str) -> AgentGPTAction:\n        \"\"\"Return AgentGPTAction\"\"\"\n\n\nclass AgentSchemaOutputParser(BaseOutputParser):\n    \"\"\"Parses the output from the agent schema\"\"\"\n    def parse(self, response: str) -> AgentGPTAction:\n        if response.startswith(\"```\") and response.endswith(\"```\"):\n            response = \"```\".join(response.split(\"```\")[1:-1])\n        response = JsonCleaner.extract_json_section(response)\n        # ast throws error if true/false params passed in json\n        response = JsonCleaner.clean_boolean(response)\n\n        # OpenAI returns `str(content_dict)`, literal_eval reverses this\n        try:\n            logger.debug(\"AgentSchemaOutputParser: \", response)\n            response_obj = ast.literal_eval(response)\n            args = response_obj['tool']['args'] if 'args' in response_obj['tool'] else {}\n            return AgentGPTAction(\n                name=response_obj['tool']['name'],\n                args=args,\n            )\n        except BaseException as e:\n            logger.info(f\"AgentSchemaOutputParser: Error parsing JSON response {e}\")\n            raise e\n\n\nclass AgentSchemaToolOutputParser(BaseOutputParser):\n    \"\"\"Parses the output from the agent schema for the tool\"\"\"\n    def parse(self, response: str) -> AgentGPTAction:\n        if response.startswith(\"```\") and response.endswith(\"```\"):\n            response = \"```\".join(response.split(\"```\")[1:-1])\n        response = JsonCleaner.extract_json_section(response)\n        # ast throws error if true/false params passed in json\n        response = JsonCleaner.clean_boolean(response)\n\n        # OpenAI returns `str(content_dict)`, literal_eval reverses this\n        try:\n            logger.debug(\"AgentSchemaOutputParser: \", response)\n            response_obj = ast.literal_eval(response)\n            args = response_obj['args'] if 'args' in response_obj else {}\n            return AgentGPTAction(\n                name=response_obj['name'],\n                args=args,\n            )\n        except BaseException as e:\n            logger.info(f\"AgentSchemaToolOutputParser: Error parsing JSON response {e}\")\n            raise e\n"}
{"type": "source_file", "path": "migrations/versions/83424de1347e_added_agent_execution_config.py", "content": "\"\"\"added_agent_execution_config\n\nRevision ID: 83424de1347e\nRevises: c02f3d759bf3\nCreate Date: 2023-07-03 22:42:50.091762\n\n\"\"\"\nfrom alembic import op\nimport sqlalchemy as sa\n\n\n# revision identifiers, used by Alembic.\nrevision = '83424de1347e'\ndown_revision = 'c02f3d759bf3'\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.create_table('agent_execution_configs',\n    sa.Column('id', sa.Integer(), nullable=False),\n    sa.Column('agent_execution_id', sa.Integer(), nullable=True),\n    sa.Column('key', sa.String(), nullable=True),\n    sa.Column('value', sa.Text(), nullable=True),\n    sa.Column('created_at', sa.DateTime(), nullable=True),\n    sa.Column('updated_at', sa.DateTime(), nullable=True),\n    sa.PrimaryKeyConstraint('id')\n    )\n    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_table('agent_execution_configs')\n    # ### end Alembic commands ###\n"}
{"type": "source_file", "path": "migrations/versions/35e47f20475b_renamed_tokens_calls.py", "content": "\"\"\"renamed_tokens_calls\n\nRevision ID: 35e47f20475b\nRevises: 598cfb37292a\nCreate Date: 2023-06-06 04:34:15.101672\n\n\"\"\"\nfrom alembic import op\nimport sqlalchemy as sa\n\n\n# revision identifiers, used by Alembic.\nrevision = '35e47f20475b'\ndown_revision = '598cfb37292a'\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.add_column('agent_executions', sa.Column('num_of_calls', sa.Integer(), nullable=True))\n    op.add_column('agent_executions', sa.Column('num_of_tokens', sa.Integer(), nullable=True))\n    op.drop_column('agent_executions', 'calls')\n    op.drop_column('agent_executions', 'tokens')\n    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.add_column('agent_executions', sa.Column('tokens', sa.INTEGER(), autoincrement=False, nullable=True))\n    op.add_column('agent_executions', sa.Column('calls', sa.INTEGER(), autoincrement=False, nullable=True))\n    op.drop_column('agent_executions', 'num_of_tokens')\n    op.drop_column('agent_executions', 'num_of_calls')\n    # ### end Alembic commands ###\n"}
{"type": "source_file", "path": "migrations/versions/467e85d5e1cd_updated_resources_added_exec_id.py", "content": "\"\"\"updated_resources_added_exec_id\n\nRevision ID: 467e85d5e1cd\nRevises: ba60b12ae109\nCreate Date: 2023-07-10 08:54:46.702652\n\n\"\"\"\nfrom alembic import op\nimport sqlalchemy as sa\n\n\n# revision identifiers, used by Alembic.\nrevision = '467e85d5e1cd'\ndown_revision = 'ba60b12ae109'\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.add_column('resources', sa.Column('agent_execution_id', sa.Integer(), nullable=True))\n    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_column('resources', 'agent_execution_id')\n    # ### end Alembic commands ###\n"}
{"type": "source_file", "path": "migrations/env.py", "content": "from logging.config import fileConfig\n\nfrom sqlalchemy import engine_from_config\nfrom sqlalchemy import pool\nfrom alembic import context\nfrom urllib.parse import urlparse\n\n# this is the Alembic Config object, which provides\n# access to the values within the .ini file in use.\nconfig = context.config\n\n# Interpret the config file for Python logging.\n# This line sets up loggers basically.\nif config.config_file_name is not None:\n    fileConfig(config.config_file_name)\n\n# add your model's MetaData object here\n# for 'autogenerate' support\n# from myapp import mymodel\n# target_metadata = mymodel.Base.metadata\nfrom superagi.models.base_model import DBBaseModel\ntarget_metadata = DBBaseModel.metadata\nfrom superagi.models import *\nfrom superagi.config.config import get_config\n\n# other values from the config, defined by the needs of env.py,\n# can be acquired:\n# my_important_option = config.get_main_option(\"my_important_option\")\n# ... etc.\n\ndb_host = get_config('DB_HOST', 'super__postgres')\ndb_username = get_config('DB_USERNAME')\ndb_password = get_config('DB_PASSWORD')\ndb_name = get_config('DB_NAME')\ndatabase_url = get_config('DB_URL', None)\n\ndef run_migrations_offline() -> None:\n    \"\"\"Run migrations in 'offline' mode.\n\n    This configures the context with just a URL\n    and not an Engine, though an Engine is acceptable\n    here as well.  By skipping the Engine creation\n    we don't even need a DBAPI to be available.\n\n    Calls to context.execute() here emit the given string to the\n    script output.\n\n    \"\"\"\n\n    db_url = database_url\n    if db_url is None:\n        if db_username is None:\n            db_url = f'postgresql://{db_host}/{db_name}'\n        else:\n            db_url = f'postgresql://{db_username}:{db_password}@{db_host}/{db_name}'\n    else:\n        db_url = urlparse(db_url)\n        db_url = db_url.scheme + \"://\" + db_url.netloc + db_url.path\n\n    config.set_main_option(\"sqlalchemy.url\", db_url)\n\n    url = config.get_main_option(\"sqlalchemy.url\")\n    context.configure(\n        url=url,\n        target_metadata=target_metadata,\n        literal_binds=True,\n        dialect_opts={\"paramstyle\": \"named\"},\n    )\n\n    with context.begin_transaction():\n        context.run_migrations()\n\n\ndef run_migrations_online() -> None:\n    \"\"\"Run migrations in 'online' mode.\n\n    In this scenario we need to create an Engine\n    and associate a connection with the context.\n\n    \"\"\"\n\n    db_host = get_config('DB_HOST', 'super__postgres')\n    db_username = get_config('DB_USERNAME')\n    db_password = get_config('DB_PASSWORD')\n    db_name = get_config('DB_NAME')\n    db_url = get_config('DB_URL', None)\n\n    if db_url is None:\n        if db_username is None:\n            db_url = f'postgresql://{db_host}/{db_name}'\n        else:\n            db_url = f'postgresql://{db_username}:{db_password}@{db_host}/{db_name}'\n    else:\n        db_url = urlparse(db_url)\n        db_url = db_url.scheme + \"://\" + db_url.netloc + db_url.path\n        \n    config.set_main_option('sqlalchemy.url', db_url)\n    connectable = engine_from_config(\n        config.get_section(config.config_ini_section, {}),\n        prefix=\"sqlalchemy.\",\n        poolclass=pool.NullPool,\n    )\n\n    with connectable.connect() as connection:\n        context.configure(\n            connection=connection, target_metadata=target_metadata\n        )\n\n        with context.begin_transaction():\n            context.run_migrations()\n\n\nif context.is_offline_mode():\n    run_migrations_offline()\nelse:\n    run_migrations_online()\n"}
{"type": "source_file", "path": "migrations/versions/a91808a89623_added_resources.py", "content": "\"\"\"added resources\n\nRevision ID: a91808a89623\nRevises: 44b0d6f2d1b3\nCreate Date: 2023-06-01 07:00:33.982485\n\n\"\"\"\nfrom alembic import op\nimport sqlalchemy as sa\n\n\n# revision identifiers, used by Alembic.\nrevision = 'a91808a89623'\ndown_revision = '44b0d6f2d1b3'\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.create_table('resources',\n    sa.Column('created_at', sa.DateTime(), nullable=True),\n    sa.Column('updated_at', sa.DateTime(), nullable=True),\n    sa.Column('id', sa.Integer(), nullable=False),\n    sa.Column('name', sa.String(), nullable=True),\n    sa.Column('storage_type', sa.String(), nullable=True),\n    sa.Column('path', sa.String(), nullable=True),\n    sa.Column('size', sa.Integer(), nullable=True),\n    sa.Column('type', sa.String(), nullable=True),\n    sa.Column('channel', sa.String(), nullable=True),\n    sa.Column('project_id', sa.Integer(), nullable=True),\n    sa.PrimaryKeyConstraint('id')\n    )\n    op.add_column('agent_execution_feeds', sa.Column('extra_info', sa.String(), nullable=True))\n    op.add_column('agent_executions', sa.Column('name', sa.String(), nullable=True))\n    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_column('agent_executions', 'name')\n    op.drop_column('agent_execution_feeds', 'extra_info')\n    op.drop_table('resources')\n    # ### end Alembic commands ###\n"}
