{
  "repo_name": "deepfakes/faceswap",
  "repo_url": "https://github.com/deepfakes/faceswap",
  "description": "Deepfakes Software For All",
  "stars": 53487,
  "language": "Python",
  "created_at": "2017-12-19T09:44:13Z",
  "updated_at": "2025-03-19T06:12:01Z",
  "files": {
    "tests/lib/gpu_stats/_base_test.py": "#!/usr/bin python3\n\"\"\" Pytest unit tests for :mod:`lib.gpu_stats._base` \"\"\"\nimport typing as T\n\nfrom dataclasses import dataclass\nfrom unittest.mock import MagicMock\n\nimport pytest\nimport pytest_mock\n\n# pylint:disable=protected-access\nfrom lib.gpu_stats import _base\nfrom lib.gpu_stats._base import BiggestGPUInfo, GPUInfo, _GPUStats, set_exclude_devices\nfrom lib.utils import get_backend\n\n\ndef test_set_exclude_devices(monkeypatch: pytest.MonkeyPatch) -> None:\n    \"\"\" Test that :func:`~lib.gpu_stats._base.set_exclude_devices` adds devices\n\n    Parameters\n    ----------\n    monkeypatch: :class:`pytest.MonkeyPatch`\n        Monkey patching _EXCLUDE_DEVICES\n    \"\"\"\n    monkeypatch.setattr(_base, \"_EXCLUDE_DEVICES\", [])\n    assert not _base._EXCLUDE_DEVICES\n    set_exclude_devices([0, 1])\n    assert _base._EXCLUDE_DEVICES == [0, 1]\n\n\n@dataclass\nclass _DummyData:\n    \"\"\" Dummy data for initializing and testing :class:`~lib.gpu_stats._base._GPUStats` \"\"\"\n    device_count = 2\n    active_devices = [0, 1]\n    handles = [0, 1]\n    driver = \"test_driver\"\n    device_names = ['test_device_0', 'test_device_1']\n    vram = [1024, 2048]\n    free_vram = [512, 1024]\n\n\n@pytest.fixture(name=\"gpu_stats_instance\")\ndef fixture__gpu_stats_instance(mocker: pytest_mock.MockerFixture) -> _GPUStats:\n    \"\"\" Create a fixture of the :class:`~lib.gpu_stats._base._GPUStats` object\n\n    Parameters\n    ----------\n    mocker: :class:`pytest_mock.MockerFixture`\n        Mocker for dummying in function calls\n    \"\"\"\n    mocker.patch.object(_GPUStats, '_initialize')\n    mocker.patch.object(_GPUStats, '_shutdown')\n    mocker.patch.object(_GPUStats, '_get_device_count', return_value=_DummyData.device_count)\n    mocker.patch.object(_GPUStats, '_get_active_devices', return_value=_DummyData.active_devices)\n    mocker.patch.object(_GPUStats, '_get_handles', return_value=_DummyData.handles)\n    mocker.patch.object(_GPUStats, '_get_driver', return_value=_DummyData.driver)\n    mocker.patch.object(_GPUStats, '_get_device_names', return_value=_DummyData.device_names)\n    mocker.patch.object(_GPUStats, '_get_vram', return_value=_DummyData.vram)\n    mocker.patch.object(_GPUStats, '_get_free_vram', return_value=_DummyData.free_vram)\n    gpu_stats = _GPUStats()\n    return gpu_stats\n\n\ndef test__gpu_stats_init_(gpu_stats_instance: _GPUStats) -> None:\n    \"\"\" Test that the base :class:`~lib.gpu_stats._base._GPUStats` class initializes correctly\n\n    Parameters\n    ----------\n    gpu_stats_instance: :class:`_GPUStats`\n        Fixture instance of the _GPUStats base class\n    \"\"\"\n    # Ensure that the object is initialized and shutdown correctly\n    assert gpu_stats_instance._is_initialized is False\n    assert T.cast(MagicMock, gpu_stats_instance._initialize).call_count == 1\n    assert T.cast(MagicMock, gpu_stats_instance._shutdown).call_count == 1\n\n    # Ensure that the object correctly gets and stores the device count, active devices,\n    # handles, driver, device names, and VRAM information\n    assert gpu_stats_instance.device_count == _DummyData.device_count\n    assert gpu_stats_instance._active_devices == _DummyData.active_devices\n    assert gpu_stats_instance._handles == _DummyData.handles\n    assert gpu_stats_instance._driver == _DummyData.driver\n    assert gpu_stats_instance._device_names == _DummyData.device_names\n    assert gpu_stats_instance._vram == _DummyData.vram\n\n\ndef test__gpu_stats_properties(gpu_stats_instance: _GPUStats) -> None:\n    \"\"\" Test that the :class:`~lib.gpu_stats._base._GPUStats` properties are set and formatted\n    correctly.\n\n    Parameters\n    ----------\n    gpu_stats_instance: :class:`_GPUStats`\n        Fixture instance of the _GPUStats base class\n    \"\"\"\n    assert gpu_stats_instance.cli_devices == ['0: test_device_0', '1: test_device_1']\n    assert gpu_stats_instance.sys_info == GPUInfo(vram=_DummyData.vram,\n                                                  vram_free=_DummyData.free_vram,\n                                                  driver=_DummyData.driver,\n                                                  devices=_DummyData.device_names,\n                                                  devices_active=_DummyData.active_devices)\n\n\ndef test__gpu_stats_get_card_most_free(mocker: pytest_mock.MockerFixture,\n                                       gpu_stats_instance: _GPUStats) -> None:\n    \"\"\" Confirm that :func:`ib.gpu_stats._base._GPUStats.get_card_most_free` functions\n    correctly\n\n    Parameters\n    ----------\n    mocker: :class:`pytest_mock.MockerFixture`\n        Mocker for dummying in function calls\n    gpu_stats_instance: :class:`_GPUStats`\n        Fixture instance of the _GPUStats base class\n    \"\"\"\n    assert gpu_stats_instance.get_card_most_free() == BiggestGPUInfo(card_id=1,\n                                                                     device='test_device_1',\n                                                                     free=1024,\n                                                                     total=2048)\n    mocker.patch.object(_GPUStats, '_get_active_devices', return_value=[])\n    gpu_stats = _GPUStats()\n    assert gpu_stats.get_card_most_free() == BiggestGPUInfo(card_id=-1,\n                                                            device='No GPU devices found',\n                                                            free=2048,\n                                                            total=2048)\n\n\ndef test__gpu_stats_exclude_all_devices(gpu_stats_instance: _GPUStats) -> None:\n    \"\"\" Ensure that the object correctly returns whether all devices are excluded\n\n    Parameters\n    ----------\n    gpu_stats_instance: :class:`_GPUStats`\n        Fixture instance of the _GPUStats base class\n    \"\"\"\n    assert gpu_stats_instance.exclude_all_devices is False\n    set_exclude_devices([0, 1])\n    assert gpu_stats_instance.exclude_all_devices is True\n\n\ndef test__gpu_stats_no_active_devices(\n        caplog: pytest.LogCaptureFixture,\n        gpu_stats_instance: _GPUStats,  # pylint:disable=unused-argument\n        mocker: pytest_mock.MockerFixture) -> None:\n    \"\"\" Ensure that no active GPUs raises a warning when not in CPU mode\n\n    Parameters\n    ----------\n    caplog: :class:`pytest.LogCaptureFixture`\n        Pytest's log capturing fixture\n    gpu_stats_instance: :class:`_GPUStats`\n        Fixture instance of the _GPUStats base class\n    mocker: :class:`pytest_mock.MockerFixture`\n        Mocker for dummying in function calls\n    \"\"\"\n    if get_backend() == \"cpu\":\n        return\n    caplog.set_level(\"WARNING\")\n    mocker.patch.object(_GPUStats, '_get_active_devices', return_value=[])\n    _GPUStats()\n    assert \"No GPU detected\" in caplog.messages\n",
    "tests/lib/gui/stats/event_reader_test.py": "#!/usr/bin python3\n\"\"\" Pytest unit tests for :mod:`lib.gui.stats.event_reader` \"\"\"\n# pylint:disable=protected-access\nfrom __future__ import annotations\nimport json\nimport os\nimport typing as T\n\nfrom shutil import rmtree\nfrom time import time\nfrom unittest.mock import MagicMock\n\nimport numpy as np\nimport pytest\nimport pytest_mock\n\nimport tensorflow as tf\nfrom tensorflow.core.util import event_pb2  # pylint:disable=no-name-in-module\n\nfrom lib.gui.analysis.event_reader import (_Cache, _CacheData, _EventParser,\n                                           _LogFiles, EventData, TensorBoardLogs)\n\nif T.TYPE_CHECKING:\n    from collections.abc import Iterator\n\n\ndef test__logfiles(tmp_path: str):\n    \"\"\" Test the _LogFiles class operates correctly\n\n    Parameters\n    ----------\n    tmp_path: :class:`pathlib.Path`\n    \"\"\"\n    # dummy logfiles + junk data\n    sess_1 = os.path.join(tmp_path, \"session_1\", \"train\")\n    sess_2 = os.path.join(tmp_path, \"session_2\", \"train\")\n    os.makedirs(sess_1)\n    os.makedirs(sess_2)\n\n    test_log_1 = os.path.join(sess_1, \"events.out.tfevents.123.456.v2\")\n    test_log_2 = os.path.join(sess_2, \"events.out.tfevents.789.012.v2\")\n    test_log_junk = os.path.join(sess_2, \"test_file.txt\")\n\n    for fname in (test_log_1, test_log_2, test_log_junk):\n        with open(fname, \"a\", encoding=\"utf-8\"):\n            pass\n\n    log_files = _LogFiles(tmp_path)\n    # Test all correct\n    assert isinstance(log_files._filenames, dict)\n    assert len(log_files._filenames) == 2\n    assert log_files._filenames == {1: test_log_1, 2: test_log_2}\n\n    assert log_files.session_ids == [1, 2]\n\n    assert log_files.get(1) == test_log_1\n    assert log_files.get(2) == test_log_2\n\n    # Remove a file, refresh and check again\n    rmtree(sess_1)\n    log_files.refresh()\n    assert log_files._filenames == {2: test_log_2}\n    assert log_files.get(2) == test_log_2\n    assert log_files.get(3) == \"\"\n\n\ndef test__cachedata():\n    \"\"\" Test the _CacheData class operates correctly \"\"\"\n    labels = [\"label_a\", \"label_b\"]\n    timestamps = np.array([1.23, 4.56], dtype=\"float64\")\n    loss = np.array([[2.34, 5.67], [3.45, 6.78]], dtype=\"float32\")\n\n    # Initial test\n    cache = _CacheData(labels, timestamps, loss)\n    assert cache.labels == labels\n    assert cache._timestamps_shape == timestamps.shape\n    assert cache._loss_shape == loss.shape\n    np.testing.assert_array_equal(cache.timestamps, timestamps)\n    np.testing.assert_array_equal(cache.loss, loss)\n\n    # Add data test\n    new_timestamps = np.array([2.34, 6.78], dtype=\"float64\")\n    new_loss = np.array([[3.45, 7.89], [8.90, 1.23]], dtype=\"float32\")\n\n    expected_timestamps = np.concatenate([timestamps, new_timestamps])\n    expected_loss = np.concatenate([loss, new_loss])\n\n    cache.add_live_data(new_timestamps, new_loss)\n    assert cache.labels == labels\n    assert cache._timestamps_shape == expected_timestamps.shape\n    assert cache._loss_shape == expected_loss.shape\n    np.testing.assert_array_equal(cache.timestamps, expected_timestamps)\n    np.testing.assert_array_equal(cache.loss, expected_loss)\n\n\n# _Cache tests\nclass Test_Cache:  # pylint:disable=invalid-name\n    \"\"\" Test that :class:`lib.gui.analysis.event_reader._Cache` works correctly \"\"\"\n    @staticmethod\n    def test_init() -> None:\n        \"\"\" Test __init__ \"\"\"\n        cache = _Cache()\n        assert isinstance(cache._data, dict)\n        assert isinstance(cache._carry_over, dict)\n        assert isinstance(cache._loss_labels, list)\n        assert not cache._data\n        assert not cache._carry_over\n        assert not cache._loss_labels\n\n    @staticmethod\n    def test_is_cached() -> None:\n        \"\"\" Test is_cached function works \"\"\"\n        cache = _Cache()\n\n        data = _CacheData([\"test_1\", \"test_2\"],\n                          np.array([1.23, ], dtype=\"float64\"),\n                          np.array([[2.34, ], [4.56]], dtype=\"float32\"))\n        cache._data[1] = data\n        assert cache.is_cached(1)\n        assert not cache.is_cached(2)\n\n    @staticmethod\n    def test_cache_data(mocker: pytest_mock.MockerFixture) -> None:\n        \"\"\" Test cache_data function works\n\n        Parameters\n        ----------\n        mocker: :class:`pytest_mock.MockerFixture`\n            Mocker for checking full_info called from _SysInfo\n        \"\"\"\n        cache = _Cache()\n\n        session_id = 1\n        data = {1: EventData(4., [1., 2.]), 2: EventData(5., [3., 4.])}\n        labels = ['label1', 'label2']\n        is_live = False\n\n        cache.cache_data(session_id, data, labels, is_live)\n        assert cache._loss_labels == labels\n        assert cache.is_cached(session_id)\n        np.testing.assert_array_equal(cache._data[session_id].timestamps, np.array([4., 5.]))\n        np.testing.assert_array_equal(cache._data[session_id].loss, np.array([[1., 2.], [3., 4.]]))\n\n        add_live = mocker.patch(\"lib.gui.analysis.event_reader._Cache._add_latest_live\")\n        is_live = True\n        cache.cache_data(session_id, data, labels, is_live)\n        assert add_live.called\n\n    @staticmethod\n    def test__to_numpy() -> None:\n        \"\"\" Test _to_numpy function works \"\"\"\n        cache = _Cache()\n        cache._loss_labels = ['label1', 'label2']\n        data = {1: EventData(4., [1., 2.]), 2: EventData(5., [3., 4.])}\n\n        # Non-live\n        is_live = False\n        times, loss = cache._to_numpy(data, is_live)\n        np.testing.assert_array_equal(times, np.array([4., 5.]))\n        np.testing.assert_array_equal(loss, np.array([[1., 2.], [3., 4.]]))\n\n        # Correctly collected live\n        is_live = True\n        times, loss = cache._to_numpy(data, is_live)\n        np.testing.assert_array_equal(times, np.array([4., 5.]))\n        np.testing.assert_array_equal(loss, np.array([[1., 2.], [3., 4.]]))\n\n        # Incorrectly collected live\n        live_data = {1: EventData(4., [1., 2.]),\n                     2: EventData(5., [3.]),\n                     3: EventData(6., [4., 5., 6.])}\n        times, loss = cache._to_numpy(live_data, is_live)\n        np.testing.assert_array_equal(times, np.array([4.]))\n        np.testing.assert_array_equal(loss, np.array([[1., 2.]]))\n\n    @staticmethod\n    def test__collect_carry_over() -> None:\n        \"\"\" Test _collect_carry_over function works \"\"\"\n        data = {1: EventData(3., [4., 5.]), 2: EventData(6., [7., 8.])}\n        carry_over = {1: EventData(3., [2., 3.])}\n        expected = {1: EventData(3., [2., 3., 4., 5.]), 2: EventData(6., [7., 8.])}\n\n        cache = _Cache()\n        cache._carry_over = carry_over\n        cache._collect_carry_over(data)\n        assert data == expected\n\n    @staticmethod\n    def test__process_data() -> None:\n        \"\"\" Test _process_data function works \"\"\"\n        cache = _Cache()\n        cache._loss_labels = ['label1', 'label2']\n\n        data = {1: EventData(4., [5., 6.]),\n                2: EventData(5., [7., 8.]),\n                3: EventData(6., [9.])}\n        is_live = False\n        expected_timestamps = np.array([4., 5.])\n        expected_loss = np.array([[5., 6.], [7., 8.]])\n        expected_carry_over = {3: EventData(6., [9.])}\n\n        timestamps, loss = cache._process_data(data, is_live)\n        np.testing.assert_array_equal(timestamps, expected_timestamps)\n        np.testing.assert_array_equal(loss, expected_loss)\n        assert not cache._carry_over\n\n        is_live = True\n        timestamps, loss = cache._process_data(data, is_live)\n        np.testing.assert_array_equal(timestamps, expected_timestamps)\n        np.testing.assert_array_equal(loss, expected_loss)\n        assert cache._carry_over == expected_carry_over\n\n    @staticmethod\n    def test__add_latest_live() -> None:\n        \"\"\" Test _add_latest_live function works \"\"\"\n        session_id = 1\n        labels = ['label1', 'label2']\n        data = {1: EventData(3., [5., 6.]), 2: EventData(4., [7., 8.])}\n        new_timestamp = np.array([5.], dtype=\"float64\")\n        new_loss = np.array([[8., 9.]], dtype=\"float32\")\n        expected_timestamps = np.array([3., 4., 5.])\n        expected_loss = np.array([[5., 6.], [7., 8.], [8., 9.]])\n\n        cache = _Cache()\n        cache.cache_data(session_id, data, labels)  # Initial data\n        cache._add_latest_live(session_id, new_loss, new_timestamp)\n\n        assert cache.is_cached(session_id)\n        assert cache._loss_labels == labels\n        np.testing.assert_array_equal(cache._data[session_id].timestamps, expected_timestamps)\n        np.testing.assert_array_equal(cache._data[session_id].loss, expected_loss)\n\n    @staticmethod\n    def test_get_data() -> None:\n        \"\"\" Test get_data function works \"\"\"\n        session_id = 1\n\n        cache = _Cache()\n        assert cache.get_data(session_id, \"loss\") is None\n        assert cache.get_data(session_id, \"timestamps\") is None\n\n        labels = ['label1', 'label2']\n        data = {1: EventData(3., [5., 6.]), 2: EventData(4., [7., 8.])}\n        expected_timestamps = np.array([3., 4.])\n        expected_loss = np.array([[5., 6.], [7., 8.]])\n\n        cache.cache_data(session_id, data, labels, is_live=False)\n        get_timestamps = cache.get_data(session_id, \"timestamps\")\n        get_loss = cache.get_data(session_id, \"loss\")\n\n        assert isinstance(get_timestamps, dict)\n        assert len(get_timestamps) == 1\n        assert list(get_timestamps) == [session_id]\n        result = get_timestamps[session_id]\n        assert list(result) == [\"timestamps\"]\n        np.testing.assert_array_equal(result[\"timestamps\"], expected_timestamps)\n\n        assert isinstance(get_loss, dict)\n        assert len(get_loss) == 1\n        assert list(get_loss) == [session_id]\n        result = get_loss[session_id]\n        assert list(result) == [\"loss\", \"labels\"]\n        np.testing.assert_array_equal(result[\"loss\"], expected_loss)\n\n\n# TensorBoardLogs\nclass TestTensorBoardLogs:\n    \"\"\" Test that :class:`lib.gui.analysis.event_reader.TensorBoardLogs` works correctly \"\"\"\n\n    @pytest.fixture(name=\"tensorboardlogs_instance\")\n    def tensorboardlogs_fixture(self,\n                                tmp_path: str,\n                                request: pytest.FixtureRequest) -> TensorBoardLogs:\n        \"\"\" Pytest fixture for :class:`lib.gui.analysis.event_reader.TensorBoardLogs`\n\n        Parameters\n        ----------\n        tmp_path: :class:`pathlib.Path`\n            Temporary folder for dummy data\n\n        Returns\n        -------\n        :class::class:`lib.gui.analysis.event_reader.TensorBoardLogs`\n            The class instance for testing\n        \"\"\"\n        sess_1 = os.path.join(tmp_path, \"session_1\", \"train\")\n        sess_2 = os.path.join(tmp_path, \"session_2\", \"train\")\n        os.makedirs(sess_1)\n        os.makedirs(sess_2)\n\n        test_log_1 = os.path.join(sess_1, \"events.out.tfevents.123.456.v2\")\n        test_log_2 = os.path.join(sess_2, \"events.out.tfevents.789.012.v2\")\n\n        for fname in (test_log_1, test_log_2):\n            with open(fname, \"a\", encoding=\"utf-8\"):\n                pass\n\n        tblogs_instance = TensorBoardLogs(tmp_path, False)\n\n        def teardown():\n            rmtree(tmp_path)\n\n        request.addfinalizer(teardown)\n        return tblogs_instance\n\n    @staticmethod\n    def test_init(tensorboardlogs_instance: TensorBoardLogs) -> None:\n        \"\"\" Test __init__ works correctly\n\n        Parameters\n        ----------\n        tensorboadlogs_instance: :class:`lib.gui.analysis.event_reader.TensorBoardLogs`\n            The class instance to test\n        \"\"\"\n        tb_logs = tensorboardlogs_instance\n        assert isinstance(tb_logs._log_files, _LogFiles)\n        assert isinstance(tb_logs._cache, _Cache)\n        assert not tb_logs._is_training\n\n        is_training = True\n        folder = tb_logs._log_files._logs_folder\n        tb_logs = TensorBoardLogs(folder, is_training)\n        assert tb_logs._is_training\n\n    @staticmethod\n    def test_session_ids(tensorboardlogs_instance: TensorBoardLogs) -> None:\n        \"\"\" Test session_ids property works correctly\n\n        Parameters\n        ----------\n        tensorboadlogs_instance: :class:`lib.gui.analysis.event_reader.TensorBoardLogs`\n            The class instance to test\n        \"\"\"\n        tb_logs = tensorboardlogs_instance\n        assert tb_logs.session_ids == [1, 2]\n\n    @staticmethod\n    def test_set_training(tensorboardlogs_instance: TensorBoardLogs) -> None:\n        \"\"\" Test set_training works correctly\n\n        Parameters\n        ----------\n        tensorboadlogs_instance: :class:`lib.gui.analysis.event_reader.TensorBoardLogs`\n            The class instance to test\n        \"\"\"\n        tb_logs = tensorboardlogs_instance\n        assert not tb_logs._is_training\n        assert tb_logs._training_iterator is None\n        tb_logs.set_training(True)\n        assert tb_logs._is_training\n        assert tb_logs._training_iterator is not None\n        tb_logs.set_training(False)\n        assert not tb_logs._is_training\n        assert tb_logs._training_iterator is None\n\n    @staticmethod\n    def test__cache_data(tensorboardlogs_instance: TensorBoardLogs,\n                         mocker: pytest_mock.MockerFixture) -> None:\n        \"\"\" Test _cache_data works correctly\n\n        Parameters\n        ----------\n        tensorboadlogs_instance: :class:`lib.gui.analysis.event_reader.TensorBoardLogs`\n            The class instance to test\n        mocker: :class:`pytest_mock.MockerFixture`\n            Mocker for checking event parser caching is called\n        \"\"\"\n        tb_logs = tensorboardlogs_instance\n        session_id = 1\n        cacher = mocker.patch(\"lib.gui.analysis.event_reader._EventParser.cache_events\")\n        tb_logs._cache_data(session_id)\n        assert cacher.called\n        cacher.reset_mock()\n\n        tb_logs.set_training(True)\n        tb_logs._cache_data(session_id)\n        assert cacher.called\n\n    @staticmethod\n    def test__check_cache(tensorboardlogs_instance: TensorBoardLogs,\n                          mocker: pytest_mock.MockerFixture) -> None:\n        \"\"\" Test _check_cache works correctly\n\n        Parameters\n        ----------\n        tensorboadlogs_instance: :class:`lib.gui.analysis.event_reader.TensorBoardLogs`\n            The class instance to test\n        mocker: :class:`pytest_mock.MockerFixture`\n            Mocker for checking _cache_data is called\n        \"\"\"\n        is_cached = mocker.patch(\"lib.gui.analysis.event_reader._Cache.is_cached\")\n        cache_data = mocker.patch(\"lib.gui.analysis.event_reader.TensorBoardLogs._cache_data\")\n        tb_logs = tensorboardlogs_instance\n\n        # Session ID not training\n        is_cached.return_value = False\n        tb_logs._check_cache(1)\n        assert is_cached.called\n        assert cache_data.called\n        is_cached.reset_mock()\n        cache_data.reset_mock()\n\n        is_cached.return_value = True\n        tb_logs._check_cache(1)\n        assert is_cached.called\n        assert not cache_data.called\n        is_cached.reset_mock()\n        cache_data.reset_mock()\n\n        # Session ID and training\n        tb_logs.set_training(True)\n        tb_logs._check_cache(1)\n        assert not cache_data.called\n        cache_data.reset_mock()\n\n        tb_logs._check_cache(2)\n        assert cache_data.called\n        cache_data.reset_mock()\n\n        # No session id\n        tb_logs.set_training(False)\n        is_cached.return_value = False\n\n        tb_logs._check_cache(None)\n        assert is_cached.called\n        assert cache_data.called\n        is_cached.reset_mock()\n        cache_data.reset_mock()\n\n        is_cached.return_value = True\n        tb_logs._check_cache(None)\n        assert is_cached.called\n        assert not cache_data.called\n        is_cached.reset_mock()\n        cache_data.reset_mock()\n\n    @staticmethod\n    def test_get_loss(tensorboardlogs_instance: TensorBoardLogs,\n                      mocker: pytest_mock.MockerFixture) -> None:\n        \"\"\" Test get_loss works correctly\n\n        Parameters\n        ----------\n        tensorboadlogs_instance: :class:`lib.gui.analysis.event_reader.TensorBoardLogs`\n            The class instance to test\n        mocker: :class:`pytest_mock.MockerFixture`\n            Mocker for checking _cache_data is called\n        \"\"\"\n        tb_logs = tensorboardlogs_instance\n\n        with pytest.raises(tf.errors.NotFoundError):  # Invalid session id\n            tb_logs.get_loss(3)\n\n        check_cache = mocker.patch(\"lib.gui.analysis.event_reader.TensorBoardLogs._check_cache\")\n        get_data = mocker.patch(\"lib.gui.analysis.event_reader._Cache.get_data\")\n        get_data.return_value = None\n\n        assert isinstance(tb_logs.get_loss(None), dict)\n        assert check_cache.call_count == 2\n        assert get_data.call_count == 2\n        check_cache.reset_mock()\n        get_data.reset_mock()\n\n        assert isinstance(tb_logs.get_loss(1), dict)\n        assert check_cache.call_count == 1\n        assert get_data.call_count == 1\n        check_cache.reset_mock()\n        get_data.reset_mock()\n\n    @staticmethod\n    def test_get_timestamps(tensorboardlogs_instance: TensorBoardLogs,\n                            mocker: pytest_mock.MockerFixture) -> None:\n        \"\"\" Test get_timestamps works correctly\n\n        Parameters\n        ----------\n        tensorboadlogs_instance: :class:`lib.gui.analysis.event_reader.TensorBoardLogs`\n            The class instance to test\n        mocker: :class:`pytest_mock.MockerFixture`\n            Mocker for checking _cache_data is called\n        \"\"\"\n        tb_logs = tensorboardlogs_instance\n        with pytest.raises(tf.errors.NotFoundError):  # invalid session_id\n            tb_logs.get_timestamps(3)\n\n        check_cache = mocker.patch(\"lib.gui.analysis.event_reader.TensorBoardLogs._check_cache\")\n        get_data = mocker.patch(\"lib.gui.analysis.event_reader._Cache.get_data\")\n        get_data.return_value = None\n\n        assert isinstance(tb_logs.get_timestamps(None), dict)\n        assert check_cache.call_count == 2\n        assert get_data.call_count == 2\n        check_cache.reset_mock()\n        get_data.reset_mock()\n\n        assert isinstance(tb_logs.get_timestamps(1), dict)\n        assert check_cache.call_count == 1\n        assert get_data.call_count == 1\n        check_cache.reset_mock()\n        get_data.reset_mock()\n\n\n# EventParser\nclass Test_EventParser:  # pylint:disable=invalid-name\n    \"\"\" Test that :class:`lib.gui.analysis.event_reader.TensorBoardLogs` works correctly \"\"\"\n    def _create_example_event(self,\n                              step: int,\n                              loss_value: float,\n                              timestamp: float,\n                              serialize: bool = True) -> bytes:\n        \"\"\" Generate a test TensorBoard event\n\n        Parameters\n        ----------\n        step: int\n            The step value to use\n        loss_value: float\n            The loss value to store\n        timestamp: float\n            The timestamp to store\n        serialize: bool, optional\n            ``True`` to serialize the event to bytes, ``False`` to return the Event object\n        \"\"\"\n        tags = {0: \"keras\", 1: \"batch_total\", 2: \"batch_face_a\", 3: \"batch_face_b\"}\n        event = event_pb2.Event(step=step)\n        event.summary.value.add(tag=tags[step],  # pylint:disable=no-member\n                                simple_value=loss_value)\n        event.wall_time = timestamp\n        retval = event.SerializeToString() if serialize else event\n        return retval\n\n    @pytest.fixture(name=\"mock_iterator\")\n    def iterator(self) -> Iterator[bytes]:\n        \"\"\" Dummy iterator for generating test events\n\n        Yields\n        ------\n        bytes\n            A serialized test Tensorboard Event\n        \"\"\"\n        return iter([self._create_example_event(i, 1 + (i / 10), time()) for i in range(4)])\n\n    @pytest.fixture(name=\"mock_cache\")\n    def mock_cache(self):\n        \"\"\" Dummy :class:`_Cache` for testing\"\"\"\n        class _CacheMock:\n            def __init__(self):\n                self.data = {}\n                self._loss_labels = []\n\n            def is_cached(self, session_id):\n                \"\"\" Dummy is_cached method\"\"\"\n                return session_id in self.data\n\n            def cache_data(self, session_id, data, labels,\n                           is_live=False):  # pylint:disable=unused-argument\n                \"\"\" Dummy cache_data method\"\"\"\n                self.data[session_id] = {'data': data, 'labels': labels}\n\n        return _CacheMock()\n\n    @pytest.fixture(name=\"event_parser_instance\")\n    def event_parser_fixture(self,\n                             mock_iterator: Iterator[bytes],\n                             mock_cache: _Cache) -> _EventParser:\n        \"\"\" Pytest fixture for :class:`lib.gui.analysis.event_reader._EventParser`\n\n        Parameters\n        ----------\n        mock_iterator: Iterator[bytes]\n            Dummy iterator for generating TF Event data\n        mock_cache: :class:'_CacheMock'\n            Dummy _Cache object\n\n        Returns\n        -------\n        :class::class:`lib.gui.analysis.event_reader._EventParser`\n            The class instance for testing\n        \"\"\"\n        event_parser = _EventParser(mock_iterator, mock_cache, live_data=False)\n        return event_parser\n\n    def test__init_(self,\n                    event_parser_instance: _EventParser,\n                    mock_iterator: Iterator[bytes],\n                    mock_cache: _Cache) -> None:\n        \"\"\" Test __init__ works correctly\n\n        Parameters\n        ----------\n        event_parser_instance: :class:`lib.gui.analysis.event_reader._EventParser`\n            The class instance to test\n        mock_iterator: Iterator[bytes]\n            Dummy iterator for generating TF Event data\n        mock_cache: :class:'_CacheMock'\n            Dummy _Cache object\n        \"\"\"\n        event_parse = event_parser_instance\n        assert not hasattr(event_parse._iterator, \"__name__\")\n        evp_live = _EventParser(mock_iterator, mock_cache, live_data=True)\n        assert evp_live._iterator.__name__ == \"_get_latest_live\"  # type:ignore[attr-defined]\n\n    def test__get_latest_live(self, event_parser_instance: _EventParser) -> None:\n        \"\"\" Test _get_latest_live works correctly\n\n        Parameters\n        ----------\n        event_parser_instance: :class:`lib.gui.analysis.event_reader._EventParser`\n            The class instance to test\n        \"\"\"\n        event_parse = event_parser_instance\n        test = list(event_parse._get_latest_live(event_parse._iterator))\n        assert len(test) == 4\n\n    def test_cache_events(self,\n                          event_parser_instance: _EventParser,\n                          mocker: pytest_mock.MockerFixture,\n                          monkeypatch: pytest.MonkeyPatch) -> None:\n        \"\"\" Test cache_events works correctly\n\n        Parameters\n        ----------\n        event_parser_instance: :class:`lib.gui.analysis.event_reader._EventParser`\n            The class instance to test\n        mocker: :class:`pytest_mock.MockerFixture`\n            Mocker for capturing method calls\n        monkeypatch: :class:`pytest.MonkeyPatch`\n            For patching different iterators for testing output\n        \"\"\"\n        monkeypatch.setattr(\"lib.utils._FS_BACKEND\", \"cpu\")\n\n        event_parse = event_parser_instance\n        event_parse._parse_outputs = T.cast(MagicMock, mocker.MagicMock())  # type:ignore\n        event_parse._process_event = T.cast(MagicMock, mocker.MagicMock())  # type:ignore\n        event_parse._cache.cache_data = T.cast(MagicMock, mocker.MagicMock())  # type:ignore\n\n        # keras model\n        monkeypatch.setattr(event_parse,\n                            \"_iterator\",\n                            iter([self._create_example_event(0, 1., time())]))\n        event_parse.cache_events(1)\n        assert event_parse._parse_outputs.called\n        assert not event_parse._process_event.called\n        assert event_parse._cache.cache_data.called\n        event_parse._parse_outputs.reset_mock()\n        event_parse._process_event.reset_mock()\n        event_parse._cache.cache_data.reset_mock()\n\n        # Batch item\n        monkeypatch.setattr(event_parse,\n                            \"_iterator\",\n                            iter([self._create_example_event(1, 1., time())]))\n        event_parse.cache_events(1)\n        assert not event_parse._parse_outputs.called\n        assert event_parse._process_event.called\n        assert event_parse._cache.cache_data.called\n        event_parse._parse_outputs.reset_mock()\n        event_parse._process_event.reset_mock()\n        event_parse._cache.cache_data.reset_mock()\n\n        # No summary value\n        monkeypatch.setattr(event_parse,\n                            \"_iterator\",\n                            iter([event_pb2.Event(step=1).SerializeToString()]))\n        assert not event_parse._parse_outputs.called\n        assert not event_parse._process_event.called\n        assert not event_parse._cache.cache_data.called\n        event_parse._parse_outputs.reset_mock()\n        event_parse._process_event.reset_mock()\n        event_parse._cache.cache_data.reset_mock()\n\n    def test__parse_outputs(self,\n                            event_parser_instance: _EventParser,\n                            mocker: pytest_mock.MockerFixture) -> None:\n        \"\"\" Test _parse_outputs works correctly\n\n        Parameters\n        ----------\n        event_parser_instance: :class:`lib.gui.analysis.event_reader._EventParser`\n            The class instance to test\n        mocker: :class:`pytest_mock.MockerFixture`\n            Mocker for event object\n        \"\"\"\n        event_parse = event_parser_instance\n        model = {\"config\": {\"layers\": [{\"name\": \"decoder_a\",\n                                        \"config\": {\"output_layers\": [[\"face_out_a\", 0, 0]]}},\n                                       {\"name\": \"decoder_b\",\n                                        \"config\": {\"output_layers\": [[\"face_out_b\", 0, 0]]}}],\n                            \"output_layers\": [[\"decoder_a\", 1, 0], [\"decoder_b\", 1, 0]]}}\n        data = json.dumps(model).encode(\"utf-8\")\n\n        event = mocker.MagicMock()\n        event.summary.value.__getitem__ = lambda self, x: event\n        event.tensor.string_val.__getitem__ = lambda self, x: data\n\n        assert not event_parse._loss_labels\n        event_parse._parse_outputs(event)\n        assert event_parse._loss_labels == [\"face_out_a\", \"face_out_b\"]\n\n    def test__get_outputs(self, event_parser_instance: _EventParser) -> None:\n        \"\"\" Test _get_outputs works correctly\n\n        Parameters\n        ----------\n        event_parser_instance: :class:`lib.gui.analysis.event_reader._EventParser`\n            The class instance to test\n        \"\"\"\n        outputs = [[\"decoder_a\", 1, 0], [\"decoder_b\", 1, 0]]\n        model_config = {\"output_layers\": outputs}\n\n        expected = np.array([[out] for out in outputs])\n        actual = event_parser_instance._get_outputs(model_config)\n        assert isinstance(actual, np.ndarray)\n        assert actual.shape == (2, 1, 3)\n        np.testing.assert_equal(expected, actual)\n\n    def test__process_event(self, event_parser_instance: _EventParser) -> None:\n        \"\"\" Test _process_event works correctly\n\n        Parameters\n        ----------\n        event_parser_instance: :class:`lib.gui.analysis.event_reader._EventParser`\n            The class instance to test\n        \"\"\"\n        event_parse = event_parser_instance\n        event_data = EventData()\n        assert not event_data.timestamp\n        assert not event_data.loss\n        timestamp = time()\n        loss = [1.1, 2.2]\n        event = self._create_example_event(1, 1.0, timestamp, serialize=False)  # batch_total\n        event_parse._process_event(event, event_data)\n        event = self._create_example_event(2, loss[0], time(), serialize=False)  # face A\n        event_parse._process_event(event, event_data)\n        event = self._create_example_event(3, loss[1], time(), serialize=False)  # face B\n        event_parse._process_event(event, event_data)\n\n        # Original timestamp and both loss values collected\n        assert event_data.timestamp == timestamp\n        np.testing.assert_almost_equal(event_data.loss, loss)  # float rounding\n",
    "tests/lib/model/initializers_test.py": "#!/usr/bin/env python3\n\"\"\" Tests for Faceswap Initializers.\n\nAdapted from Keras tests.\n\"\"\"\n\nimport pytest\nimport numpy as np\n\nfrom tensorflow.keras import backend as K  # pylint:disable=import-error\nfrom tensorflow.keras import initializers as k_initializers  # noqa:E501  # pylint:disable=import-error\n\nfrom lib.model import initializers\nfrom lib.utils import get_backend\n\nCONV_SHAPE = (3, 3, 256, 2048)\nCONV_ID = get_backend().upper()\n\n\ndef _runner(init, shape, target_mean=None, target_std=None,\n            target_max=None, target_min=None):\n    variable = K.variable(init(shape))\n    output = K.get_value(variable)\n    lim = 3e-2\n    if target_std is not None:\n        assert abs(output.std() - target_std) < lim\n    if target_mean is not None:\n        assert abs(output.mean() - target_mean) < lim\n    if target_max is not None:\n        assert abs(output.max() - target_max) < lim\n    if target_min is not None:\n        assert abs(output.min() - target_min) < lim\n\n\n@pytest.mark.parametrize('tensor_shape', [CONV_SHAPE], ids=[CONV_ID])\ndef test_icnr(tensor_shape):\n    \"\"\" ICNR Initialization Test\n\n    Parameters\n    ----------\n    tensor_shape: tuple\n        The shape of the tensor to feed to the initializer\n    \"\"\"\n    fan_in, _ = initializers.compute_fans(tensor_shape)\n    std = np.sqrt(2. / fan_in)\n    _runner(initializers.ICNR(initializer=k_initializers.he_uniform(),  # pylint:disable=no-member\n                              scale=2),\n            tensor_shape,\n            target_mean=0,\n            target_std=std)\n\n\n@pytest.mark.parametrize('tensor_shape', [CONV_SHAPE], ids=[CONV_ID])\ndef test_convolution_aware(tensor_shape):\n    \"\"\" Convolution Aware Initialization Test\n\n    Parameters\n    ----------\n    tensor_shape: tuple\n        The shape of the tensor to feed to the initializer\n    \"\"\"\n    fan_in, _ = initializers.compute_fans(tensor_shape)\n    std = np.sqrt(2. / fan_in)\n    _runner(initializers.ConvolutionAware(seed=123), tensor_shape,\n            target_mean=0, target_std=std)\n",
    "tests/lib/model/layers_test.py": "#!/usr/bin/env python3\n\"\"\" Tests for Faceswap Custom Layers.\n\nAdapted from Keras tests.\n\"\"\"\n\n\nimport pytest\nimport numpy as np\n\nfrom numpy.testing import assert_allclose\n\n# Ignore linting errors from Tensorflow's thoroughly broken import system\nfrom tensorflow.keras import Input, Model, backend as K  # pylint:disable=import-error\n\nfrom lib.model import layers\nfrom lib.utils import get_backend\nfrom tests.utils import has_arg\n\nCONV_SHAPE = (3, 3, 256, 2048)\nCONV_ID = get_backend().upper()\n\n\ndef layer_test(layer_cls, kwargs={}, input_shape=None, input_dtype=None,  # noqa:C901\n               input_data=None, expected_output=None,\n               expected_output_dtype=None, fixed_batch_size=False):\n    \"\"\"Test routine for a layer with a single input tensor\n    and single output tensor.\n    \"\"\"\n    # generate input data\n    if input_data is None:\n        assert input_shape\n        if not input_dtype:\n            input_dtype = K.floatx()\n        input_data_shape = list(input_shape)\n        for i, var_e in enumerate(input_data_shape):\n            if var_e is None:\n                input_data_shape[i] = np.random.randint(1, 4)\n        input_data = 10 * np.random.random(input_data_shape)\n        input_data = input_data.astype(input_dtype)\n    else:\n        if input_shape is None:\n            input_shape = input_data.shape\n        if input_dtype is None:\n            input_dtype = input_data.dtype\n    if expected_output_dtype is None:\n        expected_output_dtype = input_dtype\n\n    # instantiation\n    layer = layer_cls(**kwargs)\n\n    # test get_weights , set_weights at layer level\n    weights = layer.get_weights()\n    layer.set_weights(weights)\n\n    layer.build(input_shape)\n    expected_output_shape = layer.compute_output_shape(input_shape)\n\n    # test in functional API\n    if fixed_batch_size:\n        inp = Input(batch_shape=input_shape, dtype=input_dtype)\n    else:\n        inp = Input(shape=input_shape[1:], dtype=input_dtype)\n    outp = layer(inp)\n    assert K.dtype(outp) == expected_output_dtype\n\n    # check with the functional API\n    model = Model(inp, outp)\n\n    actual_output = model.predict(input_data, verbose=0)\n    actual_output_shape = actual_output.shape\n    for expected_dim, actual_dim in zip(expected_output_shape,\n                                        actual_output_shape):\n        if expected_dim is not None:\n            assert expected_dim == actual_dim\n\n    if expected_output is not None:\n        assert_allclose(actual_output, expected_output, rtol=1e-3)\n\n    # test serialization, weight setting at model level\n    model_config = model.get_config()\n    recovered_model = model.__class__.from_config(model_config)\n    if model.weights:\n        weights = model.get_weights()\n        recovered_model.set_weights(weights)\n        _output = recovered_model.predict(input_data, verbose=0)\n        assert_allclose(_output, actual_output, rtol=1e-3)\n\n    # test training mode (e.g. useful when the layer has a\n    # different behavior at training and testing time).\n    if has_arg(layer.call, 'training'):\n        model.compile('rmsprop', 'mse')\n        model.train_on_batch(input_data, actual_output)\n\n    # test instantiation from layer config\n    layer_config = layer.get_config()\n    layer_config['batch_input_shape'] = input_shape\n    layer = layer.__class__.from_config(layer_config)\n\n    # for further checks in the caller function\n    return actual_output\n\n\n@pytest.mark.parametrize('dummy', [None], ids=[get_backend().upper()])\ndef test_global_min_pooling_2d(dummy):  # pylint:disable=unused-argument\n    \"\"\" Global Min Pooling 2D layer test \"\"\"\n    layer_test(layers.GlobalMinPooling2D, input_shape=(2, 4, 4, 1024))\n\n\n@pytest.mark.parametrize('dummy', [None], ids=[get_backend().upper()])\ndef test_global_std_pooling_2d(dummy):  # pylint:disable=unused-argument\n    \"\"\" Global Standard Deviation Pooling 2D layer test \"\"\"\n    layer_test(layers.GlobalStdDevPooling2D, input_shape=(2, 4, 4, 1024))\n\n\n@pytest.mark.parametrize('dummy', [None], ids=[get_backend().upper()])\ndef test_k_resize_images(dummy):  # pylint:disable=unused-argument\n    \"\"\" Global Standard Deviation Pooling 2D layer test \"\"\"\n    layer_test(layers.KResizeImages, input_shape=(2, 4, 4, 1024))\n\n\n@pytest.mark.parametrize('dummy', [None], ids=[get_backend().upper()])\ndef test_l2_normalize(dummy):  # pylint:disable=unused-argument\n    \"\"\" L2 Normalize layer test \"\"\"\n    layer_test(layers.L2_normalize, kwargs={\"axis\": 1}, input_shape=(2, 4, 4, 1024))\n\n\n@pytest.mark.parametrize('dummy', [None], ids=[get_backend().upper()])\ndef test_pixel_shuffler(dummy):  # pylint:disable=unused-argument\n    \"\"\" Pixel Shuffler layer test \"\"\"\n    layer_test(layers.PixelShuffler, input_shape=(2, 4, 4, 1024))\n\n\n@pytest.mark.parametrize('dummy', [None], ids=[get_backend().upper()])\ndef test_quick_gelu(dummy):  # pylint:disable=unused-argument\n    \"\"\" Global Standard Deviation Pooling 2D layer test \"\"\"\n    layer_test(layers.QuickGELU, input_shape=(2, 4, 4, 1024))\n\n\n@pytest.mark.parametrize('dummy', [None], ids=[get_backend().upper()])\ndef test_reflection_padding_2d(dummy):  # pylint:disable=unused-argument\n    \"\"\" Reflection Padding 2D layer test \"\"\"\n    layer_test(layers.ReflectionPadding2D, input_shape=(2, 4, 4, 512))\n\n\n@pytest.mark.parametrize('dummy', [None], ids=[get_backend().upper()])\ndef test_subpixel_upscaling(dummy):  # pylint:disable=unused-argument\n    \"\"\" Sub Pixel up-scaling layer test \"\"\"\n    layer_test(layers.SubPixelUpscaling, input_shape=(2, 4, 4, 1024))\n\n\n@pytest.mark.parametrize('dummy', [None], ids=[get_backend().upper()])\ndef test_swish(dummy):  # pylint:disable=unused-argument\n    \"\"\" Sub Pixel up-scaling layer test \"\"\"\n    layer_test(layers.Swish, input_shape=(2, 4, 4, 1024))\n"
  },
  "requirements": null
}