{
  "repo_name": "geekan/MetaGPT",
  "repo_url": "https://github.com/geekan/MetaGPT",
  "description": "🌟 The Multi-Agent Framework: First AI Software Company, Towards Natural Language Programming",
  "stars": 52866,
  "language": "Python",
  "created_at": "2023-06-30T09:04:55Z",
  "updated_at": "2025-03-19T06:59:35Z",
  "files": {
    "metagpt/actions/write_test.py": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\"\"\"\n@Time    : 2023/5/11 22:12\n@Author  : alexanderwu\n@File    : write_test.py\n@Modified By: mashenquan, 2023-11-27. Following the think-act principle, solidify the task parameters when creating the\n        WriteTest object, rather than passing them in when calling the run function.\n\"\"\"\n\nfrom typing import Optional\n\nfrom metagpt.actions.action import Action\nfrom metagpt.const import TEST_CODES_FILE_REPO\nfrom metagpt.logs import logger\nfrom metagpt.schema import Document, TestingContext\nfrom metagpt.utils.common import CodeParser\n\nPROMPT_TEMPLATE = \"\"\"\nNOTICE\n1. Role: You are a QA engineer; the main goal is to design, develop, and execute PEP8 compliant, well-structured, maintainable test cases and scripts for Python 3.9. Your focus should be on ensuring the product quality of the entire project through systematic testing.\n2. Requirement: Based on the context, develop a comprehensive test suite that adequately covers all relevant aspects of the code file under review. Your test suite will be part of the overall project QA, so please develop complete, robust, and reusable test cases.\n3. Attention1: Use '##' to split sections, not '#', and '## <SECTION_NAME>' SHOULD WRITE BEFORE the test case or script.\n4. Attention2: If there are any settings in your tests, ALWAYS SET A DEFAULT VALUE, ALWAYS USE STRONG TYPE AND EXPLICIT VARIABLE.\n5. Attention3: YOU MUST FOLLOW \"Data structures and interfaces\". DO NOT CHANGE ANY DESIGN. Make sure your tests respect the existing design and ensure its validity.\n6. Think before writing: What should be tested and validated in this document? What edge cases could exist? What might fail?\n7. CAREFULLY CHECK THAT YOU DON'T MISS ANY NECESSARY TEST CASES/SCRIPTS IN THIS FILE.\nAttention: Use '##' to split sections, not '#', and '## <SECTION_NAME>' SHOULD WRITE BEFORE the test case or script and triple quotes.\n-----\n## Given the following code, please write appropriate test cases using Python's unittest framework to verify the correctness and robustness of this code:\n```python\n{code_to_test}\n```\nNote that the code to test is at {source_file_path}, we will put your test code at {workspace}/tests/{test_file_name}, and run your test code from {workspace},\nyou should correctly import the necessary classes based on these file locations!\n## {test_file_name}: Write test code with triple quote. Do your best to implement THIS ONLY ONE FILE.\n\"\"\"\n\n\nclass WriteTest(Action):\n    name: str = \"WriteTest\"\n    i_context: Optional[TestingContext] = None\n\n    async def write_code(self, prompt):\n        code_rsp = await self._aask(prompt)\n\n        try:\n            code = CodeParser.parse_code(text=code_rsp)\n        except Exception:\n            # Handle the exception if needed\n            logger.error(f\"Can't parse the code: {code_rsp}\")\n\n            # Return code_rsp in case of an exception, assuming llm just returns code as it is and doesn't wrap it inside ```\n            code = code_rsp\n        return code\n\n    async def run(self, *args, **kwargs) -> TestingContext:\n        if not self.i_context.test_doc:\n            self.i_context.test_doc = Document(\n                filename=\"test_\" + self.i_context.code_doc.filename, root_path=TEST_CODES_FILE_REPO\n            )\n        fake_root = \"/data\"\n        prompt = PROMPT_TEMPLATE.format(\n            code_to_test=self.i_context.code_doc.content,\n            test_file_name=self.i_context.test_doc.filename,\n            source_file_path=fake_root + \"/\" + self.i_context.code_doc.root_relative_path,\n            workspace=fake_root,\n        )\n        self.i_context.test_doc.content = await self.write_code(prompt)\n        return self.i_context\n",
    "tests/__init__.py": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\"\"\"\n@Time    : 2023/4/29 15:53\n@Author  : alexanderwu\n@File    : __init__.py\n\"\"\"\n",
    "tests/conftest.py": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\"\"\"\n@Time    : 2023/5/1 12:10\n@Author  : alexanderwu\n@File    : conftest.py\n\"\"\"\n\nimport asyncio\nimport json\nimport logging\nimport os\nimport re\nimport uuid\nfrom typing import Callable\n\nimport aiohttp.web\nimport pytest\n\nfrom metagpt.const import DEFAULT_WORKSPACE_ROOT, TEST_DATA_PATH\nfrom metagpt.context import Context as MetagptContext\nfrom metagpt.llm import LLM\nfrom metagpt.logs import logger\nfrom metagpt.utils.git_repository import GitRepository\nfrom tests.mock.mock_aiohttp import MockAioResponse\nfrom tests.mock.mock_curl_cffi import MockCurlCffiResponse\nfrom tests.mock.mock_httplib2 import MockHttplib2Response\nfrom tests.mock.mock_llm import MockLLM\n\nRSP_CACHE_NEW = {}  # used globally for producing new and useful only response cache\nALLOW_OPENAI_API_CALL = int(\n    os.environ.get(\"ALLOW_OPENAI_API_CALL\", 1)\n)  # NOTE: should change to default 0 (False) once mock is complete\n\n\n@pytest.fixture(scope=\"session\")\ndef rsp_cache():\n    rsp_cache_file_path = TEST_DATA_PATH / \"rsp_cache.json\"  # read repo-provided\n    new_rsp_cache_file_path = TEST_DATA_PATH / \"rsp_cache_new.json\"  # exporting a new copy\n    if os.path.exists(rsp_cache_file_path):\n        with open(rsp_cache_file_path, \"r\", encoding=\"utf-8\") as f1:\n            rsp_cache_json = json.load(f1)\n    else:\n        rsp_cache_json = {}\n    yield rsp_cache_json\n    with open(rsp_cache_file_path, \"w\", encoding=\"utf-8\") as f2:\n        json.dump(rsp_cache_json, f2, indent=4, ensure_ascii=False)\n    with open(new_rsp_cache_file_path, \"w\", encoding=\"utf-8\") as f2:\n        json.dump(RSP_CACHE_NEW, f2, indent=4, ensure_ascii=False)\n\n\n# Hook to capture the test result\n@pytest.hookimpl(tryfirst=True, hookwrapper=True)\ndef pytest_runtest_makereport(item, call):\n    outcome = yield\n    rep = outcome.get_result()\n    if rep.when == \"call\":\n        item.test_outcome = rep\n\n\n@pytest.fixture(scope=\"function\", autouse=True)\ndef llm_mock(rsp_cache, mocker, request):\n    llm = MockLLM(allow_open_api_call=ALLOW_OPENAI_API_CALL)\n    llm.rsp_cache = rsp_cache\n    mocker.patch(\"metagpt.provider.base_llm.BaseLLM.aask\", llm.aask)\n    mocker.patch(\"metagpt.provider.base_llm.BaseLLM.aask_batch\", llm.aask_batch)\n    mocker.patch(\"metagpt.provider.openai_api.OpenAILLM.aask_code\", llm.aask_code)\n    yield mocker\n    if hasattr(request.node, \"test_outcome\") and request.node.test_outcome.passed:\n        if llm.rsp_candidates:\n            for rsp_candidate in llm.rsp_candidates:\n                cand_key = list(rsp_candidate.keys())[0]\n                cand_value = list(rsp_candidate.values())[0]\n                if cand_key not in llm.rsp_cache:\n                    logger.info(f\"Added '{cand_key[:100]} ... -> {str(cand_value)[:20]} ...' to response cache\")\n                    llm.rsp_cache.update(rsp_candidate)\n                RSP_CACHE_NEW.update(rsp_candidate)\n\n\nclass Context:\n    def __init__(self):\n        self._llm_ui = None\n        self._llm_api = LLM()\n\n    @property\n    def llm_api(self):\n        # 1. 初始化llm，带有缓存结果\n        # 2. 如果缓存query，那么直接返回缓存结果\n        # 3. 如果没有缓存query，那么调用llm_api，返回结果\n        # 4. 如果有缓存query，那么更新缓存结果\n        return self._llm_api\n\n\n@pytest.fixture(scope=\"package\")\ndef llm_api():\n    logger.info(\"Setting up the test\")\n    g_context = Context()\n\n    yield g_context.llm_api\n\n    logger.info(\"Tearing down the test\")\n\n\n@pytest.fixture\ndef proxy():\n    pattern = re.compile(\n        rb\"(?P<method>[a-zA-Z]+) (?P<uri>(\\w+://)?(?P<host>[^\\s\\'\\\"<>\\[\\]{}|/:]+)(:(?P<port>\\d+))?[^\\s\\'\\\"<>\\[\\]{}|]*) \"\n    )\n\n    async def pipe(reader, writer):\n        while not reader.at_eof():\n            writer.write(await reader.read(2048))\n        writer.close()\n        await writer.wait_closed()\n\n    async def handle_client(reader, writer):\n        data = await reader.readuntil(b\"\\r\\n\\r\\n\")\n        infos = pattern.match(data)\n        host, port = infos.group(\"host\"), infos.group(\"port\")\n        print(f\"Proxy: {host}\")  # checking with capfd fixture\n        port = int(port) if port else 80\n        remote_reader, remote_writer = await asyncio.open_connection(host, port)\n        if data.startswith(b\"CONNECT\"):\n            writer.write(b\"HTTP/1.1 200 Connection Established\\r\\n\\r\\n\")\n        else:\n            remote_writer.write(data)\n        await asyncio.gather(pipe(reader, remote_writer), pipe(remote_reader, writer))\n\n    async def proxy_func():\n        server = await asyncio.start_server(handle_client, \"127.0.0.1\", 0)\n        return server, \"http://{}:{}\".format(*server.sockets[0].getsockname())\n\n    return proxy_func\n\n\n# see https://github.com/Delgan/loguru/issues/59#issuecomment-466591978\n@pytest.fixture\ndef loguru_caplog(caplog):\n    class PropogateHandler(logging.Handler):\n        def emit(self, record):\n            logging.getLogger(record.name).handle(record)\n\n    logger.add(PropogateHandler(), format=\"{message}\")\n    yield caplog\n\n\n@pytest.fixture(scope=\"function\")\ndef context(request):\n    ctx = MetagptContext()\n    repo = GitRepository(local_path=DEFAULT_WORKSPACE_ROOT / f\"unittest/{uuid.uuid4().hex}\")\n    ctx.config.project_path = str(repo.workdir)\n\n    # Destroy git repo at the end of the test session.\n    def fin():\n        if ctx.config.project_path:\n            git_repo = GitRepository(ctx.config.project_path)\n            git_repo.delete_repository()\n\n    # Register the function for destroying the environment.\n    request.addfinalizer(fin)\n    return ctx\n\n\n@pytest.fixture(scope=\"session\", autouse=True)\ndef init_config():\n    pass\n\n\n@pytest.fixture(scope=\"function\")\ndef new_filename(mocker):\n    # NOTE: Mock new filename to make reproducible llm aask, should consider changing after implementing requirement segmentation\n    mocker.patch(\"metagpt.utils.file_repository.FileRepository.new_filename\", lambda: \"20240101\")\n    yield mocker\n\n\ndef _rsp_cache(name):\n    rsp_cache_file_path = TEST_DATA_PATH / f\"{name}.json\"  # read repo-provided\n    if os.path.exists(rsp_cache_file_path):\n        with open(rsp_cache_file_path, \"r\") as f1:\n            rsp_cache_json = json.load(f1)\n    else:\n        rsp_cache_json = {}\n    yield rsp_cache_json\n    with open(rsp_cache_file_path, \"w\") as f2:\n        json.dump(rsp_cache_json, f2, indent=4, ensure_ascii=False)\n\n\n@pytest.fixture(scope=\"session\")\ndef search_rsp_cache():\n    yield from _rsp_cache(\"search_rsp_cache\")\n\n\n@pytest.fixture(scope=\"session\")\ndef mermaid_rsp_cache():\n    yield from _rsp_cache(\"mermaid_rsp_cache\")\n\n\n@pytest.fixture\ndef aiohttp_mocker(mocker):\n    MockResponse = type(\"MockResponse\", (MockAioResponse,), {})\n\n    def wrap(method):\n        def run(self, url, **kwargs):\n            return MockResponse(self, method, url, **kwargs)\n\n        return run\n\n    mocker.patch(\"aiohttp.ClientSession.request\", MockResponse)\n    for i in [\"get\", \"post\", \"delete\", \"patch\"]:\n        mocker.patch(f\"aiohttp.ClientSession.{i}\", wrap(i))\n    yield MockResponse\n\n\n@pytest.fixture\ndef curl_cffi_mocker(mocker):\n    MockResponse = type(\"MockResponse\", (MockCurlCffiResponse,), {})\n\n    def request(self, *args, **kwargs):\n        return MockResponse(self, *args, **kwargs)\n\n    mocker.patch(\"curl_cffi.requests.Session.request\", request)\n    yield MockResponse\n\n\n@pytest.fixture\ndef httplib2_mocker(mocker):\n    MockResponse = type(\"MockResponse\", (MockHttplib2Response,), {})\n\n    def request(self, *args, **kwargs):\n        return MockResponse(self, *args, **kwargs)\n\n    mocker.patch(\"httplib2.Http.request\", request)\n    yield MockResponse\n\n\n@pytest.fixture\ndef search_engine_mocker(aiohttp_mocker, curl_cffi_mocker, httplib2_mocker, search_rsp_cache):\n    # aiohttp_mocker: serpapi/serper\n    # httplib2_mocker: google\n    # curl_cffi_mocker: ddg\n    check_funcs: dict[tuple[str, str], Callable[[dict], str]] = {}\n    aiohttp_mocker.rsp_cache = httplib2_mocker.rsp_cache = curl_cffi_mocker.rsp_cache = search_rsp_cache\n    aiohttp_mocker.check_funcs = httplib2_mocker.check_funcs = curl_cffi_mocker.check_funcs = check_funcs\n    yield check_funcs\n\n\n@pytest.fixture\ndef http_server():\n    async def start(handler=None):\n        if handler is None:\n\n            async def handler(request):\n                return aiohttp.web.Response(\n                    text=\"\"\"<!DOCTYPE html><html lang=\"en\"><head><meta charset=\"UTF-8\">\n                    <title>MetaGPT</title></head><body><h1>MetaGPT</h1></body></html>\"\"\",\n                    content_type=\"text/html\",\n                )\n\n        server = aiohttp.web.Server(handler)\n        runner = aiohttp.web.ServerRunner(server)\n        await runner.setup()\n        site = aiohttp.web.TCPSite(runner, \"127.0.0.1\", 0)\n        await site.start()\n        _, port, *_ = site._server.sockets[0].getsockname()\n        return site, f\"http://127.0.0.1:{port}\"\n\n    return start\n\n\n@pytest.fixture\ndef mermaid_mocker(aiohttp_mocker, mermaid_rsp_cache):\n    check_funcs: dict[tuple[str, str], Callable[[dict], str]] = {}\n    aiohttp_mocker.rsp_cache = mermaid_rsp_cache\n    aiohttp_mocker.check_funcs = check_funcs\n    yield check_funcs\n\n\n@pytest.fixture\ndef git_dir():\n    \"\"\"Fixture to get the unittest directory.\"\"\"\n    git_dir = DEFAULT_WORKSPACE_ROOT / f\"unittest/{uuid.uuid4().hex}\"\n    git_dir.mkdir(parents=True, exist_ok=True)\n    return git_dir\n",
    "tests/data/code/python/1.py": "\"\"\"\n===============\nDegree Analysis\n===============\n\nThis example shows several ways to visualize the distribution of the degree of\nnodes with two common techniques: a *degree-rank plot* and a\n*degree histogram*.\n\nIn this example, a random Graph is generated with 100 nodes. The degree of\neach node is determined, and a figure is generated showing three things:\n1. The subgraph of connected components\n2. The degree-rank plot for the Graph, and\n3. The degree histogram\n\"\"\"\nimport matplotlib.pyplot as plt\nimport networkx as nx\nimport numpy as np\n\nG = nx.gnp_random_graph(100, 0.02, seed=10374196)\n\ndegree_sequence = sorted((d for n, d in G.degree()), reverse=True)\ndmax = max(degree_sequence)\n\nfig = plt.figure(\"Degree of a random graph\", figsize=(8, 8))\n# Create a gridspec for adding subplots of different sizes\naxgrid = fig.add_gridspec(5, 4)\n\nax0 = fig.add_subplot(axgrid[0:3, :])\nGcc = G.subgraph(sorted(nx.connected_components(G), key=len, reverse=True)[0])\npos = nx.spring_layout(Gcc, seed=10396953)\nnx.draw_networkx_nodes(Gcc, pos, ax=ax0, node_size=20)\nnx.draw_networkx_edges(Gcc, pos, ax=ax0, alpha=0.4)\nax0.set_title(\"Connected components of G\")\nax0.set_axis_off()\n\nprint(\"aa\")\n\nax1 = fig.add_subplot(axgrid[3:, :2])\nax1.plot(degree_sequence, \"b-\", marker=\"o\")\nax1.set_title(\"Degree Rank Plot\")\nax1.set_ylabel(\"Degree\")\nax1.set_xlabel(\"Rank\")\n\nax2 = fig.add_subplot(axgrid[3:, 2:])\nax2.bar(*np.unique(degree_sequence, return_counts=True))\nax2.set_title(\"Degree histogram\")\nax2.set_xlabel(\"Degree\")\nax2.set_ylabel(\"# of Nodes\")\n\nfig.tight_layout()\nplt.show()\n\n\nclass Game:\n    def __init__(self):\n        self.snake = Snake(400, 300, 5, 0)\n        self.enemy = Enemy(100, 100, 3, 1)\n        self.power_up = PowerUp(200, 200)\n\n    def handle_events(self):\n        for event in pygame.event.get():\n            if event.type == pygame.QUIT:\n                return False\n            elif event.type == pygame.KEYDOWN:\n                if event.key == pygame.K_UP:\n                    self.snake.change_direction(0)\n                elif event.key == pygame.K_DOWN:\n                    self.snake.change_direction(1)\n                elif event.key == pygame.K_LEFT:\n                    self.snake.change_direction(2)\n                elif event.key == pygame.K_RIGHT:\n                    self.snake.change_direction(3)\n        return True\n\n    def update(self):\n        self.snake.move()\n        self.enemy.move()\n\n    def draw(self, screen):\n        self.snake.draw(screen)\n        self.enemy.draw(screen)\n        self.power_up.draw(screen)\n",
    "tests/data/demo_project/game.py": "## game.py\n\nimport random\nfrom typing import List, Tuple\n\n\nclass Game:\n    def __init__(self):\n        self.grid: List[List[int]] = [[0 for _ in range(4)] for _ in range(4)]\n        self.score: int = 0\n        self.game_over: bool = False\n\n    def reset_game(self):\n        self.grid = [[0 for _ in range(4)] for _ in range(4)]\n        self.score = 0\n        self.game_over = False\n        self.add_new_tile()\n        self.add_new_tile()\n\n    def move(self, direction: str):\n        if direction == \"up\":\n            self._move_up()\n        elif direction == \"down\":\n            self._move_down()\n        elif direction == \"left\":\n            self._move_left()\n        elif direction == \"right\":\n            self._move_right()\n\n    def is_game_over(self) -> bool:\n        for i in range(4):\n            for j in range(4):\n                if self.grid[i][j] == 0:\n                    return False\n                if j < 3 and self.grid[i][j] == self.grid[i][j + 1]:\n                    return False\n                if i < 3 and self.grid[i][j] == self.grid[i + 1][j]:\n                    return False\n        return True\n\n    def get_empty_cells(self) -> List[Tuple[int, int]]:\n        empty_cells = []\n        for i in range(4):\n            for j in range(4):\n                if self.grid[i][j] == 0:\n                    empty_cells.append((i, j))\n        return empty_cells\n\n    def add_new_tile(self):\n        empty_cells = self.get_empty_cells()\n        if empty_cells:\n            x, y = random.choice(empty_cells)\n            self.grid[x][y] = 2 if random.random() < 0.9 else 4\n\n    def get_score(self) -> int:\n        return self.score\n\n    def _move_up(self):\n        for j in range(4):\n            for i in range(1, 4):\n                if self.grid[i][j] != 0:\n                    for k in range(i, 0, -1):\n                        if self.grid[k - 1][j] == 0:\n                            self.grid[k - 1][j] = self.grid[k][j]\n                            self.grid[k][j] = 0\n\n    def _move_down(self):\n        for j in range(4):\n            for i in range(2, -1, -1):\n                if self.grid[i][j] != 0:\n                    for k in range(i, 3):\n                        if self.grid[k + 1][j] == 0:\n                            self.grid[k + 1][j] = self.grid[k][j]\n                            self.grid[k][j] = 0\n\n    def _move_left(self):\n        for i in range(4):\n            for j in range(1, 4):\n                if self.grid[i][j] != 0:\n                    for k in range(j, 0, -1):\n                        if self.grid[i][k - 1] == 0:\n                            self.grid[i][k - 1] = self.grid[i][k]\n                            self.grid[i][k] = 0\n\n    def _move_right(self):\n        for i in range(4):\n            for j in range(2, -1, -1):\n                if self.grid[i][j] != 0:\n                    for k in range(j, 3):\n                        if self.grid[i][k + 1] == 0:\n                            self.grid[i][k + 1] = self.grid[i][k]\n                            self.grid[i][k] = 0\n",
    "tests/data/incremental_dev_project/mock.py": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\"\"\"\n@Time    : 2024/01/17\n@Author  : mannaandpoem\n@File    : mock.py\n\"\"\"\nNEW_REQUIREMENT_SAMPLE = \"\"\"\nAdding graphical interface functionality to enhance the user experience in the number-guessing game. The existing number-guessing game currently relies on command-line input for numbers. The goal is to introduce a graphical interface to improve the game's usability and visual appeal\n\"\"\"\n\nPRD_SAMPLE = \"\"\"\n## Language\n\nen_us\n\n## Programming Language\n\nPython\n\n## Original Requirements\n\nMake a simple number guessing game\n\n## Product Goals\n\n- Ensure a user-friendly interface for the game\n- Provide a challenging yet enjoyable game experience\n- Design the game to be easily extendable for future features\n\n## User Stories\n\n- As a player, I want to guess numbers and receive feedback on whether my guess is too high or too low\n- As a player, I want to be able to set the difficulty level by choosing the range of possible numbers\n- As a player, I want to see my previous guesses to strategize my next guess\n- As a player, I want to know how many attempts it took me to guess the number once I get it right\n\n## Competitive Analysis\n\n- Guess The Number Game A: Basic text interface, no difficulty levels\n- Number Master B: Has difficulty levels, but cluttered interface\n- Quick Guess C: Sleek design, but lacks performance tracking\n- NumGuess D: Good performance tracking, but not mobile-friendly\n- GuessIt E: Mobile-friendly, but too many ads\n- Perfect Guess F: Offers hints, but the hints are not very helpful\n- SmartGuesser G: Has a learning mode, but lacks a competitive edge\n\n## Competitive Quadrant Chart\n\nquadrantChart\n    title \"User Engagement and Game Complexity\"\n    x-axis \"Low Complexity\" --> \"High Complexity\"\n    y-axis \"Low Engagement\" --> \"High Engagement\"\n    quadrant-1 \"Too Simple\"\n    quadrant-2 \"Niche Appeal\"\n    quadrant-3 \"Complex & Unengaging\"\n    quadrant-4 \"Sweet Spot\"\n    \"Guess The Number Game A\": [0.2, 0.4]\n    \"Number Master B\": [0.5, 0.3]\n    \"Quick Guess C\": [0.6, 0.7]\n    \"NumGuess D\": [0.4, 0.6]\n    \"GuessIt E\": [0.7, 0.5]\n    \"Perfect Guess F\": [0.6, 0.4]\n    \"SmartGuesser G\": [0.8, 0.6]\n    \"Our Target Product\": [0.5, 0.8]\n\n## Requirement Analysis\n\nThe game should be simple yet engaging, allowing players of different skill levels to enjoy it. It should provide immediate feedback and track the player's performance. The game should also be designed with a clean and intuitive interface, and it should be easy to add new features in the future.\n\n## Requirement Pool\n\n- ['P0', 'Implement the core game logic to randomly select a number and allow the user to guess it']\n- ['P0', 'Design a user interface that displays the game status and results clearly']\n- ['P1', 'Add difficulty levels by varying the range of possible numbers']\n- ['P1', 'Keep track of and display the number of attempts for each game session']\n- ['P2', \"Store and show the history of the player's guesses during a game session\"]\n\n## UI Design draft\n\nThe UI will feature a clean and minimalist design with a number input field, submit button, and messages area to provide feedback. There will be options to select the difficulty level and a display showing the number of attempts and history of past guesses.\n\n## Anything UNCLEAR\"\"\"\n\nDESIGN_SAMPLE = \"\"\"\n## Implementation approach\n\nWe will create a Python-based number guessing game with a simple command-line interface. For the user interface, we will use the built-in 'input' and 'print' functions for interaction. The random library will be used for generating random numbers. We will structure the code to be modular and easily extendable, separating the game logic from the user interface.\n\n## File list\n\n- main.py\n- game.py\n- ui.py\n\n## Data structures and interfaces\n\n\nclassDiagram\n    class Game {\n        -int secret_number\n        -int min_range\n        -int max_range\n        -list attempts\n        +__init__(difficulty: str)\n        +start_game()\n        +check_guess(guess: int) str\n        +get_attempts() int\n        +get_history() list\n    }\n    class UI {\n        +start()\n        +display_message(message: str)\n        +get_user_input(prompt: str) str\n        +show_attempts(attempts: int)\n        +show_history(history: list)\n        +select_difficulty() str\n    }\n    class Main {\n        +main()\n    }\n    Main --> UI\n    UI --> Game\n\n\n## Program call flow\n\n\nsequenceDiagram\n    participant M as Main\n    participant UI as UI\n    participant G as Game\n    M->>UI: start()\n    UI->>UI: select_difficulty()\n    UI-->>G: __init__(difficulty)\n    G->>G: start_game()\n    loop Game Loop\n        UI->>UI: get_user_input(\"Enter your guess:\")\n        UI-->>G: check_guess(guess)\n        G->>UI: display_message(feedback)\n        G->>UI: show_attempts(attempts)\n        G->>UI: show_history(history)\n    end\n    G->>UI: display_message(\"Correct! Game over.\")\n    UI->>M: main()  # Game session ends\n\n\n## Anything UNCLEAR\n\nThe requirement analysis suggests the need for a clean and intuitive interface. Since we are using a command-line interface, we need to ensure that the text-based UI is as user-friendly as possible. Further clarification on whether a graphical user interface (GUI) is expected in the future would be helpful for planning the extendability of the game.\"\"\"\n\nTASK_SAMPLE = \"\"\"\n## Required Python packages\n\n- random==2.2.1\n\n## Required Other language third-party packages\n\n- No third-party dependencies required\n\n## Logic Analysis\n\n- ['game.py', 'Contains Game class with methods __init__, start_game, check_guess, get_attempts, get_history and uses random library for generating secret_number']\n- ['ui.py', 'Contains UI class with methods start, display_message, get_user_input, show_attempts, show_history, select_difficulty and interacts with Game class']\n- ['main.py', 'Contains Main class with method main that initializes UI class and starts the game loop']\n\n## Task list\n\n- game.py\n- ui.py\n- main.py\n\n## Full API spec\n\n\n\n## Shared Knowledge\n\n`game.py` contains the core game logic and is used by `ui.py` to interact with the user. `main.py` serves as the entry point to start the game.\n\n## Anything UNCLEAR\n\nThe requirement analysis suggests the need for a clean and intuitive interface. Since we are using a command-line interface, we need to ensure that the text-based UI is as user-friendly as possible. Further clarification on whether a graphical user interface (GUI) is expected in the future would be helpful for planning the extendability of the game.\"\"\"\n\nOLD_CODE_SAMPLE = \"\"\"\n--- game.py\n```## game.py\n\nimport random\n\nclass Game:\n    def __init__(self, difficulty: str = 'medium'):\n        self.min_range, self.max_range = self._set_difficulty(difficulty)\n        self.secret_number = random.randint(self.min_range, self.max_range)\n        self.attempts = []\n\n    def _set_difficulty(self, difficulty: str):\n        difficulties = {\n            'easy': (1, 10),\n            'medium': (1, 100),\n            'hard': (1, 1000)\n        }\n        return difficulties.get(difficulty, (1, 100))\n\n    def start_game(self):\n        self.secret_number = random.randint(self.min_range, self.max_range)\n        self.attempts = []\n\n    def check_guess(self, guess: int) -> str:\n        self.attempts.append(guess)\n        if guess < self.secret_number:\n            return \"It's higher.\"\n        elif guess > self.secret_number:\n            return \"It's lower.\"\n        else:\n            return \"Correct! Game over.\"\n\n    def get_attempts(self) -> int:\n        return len(self.attempts)\n\n    def get_history(self) -> list:\n        return self.attempts```\n\n--- ui.py\n```## ui.py\n\nfrom game import Game\n\nclass UI:\n    def start(self):\n        difficulty = self.select_difficulty()\n        game = Game(difficulty)\n        game.start_game()\n        self.display_welcome_message(game)\n\n        feedback = \"\"\n        while feedback != \"Correct! Game over.\":\n            guess = self.get_user_input(\"Enter your guess: \")\n            if self.is_valid_guess(guess):\n                feedback = game.check_guess(int(guess))\n                self.display_message(feedback)\n                self.show_attempts(game.get_attempts())\n                self.show_history(game.get_history())\n            else:\n                self.display_message(\"Please enter a valid number.\")\n\n    def display_welcome_message(self, game):\n        print(\"Welcome to the Number Guessing Game!\")\n        print(f\"Guess the number between {game.min_range} and {game.max_range}.\")\n\n    def is_valid_guess(self, guess):\n        return guess.isdigit()\n\n    def display_message(self, message: str):\n        print(message)\n\n    def get_user_input(self, prompt: str) -> str:\n        return input(prompt)\n\n    def show_attempts(self, attempts: int):\n        print(f\"Number of attempts: {attempts}\")\n\n    def show_history(self, history: list):\n        print(\"Guess history:\")\n        for guess in history:\n            print(guess)\n\n    def select_difficulty(self) -> str:\n        while True:\n            difficulty = input(\"Select difficulty (easy, medium, hard): \").lower()\n            if difficulty in ['easy', 'medium', 'hard']:\n                return difficulty\n            else:\n                self.display_message(\"Invalid difficulty. Please choose 'easy', 'medium', or 'hard'.\")```\n\n--- main.py\n```## main.py\n\nfrom ui import UI\n\nclass Main:\n    def main(self):\n        user_interface = UI()\n        user_interface.start()\n\nif __name__ == \"__main__\":\n    main_instance = Main()\n    main_instance.main()```\n\"\"\"\n\nREFINED_PRD_JSON = {\n    \"Language\": \"en_us\",\n    \"Programming Language\": \"Python\",\n    \"Refined Requirements\": \"Adding graphical interface functionality to enhance the user experience in the number-guessing game.\",\n    \"Project Name\": \"number_guessing_game\",\n    \"Refined Product Goals\": [\n        \"Ensure a user-friendly interface for the game with the new graphical interface\",\n        \"Provide a challenging yet enjoyable game experience with visual enhancements\",\n        \"Design the game to be easily extendable for future features, including graphical elements\",\n    ],\n    \"Refined User Stories\": [\n        \"As a player, I want to interact with a graphical interface to guess numbers and receive visual feedback on my guesses\",\n        \"As a player, I want to easily select the difficulty level through the graphical interface\",\n        \"As a player, I want to visually track my previous guesses and the number of attempts in the graphical interface\",\n        \"As a player, I want to be congratulated with a visually appealing message when I guess the number correctly\",\n    ],\n    \"Competitive Analysis\": [\n        \"Guess The Number Game A: Basic text interface, no difficulty levels\",\n        \"Number Master B: Has difficulty levels, but cluttered interface\",\n        \"Quick Guess C: Sleek design, but lacks performance tracking\",\n        \"NumGuess D: Good performance tracking, but not mobile-friendly\",\n        \"GuessIt E: Mobile-friendly, but too many ads\",\n        \"Perfect Guess F: Offers hints, but the hints are not very helpful\",\n        \"SmartGuesser G: Has a learning mode, but lacks a competitive edge\",\n        \"Graphical Guess H: Graphical interface, but poor user experience due to complex design\",\n    ],\n    \"Competitive Quadrant Chart\": 'quadrantChart\\n    title \"User Engagement and Game Complexity with Graphical Interface\"\\n    x-axis \"Low Complexity\" --> \"High Complexity\"\\n    y-axis \"Low Engagement\" --> \"High Engagement\"\\n    quadrant-1 \"Too Simple\"\\n    quadrant-2 \"Niche Appeal\"\\n    quadrant-3 \"Complex & Unengaging\"\\n    quadrant-4 \"Sweet Spot\"\\n    \"Guess The Number Game A\": [0.2, 0.4]\\n    \"Number Master B\": [0.5, 0.3]\\n    \"Quick Guess C\": [0.6, 0.7]\\n    \"NumGuess D\": [0.4, 0.6]\\n    \"GuessIt E\": [0.7, 0.5]\\n    \"Perfect Guess F\": [0.6, 0.4]\\n    \"SmartGuesser G\": [0.8, 0.6]\\n    \"Graphical Guess H\": [0.7, 0.3]\\n    \"Our Target Product\": [0.5, 0.9]',\n    \"Refined Requirement Analysis\": [\n        \"The game should maintain its simplicity while integrating a graphical interface for enhanced engagement.\",\n        \"Immediate visual feedback is crucial for user satisfaction in the graphical interface.\",\n        \"The interface must be intuitive, allowing for easy navigation and selection of game options.\",\n        \"The graphical design should be clean and not detract from the game's core guessing mechanic.\",\n    ],\n    \"Refined Requirement Pool\": [\n        [\"P0\", \"Implement a graphical user interface (GUI) to replace the command-line interaction\"],\n        [\n            \"P0\",\n            \"Design a user interface that displays the game status, results, and feedback clearly with graphical elements\",\n        ],\n        [\"P1\", \"Incorporate interactive elements for selecting difficulty levels\"],\n        [\"P1\", \"Visualize the history of the player's guesses and the number of attempts within the game session\"],\n        [\"P2\", \"Create animations for correct or incorrect guesses to enhance user feedback\"],\n        [\"P2\", \"Ensure the GUI is responsive and compatible with various screen sizes\"],\n        [\"P2\", \"Store and show the history of the player's guesses during a game session\"],\n    ],\n    \"UI Design draft\": \"The UI will feature a modern and minimalist design with a graphical number input field, a submit button with animations, and a dedicated area for visual feedback. It will include interactive elements to select the difficulty level and a visual display for the number of attempts and history of past guesses.\",\n    \"Anything UNCLEAR\": \"\",\n}\n\nREFINED_DESIGN_JSON = {\n    \"Refined Implementation Approach\": \"To accommodate the new graphical user interface (GUI) requirements, we will leverage the Tkinter library, which is included with Python and supports the creation of a user-friendly GUI. The game logic will remain in Python, with Tkinter handling the rendering of the interface. We will ensure that the GUI is responsive and provides immediate visual feedback. The main game loop will be event-driven, responding to user inputs such as button clicks and difficulty selection.\",\n    \"Refined File list\": [\"main.py\", \"game.py\", \"ui.py\", \"gui.py\"],\n    \"Refined Data structures and interfaces\": \"\\nclassDiagram\\n    class Game {\\n        -int secret_number\\n        -int min_range\\n        -int max_range\\n        -list attempts\\n        +__init__(difficulty: str)\\n        +start_game()\\n        +check_guess(guess: int) str\\n        +get_attempts() int\\n        +get_history() list\\n    }\\n    class UI {\\n        +start()\\n        +display_message(message: str)\\n        +get_user_input(prompt: str) str\\n        +show_attempts(attempts: int)\\n        +show_history(history: list)\\n        +select_difficulty() str\\n    }\\n    class GUI {\\n        +__init__()\\n        +setup_window()\\n        +bind_events()\\n        +update_feedback(message: str)\\n        +update_attempts(attempts: int)\\n        +update_history(history: list)\\n        +show_difficulty_selector()\\n        +animate_guess_result(correct: bool)\\n    }\\n    class Main {\\n        +main()\\n    }\\n    Main --> UI\\n    UI --> Game\\n    UI --> GUI\\n    GUI --> Game\\n\",\n    \"Refined Program call flow\": '\\nsequenceDiagram\\n    participant M as Main\\n    participant UI as UI\\n    participant G as Game\\n    participant GU as GUI\\n    M->>UI: start()\\n    UI->>GU: setup_window()\\n    GU->>GU: bind_events()\\n    GU->>UI: select_difficulty()\\n    UI-->>G: __init__(difficulty)\\n    G->>G: start_game()\\n    loop Game Loop\\n        GU->>GU: show_difficulty_selector()\\n        GU->>UI: get_user_input(\"Enter your guess:\")\\n        UI-->>G: check_guess(guess)\\n        G->>GU: update_feedback(feedback)\\n        G->>GU: update_attempts(attempts)\\n        G->>GU: update_history(history)\\n        GU->>GU: animate_guess_result(correct)\\n    end\\n    G->>GU: update_feedback(\"Correct! Game over.\")\\n    GU->>M: main()  # Game session ends\\n',\n    \"Anything UNCLEAR\": \"\",\n}\n\nREFINED_TASK_JSON = {\n    \"Required Python packages\": [\"random==2.2.1\", \"Tkinter==8.6\"],\n    \"Required Other language third-party packages\": [\"No third-party dependencies required\"],\n    \"Refined Logic Analysis\": [\n        [\n            \"game.py\",\n            \"Contains Game class with methods __init__, start_game, check_guess, get_attempts, get_history and uses random library for generating secret_number\",\n        ],\n        [\n            \"ui.py\",\n            \"Contains UI class with methods start, display_message, get_user_input, show_attempts, show_history, select_difficulty and interacts with Game class\",\n        ],\n        [\n            \"gui.py\",\n            \"Contains GUI class with methods __init__, setup_window, bind_events, update_feedback, update_attempts, update_history, show_difficulty_selector, animate_guess_result and interacts with Game class for GUI rendering\",\n        ],\n        [\n            \"main.py\",\n            \"Contains Main class with method main that initializes UI class and starts the event-driven game loop\",\n        ],\n    ],\n    \"Refined Task list\": [\"game.py\", \"ui.py\", \"gui.py\", \"main.py\"],\n    \"Full API spec\": \"\",\n    \"Refined Shared Knowledge\": \"`game.py` contains the core game logic and is used by `ui.py` to interact with the user. `main.py` serves as the entry point to start the game. `gui.py` is introduced to handle the graphical user interface using Tkinter, which will interact with both `game.py` and `ui.py` for a responsive and user-friendly experience.\",\n    \"Anything UNCLEAR\": \"\",\n}\n\nCODE_PLAN_AND_CHANGE_SAMPLE = {\n    \"Development Plan\": [\n        \"Develop the GUI using Tkinter to replace the command-line interface. Start by setting up the main window and event handling. Then, add widgets for displaying the game status, results, and feedback. Implement interactive elements for difficulty selection and visualize the guess history. Finally, create animations for guess feedback and ensure responsiveness across different screen sizes.\",\n        \"Modify the main.py to initialize the GUI and start the event-driven game loop. Ensure that the GUI is the primary interface for user interaction.\",\n    ],\n    \"Incremental Change\": [\n        \"\"\"```diff\\nclass GUI:\\n-    pass\\n+    def __init__(self):\\n+        self.setup_window()\\n+\\n+    def setup_window(self):\\n+        # Initialize the main window using Tkinter\\n+        pass\\n+\\n+    def bind_events(self):\\n+        # Bind button clicks and other events\\n+        pass\\n+\\n+    def update_feedback(self, message: str):\\n+        # Update the feedback label with the given message\\n+        pass\\n+\\n+    def update_attempts(self, attempts: int):\\n+        # Update the attempts label with the number of attempts\\n+        pass\\n+\\n+    def update_history(self, history: list):\\n+        # Update the history view with the list of past guesses\\n+        pass\\n+\\n+    def show_difficulty_selector(self):\\n+        # Show buttons or a dropdown for difficulty selection\\n+        pass\\n+\\n+    def animate_guess_result(self, correct: bool):\\n+        # Trigger an animation for correct or incorrect guesses\\n+        pass\\n```\"\"\",\n        \"\"\"```diff\\nclass Main:\\n     def main(self):\\n-        user_interface = UI()\\n-        user_interface.start()\\n+        graphical_user_interface = GUI()\\n+        graphical_user_interface.setup_window()\\n+        graphical_user_interface.bind_events()\\n+        # Start the Tkinter main loop\\n+        pass\\n\\n if __name__ == \"__main__\":\\n     main_instance = Main()\\n     main_instance.main()\\n```\\n\\n3. Plan for ui.py: Refactor ui.py to work with the new GUI class. Remove command-line interactions and delegate display and input tasks to the GUI.\\n```python\\nclass UI:\\n-    def display_message(self, message: str):\\n-        print(message)\\n+\\n+    def display_message(self, message: str):\\n+        # This method will now pass the message to the GUI to display\\n+        pass\\n\\n-    def get_user_input(self, prompt: str) -> str:\\n-        return input(prompt)\\n+\\n+    def get_user_input(self, prompt: str) -> str:\\n+        # This method will now trigger the GUI to get user input\\n+        pass\\n\\n-    def show_attempts(self, attempts: int):\\n-        print(f\"Number of attempts: {attempts}\")\\n+\\n+    def show_attempts(self, attempts: int):\\n+        # This method will now update the GUI with the number of attempts\\n+        pass\\n\\n-    def show_history(self, history: list):\\n-        print(\"Guess history:\")\\n-        for guess in history:\\n-            print(guess)\\n+\\n+    def show_history(self, history: list):\\n+        # This method will now update the GUI with the guess history\\n+        pass\\n```\\n\\n4. Plan for game.py: Ensure game.py remains mostly unchanged as it contains the core game logic. However, make minor adjustments if necessary to integrate with the new GUI.\\n```python\\nclass Game:\\n     # No changes required for now\\n```\\n\"\"\",\n    ],\n}\n\nREFINED_CODE_INPUT_SAMPLE = \"\"\"\n-----Now, game.py to be rewritten\n```## game.py\n\nimport random\n\nclass Game:\n    def __init__(self, difficulty: str = 'medium'):\n        self.min_range, self.max_range = self._set_difficulty(difficulty)\n        self.secret_number = random.randint(self.min_range, self.max_range)\n        self.attempts = []\n\n    def _set_difficulty(self, difficulty: str):\n        difficulties = {\n            'easy': (1, 10),\n            'medium': (1, 100),\n            'hard': (1, 1000)\n        }\n        return difficulties.get(difficulty, (1, 100))\n\n    def start_game(self):\n        self.secret_number = random.randint(self.min_range, self.max_range)\n        self.attempts = []\n\n    def check_guess(self, guess: int) -> str:\n        self.attempts.append(guess)\n        if guess < self.secret_number:\n            return \"It's higher.\"\n        elif guess > self.secret_number:\n            return \"It's lower.\"\n        else:\n            return \"Correct! Game over.\"\n\n    def get_attempts(self) -> int:\n        return len(self.attempts)\n\n    def get_history(self) -> list:\n        return self.attempts```\n\"\"\"\n\nREFINED_CODE_SAMPLE = \"\"\"\n## game.py\n\nimport random\n\nclass Game:\n    def __init__(self, difficulty: str = 'medium'):\n        # Set the difficulty level with default value 'medium'\n        self.min_range, self.max_range = self._set_difficulty(difficulty)\n        # Initialize the secret number based on the difficulty\n        self.secret_number = random.randint(self.min_range, self.max_range)\n        # Initialize the list to keep track of attempts\n        self.attempts = []\n\n    def _set_difficulty(self, difficulty: str):\n        # Define the range of numbers for each difficulty level\n        difficulties = {\n            'easy': (1, 10),\n            'medium': (1, 100),\n            'hard': (1, 1000)\n        }\n        # Return the corresponding range for the selected difficulty, default to 'medium' if not found\n        return difficulties.get(difficulty, (1, 100))\n\n    def start_game(self):\n        # Reset the secret number and attempts list for a new game\n        self.secret_number = random.randint(self.min_range, self.max_range)\n        self.attempts.clear()\n\n    def check_guess(self, guess: int) -> str:\n        # Add the guess to the attempts list\n        self.attempts.append(guess)\n        # Provide feedback based on the guess\n        if guess < self.secret_number:\n            return \"It's higher.\"\n        elif guess > self.secret_number:\n            return \"It's lower.\"\n        else:\n            return \"Correct! Game over.\"\n\n    def get_attempts(self) -> int:\n        # Return the number of attempts made\n        return len(self.attempts)\n\n    def get_history(self) -> list:\n        # Return the list of attempts made\n        return self.attempts\n\"\"\"\n",
    "tests/metagpt/__init__.py": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\"\"\"\n@Time    : 2023/4/29 16:01\n@Author  : alexanderwu\n@File    : __init__.py\n\"\"\"\n",
    "tests/metagpt/actions/__init__.py": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\"\"\"\n@Time    : 2023/5/11 19:35\n@Author  : alexanderwu\n@File    : __init__.py\n\"\"\"\n",
    "tests/metagpt/actions/di/test_ask_review.py": "import pytest\n\nfrom metagpt.actions.di.ask_review import AskReview\n\n\n@pytest.mark.asyncio\nasync def test_ask_review(mocker):\n    mock_review_input = \"confirm\"\n    mocker.patch(\"metagpt.actions.di.ask_review.get_human_input\", return_value=mock_review_input)\n    rsp, confirmed = await AskReview().run()\n    assert rsp == mock_review_input\n    assert confirmed\n"
  },
  "requirements": "aiohttp==3.8.6\n#azure_storage==0.37.0\nchannels==4.0.0\n# Django==4.1.5\n# docx==0.2.4\n#faiss==1.5.3\nfaiss_cpu==1.7.4\nfire==0.4.0\ntyper==0.9.0\n# godot==0.1.1\n# google_api_python_client==2.93.0  # Used by search_engine.py\nlancedb==0.4.0\nloguru==0.6.0\nmeilisearch==0.21.0\nnumpy~=1.26.4\nopenai~=1.64.0\nopenpyxl~=3.1.5\nbeautifulsoup4==4.12.3\npandas==2.1.1\npydantic>=2.5.3\n#pygame==2.1.3\n# pymilvus==2.4.6\n# pytest==7.2.2 # test extras require\npython_docx==0.8.11\nPyYAML==6.0.1\n# sentence_transformers==2.2.2\nsetuptools==65.6.3\ntenacity==8.2.3\ntiktoken==0.7.0\ntqdm==4.66.2\n#unstructured[local-inference]\n# selenium>4\n# webdriver_manager<3.9\nanthropic==0.47.2\ntyping-inspect==0.8.0\nlibcst==1.0.1\nqdrant-client==1.7.0\ngrpcio~=1.67.0\ngrpcio-tools~=1.62.3\ngrpcio-status~=1.62.3\n# pytest-mock==3.11.1  # test extras require\n# open-interpreter==0.1.7; python_version>\"3.9\" # Conflict with openai 1.x\nta==0.10.2\nsemantic-kernel==0.4.3.dev0\nwrapt==1.15.0\n#aiohttp_jinja2\n# azure-cognitiveservices-speech~=1.31.0 # Used by metagpt/tools/azure_tts.py\n#aioboto3~=12.4.0  # Used by metagpt/utils/s3.py\nredis~=5.0.0 # Used by metagpt/utils/redis.py\ncurl-cffi~=0.7.0\nhttplib2~=0.22.0\nwebsocket-client~=1.8.0\naiofiles==23.2.1\ngitpython==3.1.40\nzhipuai~=2.1.5\nrich==13.6.0\nnbclient==0.9.0\nnbformat==5.9.2\nipython==8.17.2\nipykernel==6.27.1\nscikit_learn==1.3.2\ntyping-extensions==4.11.0\nsocksio~=1.0.0\ngitignore-parser==0.1.9\n# connexion[uvicorn]~=3.0.5 # Used by metagpt/tools/openapi_v3_hello.py\nwebsockets>=10.0,<12.0\nnetworkx~=3.2.1\ngoogle-generativeai==0.4.1\nplaywright>=1.26  # used at metagpt/tools/libs/web_scraping.py\nanytree\nipywidgets==8.1.1\nPillow\nimap_tools==1.5.0  # Used by metagpt/tools/libs/email_login.py\npylint~=3.0.3\npygithub~=2.3\nhtmlmin\nfsspec\ngrep-ast~=0.3.3  # linter\nunidiff==0.7.5 # used at metagpt/tools/libs/cr.py\nqianfan~=0.4.4\ndashscope~=1.19.3\nrank-bm25==0.2.2  # for tool recommendation\njieba==0.42.1  # for tool recommendation\nvolcengine-python-sdk[ark]~=1.0.94 # Solution for installation error in Windows: https://github.com/volcengine/volcengine-python-sdk/issues/5\ngymnasium==0.29.1\nboto3~=1.34.69\nspark_ai_python~=0.3.30\ntree_sitter~=0.23.2\ntree_sitter_python~=0.23.2\nhttpx==0.28.1\n"
}