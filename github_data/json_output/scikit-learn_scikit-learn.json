{
  "repo_name": "scikit-learn/scikit-learn",
  "repo_url": "https://github.com/scikit-learn/scikit-learn",
  "description": "scikit-learn: machine learning in Python",
  "stars": 61464,
  "language": "Python",
  "created_at": "2010-08-17T09:43:38Z",
  "updated_at": "2025-03-19T05:26:05Z",
  "files": {
    "build_tools/azure/get_selected_tests.py": "from get_commit_message import get_commit_message\n\n\ndef get_selected_tests():\n    \"\"\"Parse the commit message to check if pytest should run only specific tests.\n\n    If so, selected tests will be run with SKLEARN_TESTS_GLOBAL_RANDOM_SEED=\"all\".\n\n    The commit message must take the form:\n        <title> [all random seeds]\n        <test_name_1>\n        <test_name_2>\n        ...\n    \"\"\"\n    commit_message = get_commit_message()\n\n    if \"[all random seeds]\" in commit_message:\n        selected_tests = commit_message.split(\"[all random seeds]\")[1].strip()\n        selected_tests = selected_tests.replace(\"\\n\", \" or \")\n    else:\n        selected_tests = \"\"\n\n    return selected_tests\n\n\nif __name__ == \"__main__\":\n    # set the environment variable to be propagated to other steps\n    selected_tests = get_selected_tests()\n\n    if selected_tests:\n        print(f\"##vso[task.setvariable variable=SELECTED_TESTS]'{selected_tests}'\")\n        print(f\"selected tests: {selected_tests}\")  # helps debugging\n    else:\n        print(\"no selected tests\")\n",
    "doc/conftest.py": "import os\nfrom os import environ\nfrom os.path import exists, join\n\nimport pytest\nfrom _pytest.doctest import DoctestItem\n\nfrom sklearn.datasets import get_data_home\nfrom sklearn.datasets._base import _pkl_filepath\nfrom sklearn.datasets._twenty_newsgroups import CACHE_NAME\nfrom sklearn.utils._testing import SkipTest, check_skip_network\nfrom sklearn.utils.fixes import np_base_version, parse_version, sp_version\n\n\ndef setup_labeled_faces():\n    data_home = get_data_home()\n    if not exists(join(data_home, \"lfw_home\")):\n        raise SkipTest(\"Skipping dataset loading doctests\")\n\n\ndef setup_rcv1():\n    check_skip_network()\n    # skip the test in rcv1.rst if the dataset is not already loaded\n    rcv1_dir = join(get_data_home(), \"RCV1\")\n    if not exists(rcv1_dir):\n        raise SkipTest(\"Download RCV1 dataset to run this test.\")\n\n\ndef setup_twenty_newsgroups():\n    cache_path = _pkl_filepath(get_data_home(), CACHE_NAME)\n    if not exists(cache_path):\n        raise SkipTest(\"Skipping dataset loading doctests\")\n\n\ndef setup_working_with_text_data():\n    check_skip_network()\n    cache_path = _pkl_filepath(get_data_home(), CACHE_NAME)\n    if not exists(cache_path):\n        raise SkipTest(\"Skipping dataset loading doctests\")\n\n\ndef setup_loading_other_datasets():\n    try:\n        import pandas  # noqa\n    except ImportError:\n        raise SkipTest(\"Skipping loading_other_datasets.rst, pandas not installed\")\n\n    # checks SKLEARN_SKIP_NETWORK_TESTS to see if test should run\n    run_network_tests = environ.get(\"SKLEARN_SKIP_NETWORK_TESTS\", \"1\") == \"0\"\n    if not run_network_tests:\n        raise SkipTest(\n            \"Skipping loading_other_datasets.rst, tests can be \"\n            \"enabled by setting SKLEARN_SKIP_NETWORK_TESTS=0\"\n        )\n\n\ndef setup_compose():\n    try:\n        import pandas  # noqa\n    except ImportError:\n        raise SkipTest(\"Skipping compose.rst, pandas not installed\")\n\n\ndef setup_impute():\n    try:\n        import pandas  # noqa\n    except ImportError:\n        raise SkipTest(\"Skipping impute.rst, pandas not installed\")\n\n\ndef setup_grid_search():\n    try:\n        import pandas  # noqa\n    except ImportError:\n        raise SkipTest(\"Skipping grid_search.rst, pandas not installed\")\n\n\ndef setup_preprocessing():\n    try:\n        import pandas  # noqa\n    except ImportError:\n        raise SkipTest(\"Skipping preprocessing.rst, pandas not installed\")\n\n\ndef skip_if_matplotlib_not_installed(fname):\n    try:\n        import matplotlib  # noqa\n    except ImportError:\n        basename = os.path.basename(fname)\n        raise SkipTest(f\"Skipping doctests for {basename}, matplotlib not installed\")\n\n\ndef skip_if_cupy_not_installed(fname):\n    try:\n        import cupy  # noqa\n    except ImportError:\n        basename = os.path.basename(fname)\n        raise SkipTest(f\"Skipping doctests for {basename}, cupy not installed\")\n\n\ndef pytest_runtest_setup(item):\n    fname = item.fspath.strpath\n    # normalize filename to use forward slashes on Windows for easier handling\n    # later\n    fname = fname.replace(os.sep, \"/\")\n\n    is_index = fname.endswith(\"datasets/index.rst\")\n    if fname.endswith(\"datasets/labeled_faces.rst\") or is_index:\n        setup_labeled_faces()\n    elif fname.endswith(\"datasets/rcv1.rst\") or is_index:\n        setup_rcv1()\n    elif fname.endswith(\"datasets/twenty_newsgroups.rst\") or is_index:\n        setup_twenty_newsgroups()\n    elif fname.endswith(\"modules/compose.rst\") or is_index:\n        setup_compose()\n    elif fname.endswith(\"datasets/loading_other_datasets.rst\"):\n        setup_loading_other_datasets()\n    elif fname.endswith(\"modules/impute.rst\"):\n        setup_impute()\n    elif fname.endswith(\"modules/grid_search.rst\"):\n        setup_grid_search()\n    elif fname.endswith(\"modules/preprocessing.rst\"):\n        setup_preprocessing()\n\n    rst_files_requiring_matplotlib = [\n        \"modules/partial_dependence.rst\",\n        \"modules/tree.rst\",\n    ]\n    for each in rst_files_requiring_matplotlib:\n        if fname.endswith(each):\n            skip_if_matplotlib_not_installed(fname)\n\n    if fname.endswith(\"array_api.rst\"):\n        skip_if_cupy_not_installed(fname)\n\n\ndef pytest_configure(config):\n    # Use matplotlib agg backend during the tests including doctests\n    try:\n        import matplotlib\n\n        matplotlib.use(\"agg\")\n    except ImportError:\n        pass\n\n\ndef pytest_collection_modifyitems(config, items):\n    \"\"\"Called after collect is completed.\n\n    Parameters\n    ----------\n    config : pytest config\n    items : list of collected items\n    \"\"\"\n    skip_doctests = False\n    if np_base_version < parse_version(\"2\"):\n        # TODO: configure numpy to output scalar arrays as regular Python scalars\n        # once possible to improve readability of the tests docstrings.\n        # https://numpy.org/neps/nep-0051-scalar-representation.html#implementation\n        reason = \"Due to NEP 51 numpy scalar repr has changed in numpy 2\"\n        skip_doctests = True\n\n    if sp_version < parse_version(\"1.14\"):\n        reason = \"Scipy sparse matrix repr has changed in scipy 1.14\"\n        skip_doctests = True\n\n    # Normally doctest has the entire module's scope. Here we set globs to an empty dict\n    # to remove the module's scope:\n    # https://docs.python.org/3/library/doctest.html#what-s-the-execution-context\n    for item in items:\n        if isinstance(item, DoctestItem):\n            item.dtest.globs = {}\n\n    if skip_doctests:\n        skip_marker = pytest.mark.skip(reason=reason)\n\n        for item in items:\n            if isinstance(item, DoctestItem):\n                item.add_marker(skip_marker)\n",
    "examples/feature_selection/plot_f_test_vs_mi.py": "\"\"\"\n===========================================\nComparison of F-test and mutual information\n===========================================\n\nThis example illustrates the differences between univariate F-test statistics\nand mutual information.\n\nWe consider 3 features x_1, x_2, x_3 distributed uniformly over [0, 1], the\ntarget depends on them as follows:\n\ny = x_1 + sin(6 * pi * x_2) + 0.1 * N(0, 1), that is the third feature is\ncompletely irrelevant.\n\nThe code below plots the dependency of y against individual x_i and normalized\nvalues of univariate F-tests statistics and mutual information.\n\nAs F-test captures only linear dependency, it rates x_1 as the most\ndiscriminative feature. On the other hand, mutual information can capture any\nkind of dependency between variables and it rates x_2 as the most\ndiscriminative feature, which probably agrees better with our intuitive\nperception for this example. Both methods correctly mark x_3 as irrelevant.\n\n\"\"\"\n\n# Authors: The scikit-learn developers\n# SPDX-License-Identifier: BSD-3-Clause\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn.feature_selection import f_regression, mutual_info_regression\n\nnp.random.seed(0)\nX = np.random.rand(1000, 3)\ny = X[:, 0] + np.sin(6 * np.pi * X[:, 1]) + 0.1 * np.random.randn(1000)\n\nf_test, _ = f_regression(X, y)\nf_test /= np.max(f_test)\n\nmi = mutual_info_regression(X, y)\nmi /= np.max(mi)\n\nplt.figure(figsize=(15, 5))\nfor i in range(3):\n    plt.subplot(1, 3, i + 1)\n    plt.scatter(X[:, i], y, edgecolor=\"black\", s=20)\n    plt.xlabel(\"$x_{}$\".format(i + 1), fontsize=14)\n    if i == 0:\n        plt.ylabel(\"$y$\", fontsize=14)\n    plt.title(\"F-test={:.2f}, MI={:.2f}\".format(f_test[i], mi[i]), fontsize=16)\nplt.show()\n",
    "examples/model_selection/plot_permutation_tests_for_classification.py": "\"\"\"\n=================================================================\nTest with permutations the significance of a classification score\n=================================================================\n\nThis example demonstrates the use of\n:func:`~sklearn.model_selection.permutation_test_score` to evaluate the\nsignificance of a cross-validated score using permutations.\n\n\"\"\"\n\n# Authors: The scikit-learn developers\n# SPDX-License-Identifier: BSD-3-Clause\n\n# %%\n# Dataset\n# -------\n#\n# We will use the :ref:`iris_dataset`, which consists of measurements taken\n# from 3 Iris species. Our model will use the measurements to predict\n# the iris species.\n\nfrom sklearn.datasets import load_iris\n\niris = load_iris()\nX = iris.data\ny = iris.target\n\n# %%\n# For comparison, we also generate some random feature data (i.e., 20 features),\n# uncorrelated with the class labels in the iris dataset.\n\nimport numpy as np\n\nn_uncorrelated_features = 20\nrng = np.random.RandomState(seed=0)\n# Use same number of samples as in iris and 20 features\nX_rand = rng.normal(size=(X.shape[0], n_uncorrelated_features))\n\n# %%\n# Permutation test score\n# ----------------------\n#\n# Next, we calculate the\n# :func:`~sklearn.model_selection.permutation_test_score` for both, the original\n# iris dataset (where there's a strong relationship between features and labels) and\n# the randomly generated features with iris labels (where no dependency between features\n# and labels is expected). We use the\n# :class:`~sklearn.svm.SVC` classifier and :ref:`accuracy_score` to evaluate\n# the model at each round.\n#\n# :func:`~sklearn.model_selection.permutation_test_score` generates a null\n# distribution by calculating the accuracy of the classifier\n# on 1000 different permutations of the dataset, where features\n# remain the same but labels undergo different random permutations. This is the\n# distribution for the null hypothesis which states there is no dependency\n# between the features and labels. An empirical p-value is then calculated as\n# the proportion of permutations, for which the score obtained by the model trained on\n# the permutation, is greater than or equal to the score obtained using the original\n# data.\n\nfrom sklearn.model_selection import StratifiedKFold, permutation_test_score\nfrom sklearn.svm import SVC\n\nclf = SVC(kernel=\"linear\", random_state=7)\ncv = StratifiedKFold(n_splits=2, shuffle=True, random_state=0)\n\nscore_iris, perm_scores_iris, pvalue_iris = permutation_test_score(\n    clf, X, y, scoring=\"accuracy\", cv=cv, n_permutations=1000\n)\n\nscore_rand, perm_scores_rand, pvalue_rand = permutation_test_score(\n    clf, X_rand, y, scoring=\"accuracy\", cv=cv, n_permutations=1000\n)\n\n# %%\n# Original data\n# ^^^^^^^^^^^^^\n#\n# Below we plot a histogram of the permutation scores (the null\n# distribution). The red line indicates the score obtained by the classifier\n# on the original data (without permuted labels). The score is much better than those\n# obtained by using permuted data and the p-value is thus very low. This indicates that\n# there is a low likelihood that this good score would be obtained by chance\n# alone. It provides evidence that the iris dataset contains real dependency\n# between features and labels and the classifier was able to utilize this\n# to obtain good results. The low p-value can lead us to reject the null hypothesis.\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots()\n\nax.hist(perm_scores_iris, bins=20, density=True)\nax.axvline(score_iris, ls=\"--\", color=\"r\")\nscore_label = (\n    f\"Score on original\\niris data: {score_iris:.2f}\\n(p-value: {pvalue_iris:.3f})\"\n)\nax.text(0.7, 10, score_label, fontsize=12)\nax.set_xlabel(\"Accuracy score\")\n_ = ax.set_ylabel(\"Probability density\")\n\n# %%\n# Random data\n# ^^^^^^^^^^^\n#\n# Below we plot the null distribution for the randomized data. The permutation\n# scores are similar to those obtained using the original iris dataset\n# because the permutation always destroys any feature-label dependency present.\n# The score obtained on the randomized data in this case\n# though, is very poor. This results in a large p-value, confirming that there was no\n# feature-label dependency in the randomized data.\n\nfig, ax = plt.subplots()\n\nax.hist(perm_scores_rand, bins=20, density=True)\nax.set_xlim(0.13)\nax.axvline(score_rand, ls=\"--\", color=\"r\")\nscore_label = (\n    f\"Score on original\\nrandom data: {score_rand:.2f}\\n(p-value: {pvalue_rand:.3f})\"\n)\nax.text(0.14, 7.5, score_label, fontsize=12)\nax.set_xlabel(\"Accuracy score\")\nax.set_ylabel(\"Probability density\")\nplt.show()\n\n# %%\n# Another possible reason for obtaining a high p-value could be that the classifier\n# was not able to use the structure in the data. In this case, the p-value\n# would only be low for classifiers that are able to utilize the dependency\n# present. In our case above, where the data is random, all classifiers would\n# have a high p-value as there is no structure present in the data. We might or might\n# not fail to reject the null hypothesis depending on whether the p-value is high on a\n# more appropriate estimator as well.\n#\n# Finally, note that this test has been shown to produce low p-values even\n# if there is only weak structure in the data [1]_.\n#\n# .. rubric:: References\n#\n# .. [1] Ojala and Garriga. `Permutation Tests for Studying Classifier\n#        Performance\n#        <http://www.jmlr.org/papers/volume11/ojala10a/ojala10a.pdf>`_. The\n#        Journal of Machine Learning Research (2010) vol. 11\n#\n",
    "examples/model_selection/plot_train_error_vs_test_error.py": "\"\"\"\n=========================================================\nEffect of model regularization on training and test error\n=========================================================\n\nIn this example, we evaluate the impact of the regularization parameter in a\nlinear model called :class:`~sklearn.linear_model.ElasticNet`. To carry out this\nevaluation, we use a validation curve using\n:class:`~sklearn.model_selection.ValidationCurveDisplay`. This curve shows the\ntraining and test scores of the model for different values of the regularization\nparameter.\n\nOnce we identify the optimal regularization parameter, we compare the true and\nestimated coefficients of the model to determine if the model is able to recover\nthe coefficients from the noisy input data.\n\"\"\"\n\n# Authors: The scikit-learn developers\n# SPDX-License-Identifier: BSD-3-Clause\n\n# %%\n# Generate sample data\n# --------------------\n#\n# We generate a regression dataset that contains many features relative to the\n# number of samples. However, only 10% of the features are informative. In this context,\n# linear models exposing L1 penalization are commonly used to recover a sparse\n# set of coefficients.\nfrom sklearn.datasets import make_regression\nfrom sklearn.model_selection import train_test_split\n\nn_samples_train, n_samples_test, n_features = 150, 300, 500\nX, y, true_coef = make_regression(\n    n_samples=n_samples_train + n_samples_test,\n    n_features=n_features,\n    n_informative=50,\n    shuffle=False,\n    noise=1.0,\n    coef=True,\n    random_state=42,\n)\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, train_size=n_samples_train, test_size=n_samples_test, shuffle=False\n)\n\n# %%\n# Model definition\n# ----------------\n#\n# Here, we do not use a model that only exposes an L1 penalty. Instead, we use\n# an :class:`~sklearn.linear_model.ElasticNet` model that exposes both L1 and L2\n# penalties.\n#\n# We fix the `l1_ratio` parameter such that the solution found by the model is still\n# sparse. Therefore, this type of model tries to find a sparse solution but at the same\n# time also tries to shrink all coefficients towards zero.\n#\n# In addition, we force the coefficients of the model to be positive since we know that\n# `make_regression` generates a response with a positive signal. So we use this\n# pre-knowledge to get a better model.\n\nfrom sklearn.linear_model import ElasticNet\n\nenet = ElasticNet(l1_ratio=0.9, positive=True, max_iter=10_000)\n\n\n# %%\n# Evaluate the impact of the regularization parameter\n# ---------------------------------------------------\n#\n# To evaluate the impact of the regularization parameter, we use a validation\n# curve. This curve shows the training and test scores of the model for different\n# values of the regularization parameter.\n#\n# The regularization `alpha` is a parameter applied to the coefficients of the model:\n# when it tends to zero, no regularization is applied and the model tries to fit the\n# training data with the least amount of error. However, it leads to overfitting when\n# features are noisy. When `alpha` increases, the model coefficients are constrained,\n# and thus the model cannot fit the training data as closely, avoiding overfitting.\n# However, if too much regularization is applied, the model underfits the data and\n# is not able to properly capture the signal.\n#\n# The validation curve helps in finding a good trade-off between both extremes: the\n# model is not regularized and thus flexible enough to fit the signal, but not too\n# flexible to overfit. The :class:`~sklearn.model_selection.ValidationCurveDisplay`\n# allows us to display the training and validation scores across a range of alpha\n# values.\nimport numpy as np\n\nfrom sklearn.model_selection import ValidationCurveDisplay\n\nalphas = np.logspace(-5, 1, 60)\ndisp = ValidationCurveDisplay.from_estimator(\n    enet,\n    X_train,\n    y_train,\n    param_name=\"alpha\",\n    param_range=alphas,\n    scoring=\"r2\",\n    n_jobs=2,\n    score_type=\"both\",\n)\ndisp.ax_.set(\n    title=r\"Validation Curve for ElasticNet (R$^2$ Score)\",\n    xlabel=r\"alpha (regularization strength)\",\n    ylabel=\"R$^2$ Score\",\n)\n\ntest_scores_mean = disp.test_scores.mean(axis=1)\nidx_avg_max_test_score = np.argmax(test_scores_mean)\ndisp.ax_.vlines(\n    alphas[idx_avg_max_test_score],\n    disp.ax_.get_ylim()[0],\n    test_scores_mean[idx_avg_max_test_score],\n    color=\"k\",\n    linewidth=2,\n    linestyle=\"--\",\n    label=f\"Optimum on test\\n$\\\\alpha$ = {alphas[idx_avg_max_test_score]:.2e}\",\n)\n_ = disp.ax_.legend(loc=\"lower right\")\n\n# %%\n# To find the optimal regularization parameter, we can select the value of `alpha`\n# that maximizes the validation score.\n#\n# Coefficients comparison\n# -----------------------\n#\n# Now that we have identified the optimal regularization parameter, we can compare the\n# true coefficients and the estimated coefficients.\n#\n# First, let's set the regularization parameter to the optimal value and fit the\n# model on the training data. In addition, we'll show the test score for this model.\nenet.set_params(alpha=alphas[idx_avg_max_test_score]).fit(X_train, y_train)\nprint(\n    f\"Test score: {enet.score(X_test, y_test):.3f}\",\n)\n\n# %%\n# Now, we plot the true coefficients and the estimated coefficients.\nimport matplotlib.pyplot as plt\n\nfig, axs = plt.subplots(ncols=2, figsize=(12, 6), sharex=True, sharey=True)\nfor ax, coef, title in zip(axs, [true_coef, enet.coef_], [\"True\", \"Model\"]):\n    ax.stem(coef)\n    ax.set(\n        title=f\"{title} Coefficients\",\n        xlabel=\"Feature Index\",\n        ylabel=\"Coefficient Value\",\n    )\nfig.suptitle(\n    \"Comparison of the coefficients of the true generative model and \\n\"\n    \"the estimated elastic net coefficients\"\n)\n\nplt.show()\n\n# %%\n# While the original coefficients are sparse, the estimated coefficients are not\n# as sparse. The reason is that we fixed the `l1_ratio` parameter to 0.9. We could\n# force the model to get a sparser solution by increasing the `l1_ratio` parameter.\n#\n# However, we observed that for the estimated coefficients that are close to zero in\n# the true generative model, our model shrinks them towards zero. So we don't recover\n# the true coefficients, but we get a sensible outcome in line with the performance\n# obtained on the test set.\n",
    "sklearn/_loss/tests/test_link.py": "import numpy as np\nimport pytest\nfrom numpy.testing import assert_allclose, assert_array_equal\n\nfrom sklearn._loss.link import (\n    _LINKS,\n    HalfLogitLink,\n    Interval,\n    MultinomialLogit,\n    _inclusive_low_high,\n)\n\nLINK_FUNCTIONS = list(_LINKS.values())\n\n\ndef test_interval_raises():\n    \"\"\"Test that interval with low > high raises ValueError.\"\"\"\n    with pytest.raises(\n        ValueError, match=\"One must have low <= high; got low=1, high=0.\"\n    ):\n        Interval(1, 0, False, False)\n\n\n@pytest.mark.parametrize(\n    \"interval\",\n    [\n        Interval(0, 1, False, False),\n        Interval(0, 1, False, True),\n        Interval(0, 1, True, False),\n        Interval(0, 1, True, True),\n        Interval(-np.inf, np.inf, False, False),\n        Interval(-np.inf, np.inf, False, True),\n        Interval(-np.inf, np.inf, True, False),\n        Interval(-np.inf, np.inf, True, True),\n        Interval(-10, -1, False, False),\n        Interval(-10, -1, False, True),\n        Interval(-10, -1, True, False),\n        Interval(-10, -1, True, True),\n    ],\n)\ndef test_is_in_range(interval):\n    # make sure low and high are always within the interval, used for linspace\n    low, high = _inclusive_low_high(interval)\n\n    x = np.linspace(low, high, num=10)\n    assert interval.includes(x)\n\n    # x contains lower bound\n    assert interval.includes(np.r_[x, interval.low]) == interval.low_inclusive\n\n    # x contains upper bound\n    assert interval.includes(np.r_[x, interval.high]) == interval.high_inclusive\n\n    # x contains upper and lower bound\n    assert interval.includes(np.r_[x, interval.low, interval.high]) == (\n        interval.low_inclusive and interval.high_inclusive\n    )\n\n\n@pytest.mark.parametrize(\"link\", LINK_FUNCTIONS)\ndef test_link_inverse_identity(link, global_random_seed):\n    # Test that link of inverse gives identity.\n    rng = np.random.RandomState(global_random_seed)\n    link = link()\n    n_samples, n_classes = 100, None\n    # The values for `raw_prediction` are limited from -20 to 20 because in the\n    # class `LogitLink` the term `expit(x)` comes very close to 1 for large\n    # positive x and therefore loses precision.\n    if link.is_multiclass:\n        n_classes = 10\n        raw_prediction = rng.uniform(low=-20, high=20, size=(n_samples, n_classes))\n        if isinstance(link, MultinomialLogit):\n            raw_prediction = link.symmetrize_raw_prediction(raw_prediction)\n    elif isinstance(link, HalfLogitLink):\n        raw_prediction = rng.uniform(low=-10, high=10, size=(n_samples))\n    else:\n        raw_prediction = rng.uniform(low=-20, high=20, size=(n_samples))\n\n    assert_allclose(link.link(link.inverse(raw_prediction)), raw_prediction)\n    y_pred = link.inverse(raw_prediction)\n    assert_allclose(link.inverse(link.link(y_pred)), y_pred)\n\n\n@pytest.mark.parametrize(\"link\", LINK_FUNCTIONS)\ndef test_link_out_argument(link):\n    # Test that out argument gets assigned the result.\n    rng = np.random.RandomState(42)\n    link = link()\n    n_samples, n_classes = 100, None\n    if link.is_multiclass:\n        n_classes = 10\n        raw_prediction = rng.normal(loc=0, scale=10, size=(n_samples, n_classes))\n        if isinstance(link, MultinomialLogit):\n            raw_prediction = link.symmetrize_raw_prediction(raw_prediction)\n    else:\n        # So far, the valid interval of raw_prediction is (-inf, inf) and\n        # we do not need to distinguish.\n        raw_prediction = rng.uniform(low=-10, high=10, size=(n_samples))\n\n    y_pred = link.inverse(raw_prediction, out=None)\n    out = np.empty_like(raw_prediction)\n    y_pred_2 = link.inverse(raw_prediction, out=out)\n    assert_allclose(y_pred, out)\n    assert_array_equal(out, y_pred_2)\n    assert np.shares_memory(out, y_pred_2)\n\n    out = np.empty_like(y_pred)\n    raw_prediction_2 = link.link(y_pred, out=out)\n    assert_allclose(raw_prediction, out)\n    assert_array_equal(out, raw_prediction_2)\n    assert np.shares_memory(out, raw_prediction_2)\n",
    "sklearn/_loss/tests/test_loss.py": "import pickle\n\nimport numpy as np\nimport pytest\nfrom numpy.testing import assert_allclose, assert_array_equal\nfrom pytest import approx\nfrom scipy.optimize import (\n    LinearConstraint,\n    minimize,\n    minimize_scalar,\n    newton,\n)\nfrom scipy.special import logsumexp\n\nfrom sklearn._loss.link import IdentityLink, _inclusive_low_high\nfrom sklearn._loss.loss import (\n    _LOSSES,\n    AbsoluteError,\n    BaseLoss,\n    HalfBinomialLoss,\n    HalfGammaLoss,\n    HalfMultinomialLoss,\n    HalfPoissonLoss,\n    HalfSquaredError,\n    HalfTweedieLoss,\n    HalfTweedieLossIdentity,\n    HuberLoss,\n    PinballLoss,\n)\nfrom sklearn.utils import assert_all_finite\nfrom sklearn.utils._testing import create_memmap_backed_data, skip_if_32bit\nfrom sklearn.utils.fixes import _IS_WASM\n\nALL_LOSSES = list(_LOSSES.values())\n\nLOSS_INSTANCES = [loss() for loss in ALL_LOSSES]\n# HalfTweedieLoss(power=1.5) is already there as default\nLOSS_INSTANCES += [\n    PinballLoss(quantile=0.25),\n    HuberLoss(quantile=0.75),\n    HalfTweedieLoss(power=-1.5),\n    HalfTweedieLoss(power=0),\n    HalfTweedieLoss(power=1),\n    HalfTweedieLoss(power=2),\n    HalfTweedieLoss(power=3.0),\n    HalfTweedieLossIdentity(power=0),\n    HalfTweedieLossIdentity(power=1),\n    HalfTweedieLossIdentity(power=2),\n    HalfTweedieLossIdentity(power=3.0),\n]\n\n\ndef loss_instance_name(param):\n    if isinstance(param, BaseLoss):\n        loss = param\n        name = loss.__class__.__name__\n        if isinstance(loss, PinballLoss):\n            name += f\"(quantile={loss.closs.quantile})\"\n        elif isinstance(loss, HuberLoss):\n            name += f\"(quantile={loss.quantile}\"\n        elif hasattr(loss, \"closs\") and hasattr(loss.closs, \"power\"):\n            name += f\"(power={loss.closs.power})\"\n        return name\n    else:\n        return str(param)\n\n\ndef random_y_true_raw_prediction(\n    loss, n_samples, y_bound=(-100, 100), raw_bound=(-5, 5), seed=42\n):\n    \"\"\"Random generate y_true and raw_prediction in valid range.\"\"\"\n    rng = np.random.RandomState(seed)\n    if loss.is_multiclass:\n        raw_prediction = np.empty((n_samples, loss.n_classes))\n        raw_prediction.flat[:] = rng.uniform(\n            low=raw_bound[0],\n            high=raw_bound[1],\n            size=n_samples * loss.n_classes,\n        )\n        y_true = np.arange(n_samples).astype(float) % loss.n_classes\n    else:\n        # If link is identity, we must respect the interval of y_pred:\n        if isinstance(loss.link, IdentityLink):\n            low, high = _inclusive_low_high(loss.interval_y_pred)\n            low = np.amax([low, raw_bound[0]])\n            high = np.amin([high, raw_bound[1]])\n            raw_bound = (low, high)\n        raw_prediction = rng.uniform(\n            low=raw_bound[0], high=raw_bound[1], size=n_samples\n        )\n        # generate a y_true in valid range\n        low, high = _inclusive_low_high(loss.interval_y_true)\n        low = max(low, y_bound[0])\n        high = min(high, y_bound[1])\n        y_true = rng.uniform(low, high, size=n_samples)\n        # set some values at special boundaries\n        if loss.interval_y_true.low == 0 and loss.interval_y_true.low_inclusive:\n            y_true[:: (n_samples // 3)] = 0\n        if loss.interval_y_true.high == 1 and loss.interval_y_true.high_inclusive:\n            y_true[1 :: (n_samples // 3)] = 1\n\n    return y_true, raw_prediction\n\n\ndef numerical_derivative(func, x, eps):\n    \"\"\"Helper function for numerical (first) derivatives.\"\"\"\n    # For numerical derivatives, see\n    # https://en.wikipedia.org/wiki/Numerical_differentiation\n    # https://en.wikipedia.org/wiki/Finite_difference_coefficient\n    # We use central finite differences of accuracy 4.\n    h = np.full_like(x, fill_value=eps)\n    f_minus_2h = func(x - 2 * h)\n    f_minus_1h = func(x - h)\n    f_plus_1h = func(x + h)\n    f_plus_2h = func(x + 2 * h)\n    return (-f_plus_2h + 8 * f_plus_1h - 8 * f_minus_1h + f_minus_2h) / (12.0 * eps)\n\n\n@pytest.mark.parametrize(\"loss\", LOSS_INSTANCES, ids=loss_instance_name)\ndef test_loss_boundary(loss):\n    \"\"\"Test interval ranges of y_true and y_pred in losses.\"\"\"\n    # make sure low and high are always within the interval, used for linspace\n    if loss.is_multiclass:\n        n_classes = 3  # default value\n        y_true = np.tile(np.linspace(0, n_classes - 1, num=n_classes), 3)\n    else:\n        low, high = _inclusive_low_high(loss.interval_y_true)\n        y_true = np.linspace(low, high, num=10)\n\n    # add boundaries if they are included\n    if loss.interval_y_true.low_inclusive:\n        y_true = np.r_[y_true, loss.interval_y_true.low]\n    if loss.interval_y_true.high_inclusive:\n        y_true = np.r_[y_true, loss.interval_y_true.high]\n\n    assert loss.in_y_true_range(y_true)\n\n    n = y_true.shape[0]\n    low, high = _inclusive_low_high(loss.interval_y_pred)\n    if loss.is_multiclass:\n        y_pred = np.empty((n, n_classes))\n        y_pred[:, 0] = np.linspace(low, high, num=n)\n        y_pred[:, 1] = 0.5 * (1 - y_pred[:, 0])\n        y_pred[:, 2] = 0.5 * (1 - y_pred[:, 0])\n    else:\n        y_pred = np.linspace(low, high, num=n)\n\n    assert loss.in_y_pred_range(y_pred)\n\n    # calculating losses should not fail\n    raw_prediction = loss.link.link(y_pred)\n    loss.loss(y_true=y_true, raw_prediction=raw_prediction)\n\n\n# Fixture to test valid value ranges.\nY_COMMON_PARAMS = [\n    # (loss, [y success], [y fail])\n    (HalfSquaredError(), [-100, 0, 0.1, 100], [-np.inf, np.inf]),\n    (AbsoluteError(), [-100, 0, 0.1, 100], [-np.inf, np.inf]),\n    (PinballLoss(), [-100, 0, 0.1, 100], [-np.inf, np.inf]),\n    (HuberLoss(), [-100, 0, 0.1, 100], [-np.inf, np.inf]),\n    (HalfPoissonLoss(), [0.1, 100], [-np.inf, -3, -0.1, np.inf]),\n    (HalfGammaLoss(), [0.1, 100], [-np.inf, -3, -0.1, 0, np.inf]),\n    (HalfTweedieLoss(power=-3), [0.1, 100], [-np.inf, np.inf]),\n    (HalfTweedieLoss(power=0), [0.1, 100], [-np.inf, np.inf]),\n    (HalfTweedieLoss(power=1.5), [0.1, 100], [-np.inf, -3, -0.1, np.inf]),\n    (HalfTweedieLoss(power=2), [0.1, 100], [-np.inf, -3, -0.1, 0, np.inf]),\n    (HalfTweedieLoss(power=3), [0.1, 100], [-np.inf, -3, -0.1, 0, np.inf]),\n    (HalfTweedieLossIdentity(power=-3), [0.1, 100], [-np.inf, np.inf]),\n    (HalfTweedieLossIdentity(power=0), [-3, -0.1, 0, 0.1, 100], [-np.inf, np.inf]),\n    (HalfTweedieLossIdentity(power=1.5), [0.1, 100], [-np.inf, -3, -0.1, np.inf]),\n    (HalfTweedieLossIdentity(power=2), [0.1, 100], [-np.inf, -3, -0.1, 0, np.inf]),\n    (HalfTweedieLossIdentity(power=3), [0.1, 100], [-np.inf, -3, -0.1, 0, np.inf]),\n    (HalfBinomialLoss(), [0.1, 0.5, 0.9], [-np.inf, -1, 2, np.inf]),\n    (HalfMultinomialLoss(), [], [-np.inf, -1, 1.1, np.inf]),\n]\n# y_pred and y_true do not always have the same domain (valid value range).\n# Hence, we define extra sets of parameters for each of them.\nY_TRUE_PARAMS = [  # type: ignore\n    # (loss, [y success], [y fail])\n    (HalfPoissonLoss(), [0], []),\n    (HuberLoss(), [0], []),\n    (HalfTweedieLoss(power=-3), [-100, -0.1, 0], []),\n    (HalfTweedieLoss(power=0), [-100, 0], []),\n    (HalfTweedieLoss(power=1.5), [0], []),\n    (HalfTweedieLossIdentity(power=-3), [-100, -0.1, 0], []),\n    (HalfTweedieLossIdentity(power=0), [-100, 0], []),\n    (HalfTweedieLossIdentity(power=1.5), [0], []),\n    (HalfBinomialLoss(), [0, 1], []),\n    (HalfMultinomialLoss(), [0.0, 1.0, 2], []),\n]\nY_PRED_PARAMS = [\n    # (loss, [y success], [y fail])\n    (HalfPoissonLoss(), [], [0]),\n    (HalfTweedieLoss(power=-3), [], [-3, -0.1, 0]),\n    (HalfTweedieLoss(power=0), [], [-3, -0.1, 0]),\n    (HalfTweedieLoss(power=1.5), [], [0]),\n    (HalfTweedieLossIdentity(power=-3), [], [-3, -0.1, 0]),\n    (HalfTweedieLossIdentity(power=0), [-3, -0.1, 0], []),\n    (HalfTweedieLossIdentity(power=1.5), [], [0]),\n    (HalfBinomialLoss(), [], [0, 1]),\n    (HalfMultinomialLoss(), [0.1, 0.5], [0, 1]),\n]\n\n\n@pytest.mark.parametrize(\n    \"loss, y_true_success, y_true_fail\", Y_COMMON_PARAMS + Y_TRUE_PARAMS\n)\ndef test_loss_boundary_y_true(loss, y_true_success, y_true_fail):\n    \"\"\"Test boundaries of y_true for loss functions.\"\"\"\n    for y in y_true_success:\n        assert loss.in_y_true_range(np.array([y]))\n    for y in y_true_fail:\n        assert not loss.in_y_true_range(np.array([y]))\n\n\n@pytest.mark.parametrize(\n    \"loss, y_pred_success, y_pred_fail\", Y_COMMON_PARAMS + Y_PRED_PARAMS  # type: ignore\n)\ndef test_loss_boundary_y_pred(loss, y_pred_success, y_pred_fail):\n    \"\"\"Test boundaries of y_pred for loss functions.\"\"\"\n    for y in y_pred_success:\n        assert loss.in_y_pred_range(np.array([y]))\n    for y in y_pred_fail:\n        assert not loss.in_y_pred_range(np.array([y]))\n\n\n@pytest.mark.parametrize(\n    \"loss, y_true, raw_prediction, loss_true, gradient_true, hessian_true\",\n    [\n        (HalfSquaredError(), 1.0, 5.0, 8, 4, 1),\n        (AbsoluteError(), 1.0, 5.0, 4.0, 1.0, None),\n        (PinballLoss(quantile=0.5), 1.0, 5.0, 2, 0.5, None),\n        (PinballLoss(quantile=0.25), 1.0, 5.0, 4 * (1 - 0.25), 1 - 0.25, None),\n        (PinballLoss(quantile=0.25), 5.0, 1.0, 4 * 0.25, -0.25, None),\n        (HuberLoss(quantile=0.5, delta=3), 1.0, 5.0, 3 * (4 - 3 / 2), None, None),\n        (HuberLoss(quantile=0.5, delta=3), 1.0, 3.0, 0.5 * 2**2, None, None),\n        (HalfPoissonLoss(), 2.0, np.log(4), 4 - 2 * np.log(4), 4 - 2, 4),\n        (HalfGammaLoss(), 2.0, np.log(4), np.log(4) + 2 / 4, 1 - 2 / 4, 2 / 4),\n        (HalfTweedieLoss(power=3), 2.0, np.log(4), -1 / 4 + 1 / 4**2, None, None),\n        (HalfTweedieLossIdentity(power=1), 2.0, 4.0, 2 - 2 * np.log(2), None, None),\n        (HalfTweedieLossIdentity(power=2), 2.0, 4.0, np.log(2) - 1 / 2, None, None),\n        (\n            HalfTweedieLossIdentity(power=3),\n            2.0,\n            4.0,\n            -1 / 4 + 1 / 4**2 + 1 / 2 / 2,\n            None,\n            None,\n        ),\n        (\n            HalfBinomialLoss(),\n            0.25,\n            np.log(4),\n            np.log1p(4) - 0.25 * np.log(4),\n            None,\n            None,\n        ),\n        # Extreme log loss cases, checked with mpmath:\n        # import mpmath as mp\n        #\n        # # Stolen from scipy\n        # def mpf2float(x):\n        #     return float(mp.nstr(x, 17, min_fixed=0, max_fixed=0))\n        #\n        # def mp_logloss(y_true, raw):\n        #     with mp.workdps(100):\n        #         y_true, raw = mp.mpf(float(y_true)), mp.mpf(float(raw))\n        #         out = mp.log1p(mp.exp(raw)) - y_true * raw\n        #     return mpf2float(out)\n        #\n        # def mp_gradient(y_true, raw):\n        #     with mp.workdps(100):\n        #         y_true, raw = mp.mpf(float(y_true)), mp.mpf(float(raw))\n        #         out = mp.mpf(1) / (mp.mpf(1) + mp.exp(-raw)) - y_true\n        #     return mpf2float(out)\n        #\n        # def mp_hessian(y_true, raw):\n        #     with mp.workdps(100):\n        #         y_true, raw = mp.mpf(float(y_true)), mp.mpf(float(raw))\n        #         p = mp.mpf(1) / (mp.mpf(1) + mp.exp(-raw))\n        #         out = p * (mp.mpf(1) - p)\n        #     return mpf2float(out)\n        #\n        # y, raw = 0.0, 37.\n        # mp_logloss(y, raw), mp_gradient(y, raw), mp_hessian(y, raw)\n        (HalfBinomialLoss(), 0.0, -1e20, 0, 0, 0),\n        (HalfBinomialLoss(), 1.0, -1e20, 1e20, -1, 0),\n        (HalfBinomialLoss(), 0.0, -1e3, 0, 0, 0),\n        (HalfBinomialLoss(), 1.0, -1e3, 1e3, -1, 0),\n        (HalfBinomialLoss(), 1.0, -37.5, 37.5, -1, 0),\n        (HalfBinomialLoss(), 1.0, -37.0, 37, 1e-16 - 1, 8.533047625744065e-17),\n        (HalfBinomialLoss(), 0.0, -37.0, *[8.533047625744065e-17] * 3),\n        (HalfBinomialLoss(), 1.0, -36.9, 36.9, 1e-16 - 1, 9.430476078526806e-17),\n        (HalfBinomialLoss(), 0.0, -36.9, *[9.430476078526806e-17] * 3),\n        (HalfBinomialLoss(), 0.0, 37.0, 37, 1 - 1e-16, 8.533047625744065e-17),\n        (HalfBinomialLoss(), 1.0, 37.0, *[8.533047625744066e-17] * 3),\n        (HalfBinomialLoss(), 0.0, 37.5, 37.5, 1, 5.175555005801868e-17),\n        (HalfBinomialLoss(), 0.0, 232.8, 232.8, 1, 1.4287342391028437e-101),\n        (HalfBinomialLoss(), 1.0, 1e20, 0, 0, 0),\n        (HalfBinomialLoss(), 0.0, 1e20, 1e20, 1, 0),\n        (\n            HalfBinomialLoss(),\n            1.0,\n            232.8,\n            0,\n            -1.4287342391028437e-101,\n            1.4287342391028437e-101,\n        ),\n        (HalfBinomialLoss(), 1.0, 232.9, 0, 0, 0),\n        (HalfBinomialLoss(), 1.0, 1e3, 0, 0, 0),\n        (HalfBinomialLoss(), 0.0, 1e3, 1e3, 1, 0),\n        (\n            HalfMultinomialLoss(n_classes=3),\n            0.0,\n            [0.2, 0.5, 0.3],\n            logsumexp([0.2, 0.5, 0.3]) - 0.2,\n            None,\n            None,\n        ),\n        (\n            HalfMultinomialLoss(n_classes=3),\n            1.0,\n            [0.2, 0.5, 0.3],\n            logsumexp([0.2, 0.5, 0.3]) - 0.5,\n            None,\n            None,\n        ),\n        (\n            HalfMultinomialLoss(n_classes=3),\n            2.0,\n            [0.2, 0.5, 0.3],\n            logsumexp([0.2, 0.5, 0.3]) - 0.3,\n            None,\n            None,\n        ),\n        (\n            HalfMultinomialLoss(n_classes=3),\n            2.0,\n            [1e4, 0, 7e-7],\n            logsumexp([1e4, 0, 7e-7]) - (7e-7),\n            None,\n            None,\n        ),\n    ],\n    ids=loss_instance_name,\n)\ndef test_loss_on_specific_values(\n    loss, y_true, raw_prediction, loss_true, gradient_true, hessian_true\n):\n    \"\"\"Test losses, gradients and hessians at specific values.\"\"\"\n    loss1 = loss(y_true=np.array([y_true]), raw_prediction=np.array([raw_prediction]))\n    grad1 = loss.gradient(\n        y_true=np.array([y_true]), raw_prediction=np.array([raw_prediction])\n    )\n    loss2, grad2 = loss.loss_gradient(\n        y_true=np.array([y_true]), raw_prediction=np.array([raw_prediction])\n    )\n    grad3, hess = loss.gradient_hessian(\n        y_true=np.array([y_true]), raw_prediction=np.array([raw_prediction])\n    )\n\n    assert loss1 == approx(loss_true, rel=1e-15, abs=1e-15)\n    assert loss2 == approx(loss_true, rel=1e-15, abs=1e-15)\n\n    if gradient_true is not None:\n        assert grad1 == approx(gradient_true, rel=1e-15, abs=1e-15)\n        assert grad2 == approx(gradient_true, rel=1e-15, abs=1e-15)\n        assert grad3 == approx(gradient_true, rel=1e-15, abs=1e-15)\n\n    if hessian_true is not None:\n        assert hess == approx(hessian_true, rel=1e-15, abs=1e-15)\n\n\n@pytest.mark.parametrize(\"loss\", ALL_LOSSES)\n@pytest.mark.parametrize(\"readonly_memmap\", [False, True])\n@pytest.mark.parametrize(\"dtype_in\", [np.float32, np.float64])\n@pytest.mark.parametrize(\"dtype_out\", [np.float32, np.float64])\n@pytest.mark.parametrize(\"sample_weight\", [None, 1])\n@pytest.mark.parametrize(\"out1\", [None, 1])\n@pytest.mark.parametrize(\"out2\", [None, 1])\n@pytest.mark.parametrize(\"n_threads\", [1, 2])\ndef test_loss_dtype(\n    loss, readonly_memmap, dtype_in, dtype_out, sample_weight, out1, out2, n_threads\n):\n    \"\"\"Test acceptance of dtypes, readonly and writeable arrays in loss functions.\n\n    Check that loss accepts if all input arrays are either all float32 or all\n    float64, and all output arrays are either all float32 or all float64.\n\n    Also check that input arrays can be readonly, e.g. memory mapped.\n    \"\"\"\n    if _IS_WASM and readonly_memmap:  # pragma: nocover\n        pytest.xfail(reason=\"memmap not fully supported\")\n\n    loss = loss()\n    # generate a y_true and raw_prediction in valid range\n    n_samples = 5\n    y_true, raw_prediction = random_y_true_raw_prediction(\n        loss=loss,\n        n_samples=n_samples,\n        y_bound=(-100, 100),\n        raw_bound=(-10, 10),\n        seed=42,\n    )\n    y_true = y_true.astype(dtype_in)\n    raw_prediction = raw_prediction.astype(dtype_in)\n\n    if sample_weight is not None:\n        sample_weight = np.array([2.0] * n_samples, dtype=dtype_in)\n    if out1 is not None:\n        out1 = np.empty_like(y_true, dtype=dtype_out)\n    if out2 is not None:\n        out2 = np.empty_like(raw_prediction, dtype=dtype_out)\n\n    if readonly_memmap:\n        y_true = create_memmap_backed_data(y_true)\n        raw_prediction = create_memmap_backed_data(raw_prediction)\n        if sample_weight is not None:\n            sample_weight = create_memmap_backed_data(sample_weight)\n\n    l = loss.loss(\n        y_true=y_true,\n        raw_prediction=raw_prediction,\n        sample_weight=sample_weight,\n        loss_out=out1,\n        n_threads=n_threads,\n    )\n    assert l is out1 if out1 is not None else True\n    g = loss.gradient(\n        y_true=y_true,\n        raw_prediction=raw_prediction,\n        sample_weight=sample_weight,\n        gradient_out=out2,\n        n_threads=n_threads,\n    )\n    assert g is out2 if out2 is not None else True\n    l, g = loss.loss_gradient(\n        y_true=y_true,\n        raw_prediction=raw_prediction,\n        sample_weight=sample_weight,\n        loss_out=out1,\n        gradient_out=out2,\n        n_threads=n_threads,\n    )\n    assert l is out1 if out1 is not None else True\n    assert g is out2 if out2 is not None else True\n    if out1 is not None and loss.is_multiclass:\n        out1 = np.empty_like(raw_prediction, dtype=dtype_out)\n    g, h = loss.gradient_hessian(\n        y_true=y_true,\n        raw_prediction=raw_prediction,\n        sample_weight=sample_weight,\n        gradient_out=out1,\n        hessian_out=out2,\n        n_threads=n_threads,\n    )\n    assert g is out1 if out1 is not None else True\n    assert h is out2 if out2 is not None else True\n    loss(y_true=y_true, raw_prediction=raw_prediction, sample_weight=sample_weight)\n    loss.fit_intercept_only(y_true=y_true, sample_weight=sample_weight)\n    loss.constant_to_optimal_zero(y_true=y_true, sample_weight=sample_weight)\n    if hasattr(loss, \"predict_proba\"):\n        loss.predict_proba(raw_prediction=raw_prediction)\n    if hasattr(loss, \"gradient_proba\"):\n        g, p = loss.gradient_proba(\n            y_true=y_true,\n            raw_prediction=raw_prediction,\n            sample_weight=sample_weight,\n            gradient_out=out1,\n            proba_out=out2,\n            n_threads=n_threads,\n        )\n        assert g is out1 if out1 is not None else True\n        assert p is out2 if out2 is not None else True\n\n\n@pytest.mark.parametrize(\"loss\", LOSS_INSTANCES, ids=loss_instance_name)\n@pytest.mark.parametrize(\"sample_weight\", [None, \"range\"])\ndef test_loss_same_as_C_functions(loss, sample_weight):\n    \"\"\"Test that Python and Cython functions return same results.\"\"\"\n    y_true, raw_prediction = random_y_true_raw_prediction(\n        loss=loss,\n        n_samples=20,\n        y_bound=(-100, 100),\n        raw_bound=(-10, 10),\n        seed=42,\n    )\n    if sample_weight == \"range\":\n        sample_weight = np.linspace(1, y_true.shape[0], num=y_true.shape[0])\n\n    out_l1 = np.empty_like(y_true)\n    out_l2 = np.empty_like(y_true)\n    out_g1 = np.empty_like(raw_prediction)\n    out_g2 = np.empty_like(raw_prediction)\n    out_h1 = np.empty_like(raw_prediction)\n    out_h2 = np.empty_like(raw_prediction)\n    loss.loss(\n        y_true=y_true,\n        raw_prediction=raw_prediction,\n        sample_weight=sample_weight,\n        loss_out=out_l1,\n    )\n    loss.closs.loss(\n        y_true=y_true,\n        raw_prediction=raw_prediction,\n        sample_weight=sample_weight,\n        loss_out=out_l2,\n    ),\n    assert_allclose(out_l1, out_l2)\n    loss.gradient(\n        y_true=y_true,\n        raw_prediction=raw_prediction,\n        sample_weight=sample_weight,\n        gradient_out=out_g1,\n    )\n    loss.closs.gradient(\n        y_true=y_true,\n        raw_prediction=raw_prediction,\n        sample_weight=sample_weight,\n        gradient_out=out_g2,\n    )\n    assert_allclose(out_g1, out_g2)\n    loss.closs.loss_gradient(\n        y_true=y_true,\n        raw_prediction=raw_prediction,\n        sample_weight=sample_weight,\n        loss_out=out_l1,\n        gradient_out=out_g1,\n    )\n    loss.closs.loss_gradient(\n        y_true=y_true,\n        raw_prediction=raw_prediction,\n        sample_weight=sample_weight,\n        loss_out=out_l2,\n        gradient_out=out_g2,\n    )\n    assert_allclose(out_l1, out_l2)\n    assert_allclose(out_g1, out_g2)\n    loss.gradient_hessian(\n        y_true=y_true,\n        raw_prediction=raw_prediction,\n        sample_weight=sample_weight,\n        gradient_out=out_g1,\n        hessian_out=out_h1,\n    )\n    loss.closs.gradient_hessian(\n        y_true=y_true,\n        raw_prediction=raw_prediction,\n        sample_weight=sample_weight,\n        gradient_out=out_g2,\n        hessian_out=out_h2,\n    )\n    assert_allclose(out_g1, out_g2)\n    assert_allclose(out_h1, out_h2)\n\n\n@pytest.mark.parametrize(\"loss\", LOSS_INSTANCES, ids=loss_instance_name)\n@pytest.mark.parametrize(\"sample_weight\", [None, \"range\"])\ndef test_loss_gradients_are_the_same(loss, sample_weight, global_random_seed):\n    \"\"\"Test that loss and gradient are the same across different functions.\n\n    Also test that output arguments contain correct results.\n    \"\"\"\n    y_true, raw_prediction = random_y_true_raw_prediction(\n        loss=loss,\n        n_samples=20,\n        y_bound=(-100, 100),\n        raw_bound=(-10, 10),\n        seed=global_random_seed,\n    )\n    if sample_weight == \"range\":\n        sample_weight = np.linspace(1, y_true.shape[0], num=y_true.shape[0])\n\n    out_l1 = np.empty_like(y_true)\n    out_l2 = np.empty_like(y_true)\n    out_g1 = np.empty_like(raw_prediction)\n    out_g2 = np.empty_like(raw_prediction)\n    out_g3 = np.empty_like(raw_prediction)\n    out_h3 = np.empty_like(raw_prediction)\n\n    l1 = loss.loss(\n        y_true=y_true,\n        raw_prediction=raw_prediction,\n        sample_weight=sample_weight,\n        loss_out=out_l1,\n    )\n    g1 = loss.gradient(\n        y_true=y_true,\n        raw_prediction=raw_prediction,\n        sample_weight=sample_weight,\n        gradient_out=out_g1,\n    )\n    l2, g2 = loss.loss_gradient(\n        y_true=y_true,\n        raw_prediction=raw_prediction,\n        sample_weight=sample_weight,\n        loss_out=out_l2,\n        gradient_out=out_g2,\n    )\n    g3, h3 = loss.gradient_hessian(\n        y_true=y_true,\n        raw_prediction=raw_prediction,\n        sample_weight=sample_weight,\n        gradient_out=out_g3,\n        hessian_out=out_h3,\n    )\n    assert_allclose(l1, l2)\n    assert_array_equal(l1, out_l1)\n    assert np.shares_memory(l1, out_l1)\n    assert_array_equal(l2, out_l2)\n    assert np.shares_memory(l2, out_l2)\n    assert_allclose(g1, g2)\n    assert_allclose(g1, g3)\n    assert_array_equal(g1, out_g1)\n    assert np.shares_memory(g1, out_g1)\n    assert_array_equal(g2, out_g2)\n    assert np.shares_memory(g2, out_g2)\n    assert_array_equal(g3, out_g3)\n    assert np.shares_memory(g3, out_g3)\n\n    if hasattr(loss, \"gradient_proba\"):\n        assert loss.is_multiclass  # only for HalfMultinomialLoss\n        out_g4 = np.empty_like(raw_prediction)\n        out_proba = np.empty_like(raw_prediction)\n        g4, proba = loss.gradient_proba(\n            y_true=y_true,\n            raw_prediction=raw_prediction,\n            sample_weight=sample_weight,\n            gradient_out=out_g4,\n            proba_out=out_proba,\n        )\n        assert_allclose(g1, out_g4)\n        assert_allclose(g1, g4)\n        assert_allclose(proba, out_proba)\n        assert_allclose(np.sum(proba, axis=1), 1, rtol=1e-11)\n\n\n@pytest.mark.parametrize(\"loss\", LOSS_INSTANCES, ids=loss_instance_name)\n@pytest.mark.parametrize(\"sample_weight\", [\"ones\", \"random\"])\ndef test_sample_weight_multiplies(loss, sample_weight, global_random_seed):\n    \"\"\"Test sample weights in loss, gradients and hessians.\n\n    Make sure that passing sample weights to loss, gradient and hessian\n    computation methods is equivalent to multiplying by the weights.\n    \"\"\"\n    n_samples = 100\n    y_true, raw_prediction = random_y_true_raw_prediction(\n        loss=loss,\n        n_samples=n_samples,\n        y_bound=(-100, 100),\n        raw_bound=(-5, 5),\n        seed=global_random_seed,\n    )\n\n    if sample_weight == \"ones\":\n        sample_weight = np.ones(shape=n_samples, dtype=np.float64)\n    else:\n        rng = np.random.RandomState(global_random_seed)\n        sample_weight = rng.normal(size=n_samples).astype(np.float64)\n\n    assert_allclose(\n        loss.loss(\n            y_true=y_true,\n            raw_prediction=raw_prediction,\n            sample_weight=sample_weight,\n        ),\n        sample_weight\n        * loss.loss(\n            y_true=y_true,\n            raw_prediction=raw_prediction,\n            sample_weight=None,\n        ),\n    )\n\n    losses, gradient = loss.loss_gradient(\n        y_true=y_true,\n        raw_prediction=raw_prediction,\n        sample_weight=None,\n    )\n    losses_sw, gradient_sw = loss.loss_gradient(\n        y_true=y_true,\n        raw_prediction=raw_prediction,\n        sample_weight=sample_weight,\n    )\n    assert_allclose(losses * sample_weight, losses_sw)\n    if not loss.is_multiclass:\n        assert_allclose(gradient * sample_weight, gradient_sw)\n    else:\n        assert_allclose(gradient * sample_weight[:, None], gradient_sw)\n\n    gradient, hessian = loss.gradient_hessian(\n        y_true=y_true,\n        raw_prediction=raw_prediction,\n        sample_weight=None,\n    )\n    gradient_sw, hessian_sw = loss.gradient_hessian(\n        y_true=y_true,\n        raw_prediction=raw_prediction,\n        sample_weight=sample_weight,\n    )\n    if not loss.is_multiclass:\n        assert_allclose(gradient * sample_weight, gradient_sw)\n        assert_allclose(hessian * sample_weight, hessian_sw)\n    else:\n        assert_allclose(gradient * sample_weight[:, None], gradient_sw)\n        assert_allclose(hessian * sample_weight[:, None], hessian_sw)\n\n\n@pytest.mark.parametrize(\"loss\", LOSS_INSTANCES, ids=loss_instance_name)\ndef test_graceful_squeezing(loss):\n    \"\"\"Test that reshaped raw_prediction gives same results.\"\"\"\n    y_true, raw_prediction = random_y_true_raw_prediction(\n        loss=loss,\n        n_samples=20,\n        y_bound=(-100, 100),\n        raw_bound=(-10, 10),\n        seed=42,\n    )\n\n    if raw_prediction.ndim == 1:\n        raw_prediction_2d = raw_prediction[:, None]\n        assert_allclose(\n            loss.loss(y_true=y_true, raw_prediction=raw_prediction_2d),\n            loss.loss(y_true=y_true, raw_prediction=raw_prediction),\n        )\n        assert_allclose(\n            loss.loss_gradient(y_true=y_true, raw_prediction=raw_prediction_2d),\n            loss.loss_gradient(y_true=y_true, raw_prediction=raw_prediction),\n        )\n        assert_allclose(\n            loss.gradient(y_true=y_true, raw_prediction=raw_prediction_2d),\n            loss.gradient(y_true=y_true, raw_prediction=raw_prediction),\n        )\n        assert_allclose(\n            loss.gradient_hessian(y_true=y_true, raw_prediction=raw_prediction_2d),\n            loss.gradient_hessian(y_true=y_true, raw_prediction=raw_prediction),\n        )\n\n\n@pytest.mark.parametrize(\"loss\", LOSS_INSTANCES, ids=loss_instance_name)\n@pytest.mark.parametrize(\"sample_weight\", [None, \"range\"])\ndef test_loss_of_perfect_prediction(loss, sample_weight):\n    \"\"\"Test value of perfect predictions.\n\n    Loss of y_pred = y_true plus constant_to_optimal_zero should sums up to\n    zero.\n    \"\"\"\n    if not loss.is_multiclass:\n        # Use small values such that exp(value) is not nan.\n        raw_prediction = np.array([-10, -0.1, 0, 0.1, 3, 10])\n        # If link is identity, we must respect the interval of y_pred:\n        if isinstance(loss.link, IdentityLink):\n            eps = 1e-10\n            low = loss.interval_y_pred.low\n            if not loss.interval_y_pred.low_inclusive:\n                low = low + eps\n            high = loss.interval_y_pred.high\n            if not loss.interval_y_pred.high_inclusive:\n                high = high - eps\n            raw_prediction = np.clip(raw_prediction, low, high)\n        y_true = loss.link.inverse(raw_prediction)\n    else:\n        # HalfMultinomialLoss\n        y_true = np.arange(loss.n_classes).astype(float)\n        # raw_prediction with entries -exp(10), but +exp(10) on the diagonal\n        # this is close enough to np.inf which would produce nan\n        raw_prediction = np.full(\n            shape=(loss.n_classes, loss.n_classes),\n            fill_value=-np.exp(10),\n            dtype=float,\n        )\n        raw_prediction.flat[:: loss.n_classes + 1] = np.exp(10)\n\n    if sample_weight == \"range\":\n        sample_weight = np.linspace(1, y_true.shape[0], num=y_true.shape[0])\n\n    loss_value = loss.loss(\n        y_true=y_true,\n        raw_prediction=raw_prediction,\n        sample_weight=sample_weight,\n    )\n    constant_term = loss.constant_to_optimal_zero(\n        y_true=y_true, sample_weight=sample_weight\n    )\n    # Comparing loss_value + constant_term to zero would result in large\n    # round-off errors.\n    assert_allclose(loss_value, -constant_term, atol=1e-14, rtol=1e-15)\n\n\n@pytest.mark.parametrize(\"loss\", LOSS_INSTANCES, ids=loss_instance_name)\n@pytest.mark.parametrize(\"sample_weight\", [None, \"range\"])\ndef test_gradients_hessians_numerically(loss, sample_weight, global_random_seed):\n    \"\"\"Test gradients and hessians with numerical derivatives.\n\n    Gradient should equal the numerical derivatives of the loss function.\n    Hessians should equal the numerical derivatives of gradients.\n    \"\"\"\n    n_samples = 20\n    y_true, raw_prediction = random_y_true_raw_prediction(\n        loss=loss,\n        n_samples=n_samples,\n        y_bound=(-100, 100),\n        raw_bound=(-5, 5),\n        seed=global_random_seed,\n    )\n\n    if sample_weight == \"range\":\n        sample_weight = np.linspace(1, y_true.shape[0], num=y_true.shape[0])\n\n    g, h = loss.gradient_hessian(\n        y_true=y_true,\n        raw_prediction=raw_prediction,\n        sample_weight=sample_weight,\n    )\n\n    assert g.shape == raw_prediction.shape\n    assert h.shape == raw_prediction.shape\n\n    if not loss.is_multiclass:\n\n        def loss_func(x):\n            return loss.loss(\n                y_true=y_true,\n                raw_prediction=x,\n                sample_weight=sample_weight,\n            )\n\n        g_numeric = numerical_derivative(loss_func, raw_prediction, eps=1e-6)\n        assert_allclose(g, g_numeric, rtol=5e-6, atol=1e-10)\n\n        def grad_func(x):\n            return loss.gradient(\n                y_true=y_true,\n                raw_prediction=x,\n                sample_weight=sample_weight,\n            )\n\n        h_numeric = numerical_derivative(grad_func, raw_prediction, eps=1e-6)\n        if loss.approx_hessian:\n            # TODO: What could we test if loss.approx_hessian?\n            pass\n        else:\n            assert_allclose(h, h_numeric, rtol=5e-6, atol=1e-10)\n    else:\n        # For multiclass loss, we should only change the predictions of the\n        # class for which the derivative is taken for, e.g. offset[:, k] = eps\n        # for class k.\n        # As a softmax is computed, offsetting the whole array by a constant\n        # would have no effect on the probabilities, and thus on the loss.\n        for k in range(loss.n_classes):\n\n            def loss_func(x):\n                raw = raw_prediction.copy()\n                raw[:, k] = x\n                return loss.loss(\n                    y_true=y_true,\n                    raw_prediction=raw,\n                    sample_weight=sample_weight,\n                )\n\n            g_numeric = numerical_derivative(loss_func, raw_prediction[:, k], eps=1e-5)\n            assert_allclose(g[:, k], g_numeric, rtol=5e-6, atol=1e-10)\n\n            def grad_func(x):\n                raw = raw_prediction.copy()\n                raw[:, k] = x\n                return loss.gradient(\n                    y_true=y_true,\n                    raw_prediction=raw,\n                    sample_weight=sample_weight,\n                )[:, k]\n\n            h_numeric = numerical_derivative(grad_func, raw_prediction[:, k], eps=1e-6)\n            if loss.approx_hessian:\n                # TODO: What could we test if loss.approx_hessian?\n                pass\n            else:\n                assert_allclose(h[:, k], h_numeric, rtol=5e-6, atol=1e-10)\n\n\n@pytest.mark.parametrize(\n    \"loss, x0, y_true\",\n    [\n        (\"squared_error\", -2.0, 42),\n        (\"squared_error\", 117.0, 1.05),\n        (\"squared_error\", 0.0, 0.0),\n        # The argmin of binomial_loss for y_true=0 and y_true=1 is resp.\n        # -inf and +inf due to logit, cf. \"complete separation\". Therefore, we\n        # use 0 < y_true < 1.\n        (\"binomial_loss\", 0.3, 0.1),\n        (\"binomial_loss\", -12, 0.2),\n        (\"binomial_loss\", 30, 0.9),\n        (\"poisson_loss\", 12.0, 1.0),\n        (\"poisson_loss\", 0.0, 2.0),\n        (\"poisson_loss\", -22.0, 10.0),\n    ],\n)\n@skip_if_32bit\ndef test_derivatives(loss, x0, y_true):\n    \"\"\"Test that gradients are zero at the minimum of the loss.\n\n    We check this on a single value/sample using Halley's method with the\n    first and second order derivatives computed by the Loss instance.\n    Note that methods of Loss instances operate on arrays while the newton\n    root finder expects a scalar or a one-element array for this purpose.\n    \"\"\"\n    loss = _LOSSES[loss](sample_weight=None)\n    y_true = np.array([y_true], dtype=np.float64)\n    x0 = np.array([x0], dtype=np.float64)\n\n    def func(x: np.ndarray) -> np.ndarray:\n        \"\"\"Compute loss plus constant term.\n\n        The constant term is such that the minimum function value is zero,\n        which is required by the Newton method.\n        \"\"\"\n        return loss.loss(\n            y_true=y_true, raw_prediction=x\n        ) + loss.constant_to_optimal_zero(y_true=y_true)\n\n    def fprime(x: np.ndarray) -> np.ndarray:\n        return loss.gradient(y_true=y_true, raw_prediction=x)\n\n    def fprime2(x: np.ndarray) -> np.ndarray:\n        return loss.gradient_hessian(y_true=y_true, raw_prediction=x)[1]\n\n    optimum = newton(\n        func,\n        x0=x0,\n        fprime=fprime,\n        fprime2=fprime2,\n        maxiter=100,\n        tol=5e-8,\n    )\n\n    # Need to ravel arrays because assert_allclose requires matching\n    # dimensions.\n    y_true = y_true.ravel()\n    optimum = optimum.ravel()\n    assert_allclose(loss.link.inverse(optimum), y_true)\n    assert_allclose(func(optimum), 0, atol=1e-14)\n    assert_allclose(loss.gradient(y_true=y_true, raw_prediction=optimum), 0, atol=5e-7)\n\n\n@pytest.mark.parametrize(\"loss\", LOSS_INSTANCES, ids=loss_instance_name)\n@pytest.mark.parametrize(\"sample_weight\", [None, \"range\"])\ndef test_loss_intercept_only(loss, sample_weight):\n    \"\"\"Test that fit_intercept_only returns the argmin of the loss.\n\n    Also test that the gradient is zero at the minimum.\n    \"\"\"\n    n_samples = 50\n    if not loss.is_multiclass:\n        y_true = loss.link.inverse(np.linspace(-4, 4, num=n_samples))\n    else:\n        y_true = np.arange(n_samples).astype(np.float64) % loss.n_classes\n        y_true[::5] = 0  # exceedance of class 0\n\n    if sample_weight == \"range\":\n        sample_weight = np.linspace(0.1, 2, num=n_samples)\n\n    a = loss.fit_intercept_only(y_true=y_true, sample_weight=sample_weight)\n\n    # find minimum by optimization\n    def fun(x):\n        if not loss.is_multiclass:\n            raw_prediction = np.full(shape=(n_samples), fill_value=x)\n        else:\n            raw_prediction = np.ascontiguousarray(\n                np.broadcast_to(x, shape=(n_samples, loss.n_classes))\n            )\n        return loss(\n            y_true=y_true,\n            raw_prediction=raw_prediction,\n            sample_weight=sample_weight,\n        )\n\n    if not loss.is_multiclass:\n        opt = minimize_scalar(fun, tol=1e-7, options={\"maxiter\": 100})\n        grad = loss.gradient(\n            y_true=y_true,\n            raw_prediction=np.full_like(y_true, a),\n            sample_weight=sample_weight,\n        )\n        assert a.shape == tuple()  # scalar\n        assert a.dtype == y_true.dtype\n        assert_all_finite(a)\n        a == approx(opt.x, rel=1e-7)\n        grad.sum() == approx(0, abs=1e-12)\n    else:\n        # The constraint corresponds to sum(raw_prediction) = 0. Without it, we would\n        # need to apply loss.symmetrize_raw_prediction to opt.x before comparing.\n        opt = minimize(\n            fun,\n            np.zeros((loss.n_classes)),\n            tol=1e-13,\n            options={\"maxiter\": 100},\n            method=\"SLSQP\",\n            constraints=LinearConstraint(np.ones((1, loss.n_classes)), 0, 0),\n        )\n        grad = loss.gradient(\n            y_true=y_true,\n            raw_prediction=np.tile(a, (n_samples, 1)),\n            sample_weight=sample_weight,\n        )\n        assert a.dtype == y_true.dtype\n        assert_all_finite(a)\n        assert_allclose(a, opt.x, rtol=5e-6, atol=1e-12)\n        assert_allclose(grad.sum(axis=0), 0, atol=1e-12)\n\n\n@pytest.mark.parametrize(\n    \"loss, func, random_dist\",\n    [\n        (HalfSquaredError(), np.mean, \"normal\"),\n        (AbsoluteError(), np.median, \"normal\"),\n        (PinballLoss(quantile=0.25), lambda x: np.percentile(x, q=25), \"normal\"),\n        (HalfPoissonLoss(), np.mean, \"poisson\"),\n        (HalfGammaLoss(), np.mean, \"exponential\"),\n        (HalfTweedieLoss(), np.mean, \"exponential\"),\n        (HalfBinomialLoss(), np.mean, \"binomial\"),\n    ],\n)\ndef test_specific_fit_intercept_only(loss, func, random_dist, global_random_seed):\n    \"\"\"Test that fit_intercept_only returns the correct functional.\n\n    We test the functional for specific, meaningful distributions, e.g.\n    squared error estimates the expectation of a probability distribution.\n    \"\"\"\n    rng = np.random.RandomState(global_random_seed)\n    if random_dist == \"binomial\":\n        y_train = rng.binomial(1, 0.5, size=100)\n    else:\n        y_train = getattr(rng, random_dist)(size=100)\n    baseline_prediction = loss.fit_intercept_only(y_true=y_train)\n    # Make sure baseline prediction is the expected functional=func, e.g. mean\n    # or median.\n    assert_all_finite(baseline_prediction)\n    assert baseline_prediction == approx(loss.link.link(func(y_train)))\n    assert loss.link.inverse(baseline_prediction) == approx(func(y_train))\n    if isinstance(loss, IdentityLink):\n        assert_allclose(loss.link.inverse(baseline_prediction), baseline_prediction)\n\n    # Test baseline at boundary\n    if loss.interval_y_true.low_inclusive:\n        y_train.fill(loss.interval_y_true.low)\n        baseline_prediction = loss.fit_intercept_only(y_true=y_train)\n        assert_all_finite(baseline_prediction)\n    if loss.interval_y_true.high_inclusive:\n        y_train.fill(loss.interval_y_true.high)\n        baseline_prediction = loss.fit_intercept_only(y_true=y_train)\n        assert_all_finite(baseline_prediction)\n\n\ndef test_multinomial_loss_fit_intercept_only():\n    \"\"\"Test that fit_intercept_only returns the mean functional for CCE.\"\"\"\n    rng = np.random.RandomState(0)\n    n_classes = 4\n    loss = HalfMultinomialLoss(n_classes=n_classes)\n    # Same logic as test_specific_fit_intercept_only. Here inverse link\n    # function = softmax and link function = log - symmetry term.\n    y_train = rng.randint(0, n_classes + 1, size=100).astype(np.float64)\n    baseline_prediction = loss.fit_intercept_only(y_true=y_train)\n    assert baseline_prediction.shape == (n_classes,)\n    p = np.zeros(n_classes, dtype=y_train.dtype)\n    for k in range(n_classes):\n        p[k] = (y_train == k).mean()\n    assert_allclose(baseline_prediction, np.log(p) - np.mean(np.log(p)))\n    assert_allclose(baseline_prediction[None, :], loss.link.link(p[None, :]))\n\n    for y_train in (np.zeros(shape=10), np.ones(shape=10)):\n        y_train = y_train.astype(np.float64)\n        baseline_prediction = loss.fit_intercept_only(y_true=y_train)\n        assert baseline_prediction.dtype == y_train.dtype\n        assert_all_finite(baseline_prediction)\n\n\ndef test_multinomial_cy_gradient(global_random_seed):\n    \"\"\"Test that Multinomial cy_gradient gives the same result as gradient.\n\n    CyHalfMultinomialLoss does not inherit from CyLossFunction and has a different API.\n    As a consequence, the functions like `loss` and `gradient` do not rely on `cy_loss`\n    and `cy_gradient`.\n    \"\"\"\n    n_samples = 100\n    n_classes = 5\n    loss = HalfMultinomialLoss(n_classes=n_classes)\n    y_true, raw_prediction = random_y_true_raw_prediction(\n        loss=loss,\n        n_samples=n_samples,\n        seed=global_random_seed,\n    )\n    sample_weight = np.linspace(0.1, 2, num=n_samples)\n\n    grad1 = loss.closs._test_cy_gradient(\n        y_true=y_true,\n        raw_prediction=raw_prediction,  # needs to be C-contiguous\n        sample_weight=sample_weight,\n    )\n    grad2 = loss.gradient(\n        y_true=y_true,\n        raw_prediction=raw_prediction,\n        sample_weight=sample_weight,\n    )\n    assert_allclose(grad1, grad2)\n\n\ndef test_binomial_and_multinomial_loss(global_random_seed):\n    \"\"\"Test that multinomial loss with n_classes = 2 is the same as binomial loss.\"\"\"\n    rng = np.random.RandomState(global_random_seed)\n    n_samples = 20\n    binom = HalfBinomialLoss()\n    multinom = HalfMultinomialLoss(n_classes=2)\n    y_train = rng.randint(0, 2, size=n_samples).astype(np.float64)\n    raw_prediction = rng.normal(size=n_samples)\n    raw_multinom = np.empty((n_samples, 2))\n    raw_multinom[:, 0] = -0.5 * raw_prediction\n    raw_multinom[:, 1] = 0.5 * raw_prediction\n    assert_allclose(\n        binom.loss(y_true=y_train, raw_prediction=raw_prediction),\n        multinom.loss(y_true=y_train, raw_prediction=raw_multinom),\n    )\n\n\n@pytest.mark.parametrize(\"y_true\", (np.array([0.0, 0, 0]), np.array([1.0, 1, 1])))\n@pytest.mark.parametrize(\"y_pred\", (np.array([-5.0, -5, -5]), np.array([3.0, 3, 3])))\ndef test_binomial_vs_alternative_formulation(y_true, y_pred, global_dtype):\n    \"\"\"Test that both formulations of the binomial deviance agree.\n\n    Often, the binomial deviance or log loss is written in terms of a variable\n    z in {-1, +1}, but we use y in {0, 1}, hence z = 2 * y - 1.\n    ESL II Eq. (10.18):\n\n        -loglike(z, f) = log(1 + exp(-2 * z * f))\n\n    Note:\n        - ESL 2*f = raw_prediction, hence the factor 2 of ESL disappears.\n        - Deviance = -2*loglike + .., but HalfBinomialLoss is half of the\n          deviance, hence the factor of 2 cancels in the comparison.\n    \"\"\"\n\n    def alt_loss(y, raw_pred):\n        z = 2 * y - 1\n        return np.mean(np.log(1 + np.exp(-z * raw_pred)))\n\n    def alt_gradient(y, raw_pred):\n        # alternative gradient formula according to ESL\n        z = 2 * y - 1\n        return -z / (1 + np.exp(z * raw_pred))\n\n    bin_loss = HalfBinomialLoss()\n\n    y_true = y_true.astype(global_dtype)\n    y_pred = y_pred.astype(global_dtype)\n    datum = (y_true, y_pred)\n\n    assert bin_loss(*datum) == approx(alt_loss(*datum))\n    assert_allclose(bin_loss.gradient(*datum), alt_gradient(*datum))\n\n\n@pytest.mark.parametrize(\"loss\", LOSS_INSTANCES, ids=loss_instance_name)\ndef test_predict_proba(loss, global_random_seed):\n    \"\"\"Test that predict_proba and gradient_proba work as expected.\"\"\"\n    n_samples = 20\n    y_true, raw_prediction = random_y_true_raw_prediction(\n        loss=loss,\n        n_samples=n_samples,\n        y_bound=(-100, 100),\n        raw_bound=(-5, 5),\n        seed=global_random_seed,\n    )\n\n    if hasattr(loss, \"predict_proba\"):\n        proba = loss.predict_proba(raw_prediction)\n        assert proba.shape == (n_samples, loss.n_classes)\n        assert np.sum(proba, axis=1) == approx(1, rel=1e-11)\n\n    if hasattr(loss, \"gradient_proba\"):\n        for grad, proba in (\n            (None, None),\n            (None, np.empty_like(raw_prediction)),\n            (np.empty_like(raw_prediction), None),\n            (np.empty_like(raw_prediction), np.empty_like(raw_prediction)),\n        ):\n            grad, proba = loss.gradient_proba(\n                y_true=y_true,\n                raw_prediction=raw_prediction,\n                sample_weight=None,\n                gradient_out=grad,\n                proba_out=proba,\n            )\n            assert proba.shape == (n_samples, loss.n_classes)\n            assert np.sum(proba, axis=1) == approx(1, rel=1e-11)\n            assert_allclose(\n                grad,\n                loss.gradient(\n                    y_true=y_true,\n                    raw_prediction=raw_prediction,\n                    sample_weight=None,\n                    gradient_out=None,\n                ),\n            )\n\n\n@pytest.mark.parametrize(\"loss\", ALL_LOSSES)\n@pytest.mark.parametrize(\"sample_weight\", [None, \"range\"])\n@pytest.mark.parametrize(\"dtype\", (np.float32, np.float64))\n@pytest.mark.parametrize(\"order\", (\"C\", \"F\"))\ndef test_init_gradient_and_hessians(loss, sample_weight, dtype, order):\n    \"\"\"Test that init_gradient_and_hessian works as expected.\n\n    passing sample_weight to a loss correctly influences the constant_hessian\n    attribute, and consequently the shape of the hessian array.\n    \"\"\"\n    n_samples = 5\n    if sample_weight == \"range\":\n        sample_weight = np.ones(n_samples)\n    loss = loss(sample_weight=sample_weight)\n    gradient, hessian = loss.init_gradient_and_hessian(\n        n_samples=n_samples,\n        dtype=dtype,\n        order=order,\n    )\n    if loss.constant_hessian:\n        assert gradient.shape == (n_samples,)\n        assert hessian.shape == (1,)\n    elif loss.is_multiclass:\n        assert gradient.shape == (n_samples, loss.n_classes)\n        assert hessian.shape == (n_samples, loss.n_classes)\n    else:\n        assert hessian.shape == (n_samples,)\n        assert hessian.shape == (n_samples,)\n\n    assert gradient.dtype == dtype\n    assert hessian.dtype == dtype\n\n    if order == \"C\":\n        assert gradient.flags.c_contiguous\n        assert hessian.flags.c_contiguous\n    else:\n        assert gradient.flags.f_contiguous\n        assert hessian.flags.f_contiguous\n\n\n@pytest.mark.parametrize(\"loss\", ALL_LOSSES)\n@pytest.mark.parametrize(\n    \"params, err_msg\",\n    [\n        (\n            {\"dtype\": np.int64},\n            f\"Valid options for 'dtype' are .* Got dtype={np.int64} instead.\",\n        ),\n    ],\n)\ndef test_init_gradient_and_hessian_raises(loss, params, err_msg):\n    \"\"\"Test that init_gradient_and_hessian raises errors for invalid input.\"\"\"\n    loss = loss()\n    with pytest.raises((ValueError, TypeError), match=err_msg):\n        gradient, hessian = loss.init_gradient_and_hessian(n_samples=5, **params)\n\n\n@pytest.mark.parametrize(\n    \"loss, params, err_type, err_msg\",\n    [\n        (\n            PinballLoss,\n            {\"quantile\": None},\n            TypeError,\n            \"quantile must be an instance of float, not NoneType.\",\n        ),\n        (\n            PinballLoss,\n            {\"quantile\": 0},\n            ValueError,\n            \"quantile == 0, must be > 0.\",\n        ),\n        (PinballLoss, {\"quantile\": 1.1}, ValueError, \"quantile == 1.1, must be < 1.\"),\n        (\n            HuberLoss,\n            {\"quantile\": None},\n            TypeError,\n            \"quantile must be an instance of float, not NoneType.\",\n        ),\n        (\n            HuberLoss,\n            {\"quantile\": 0},\n            ValueError,\n            \"quantile == 0, must be > 0.\",\n        ),\n        (HuberLoss, {\"quantile\": 1.1}, ValueError, \"quantile == 1.1, must be < 1.\"),\n    ],\n)\ndef test_loss_init_parameter_validation(loss, params, err_type, err_msg):\n    \"\"\"Test that loss raises errors for invalid input.\"\"\"\n    with pytest.raises(err_type, match=err_msg):\n        loss(**params)\n\n\n@pytest.mark.parametrize(\"loss\", LOSS_INSTANCES, ids=loss_instance_name)\ndef test_loss_pickle(loss):\n    \"\"\"Test that losses can be pickled.\"\"\"\n    n_samples = 20\n    y_true, raw_prediction = random_y_true_raw_prediction(\n        loss=loss,\n        n_samples=n_samples,\n        y_bound=(-100, 100),\n        raw_bound=(-5, 5),\n        seed=42,\n    )\n    pickled_loss = pickle.dumps(loss)\n    unpickled_loss = pickle.loads(pickled_loss)\n    assert loss(y_true=y_true, raw_prediction=raw_prediction) == approx(\n        unpickled_loss(y_true=y_true, raw_prediction=raw_prediction)\n    )\n\n\n@pytest.mark.parametrize(\"p\", [-1.5, 0, 1, 1.5, 2, 3])\ndef test_tweedie_log_identity_consistency(p):\n    \"\"\"Test for identical losses when only the link function is different.\"\"\"\n    half_tweedie_log = HalfTweedieLoss(power=p)\n    half_tweedie_identity = HalfTweedieLossIdentity(power=p)\n    n_samples = 10\n    y_true, raw_prediction = random_y_true_raw_prediction(\n        loss=half_tweedie_log, n_samples=n_samples, seed=42\n    )\n    y_pred = half_tweedie_log.link.inverse(raw_prediction)  # exp(raw_prediction)\n\n    # Let's compare the loss values, up to some constant term that is dropped\n    # in HalfTweedieLoss but not in HalfTweedieLossIdentity.\n    loss_log = half_tweedie_log.loss(\n        y_true=y_true, raw_prediction=raw_prediction\n    ) + half_tweedie_log.constant_to_optimal_zero(y_true)\n    loss_identity = half_tweedie_identity.loss(\n        y_true=y_true, raw_prediction=y_pred\n    ) + half_tweedie_identity.constant_to_optimal_zero(y_true)\n    # Note that HalfTweedieLoss ignores different constant terms than\n    # HalfTweedieLossIdentity. Constant terms means terms not depending on\n    # raw_prediction. By adding these terms, `constant_to_optimal_zero`, both losses\n    # give the same values.\n    assert_allclose(loss_log, loss_identity)\n\n    # For gradients and hessians, the constant terms do not matter. We have, however,\n    # to account for the chain rule, i.e. with x=raw_prediction\n    #     gradient_log(x) = d/dx loss_log(x)\n    #                     = d/dx loss_identity(exp(x))\n    #                     = exp(x) * gradient_identity(exp(x))\n    # Similarly,\n    #     hessian_log(x) = exp(x) * gradient_identity(exp(x))\n    #                    + exp(x)**2 * hessian_identity(x)\n    gradient_log, hessian_log = half_tweedie_log.gradient_hessian(\n        y_true=y_true, raw_prediction=raw_prediction\n    )\n    gradient_identity, hessian_identity = half_tweedie_identity.gradient_hessian(\n        y_true=y_true, raw_prediction=y_pred\n    )\n    assert_allclose(gradient_log, y_pred * gradient_identity)\n    assert_allclose(\n        hessian_log, y_pred * gradient_identity + y_pred**2 * hessian_identity\n    )\n",
    "sklearn/cluster/_hdbscan/tests/test_reachibility.py": "import numpy as np\nimport pytest\n\nfrom sklearn.cluster._hdbscan._reachability import mutual_reachability_graph\nfrom sklearn.utils._testing import (\n    _convert_container,\n    assert_allclose,\n)\n\n\ndef test_mutual_reachability_graph_error_sparse_format():\n    \"\"\"Check that we raise an error if the sparse format is not CSR.\"\"\"\n    rng = np.random.RandomState(0)\n    X = rng.randn(10, 10)\n    X = X.T @ X\n    np.fill_diagonal(X, 0.0)\n    X = _convert_container(X, \"sparse_csc\")\n\n    err_msg = \"Only sparse CSR matrices are supported\"\n    with pytest.raises(ValueError, match=err_msg):\n        mutual_reachability_graph(X)\n\n\n@pytest.mark.parametrize(\"array_type\", [\"array\", \"sparse_csr\"])\ndef test_mutual_reachability_graph_inplace(array_type):\n    \"\"\"Check that the operation is happening inplace.\"\"\"\n    rng = np.random.RandomState(0)\n    X = rng.randn(10, 10)\n    X = X.T @ X\n    np.fill_diagonal(X, 0.0)\n    X = _convert_container(X, array_type)\n\n    mr_graph = mutual_reachability_graph(X)\n\n    assert id(mr_graph) == id(X)\n\n\ndef test_mutual_reachability_graph_equivalence_dense_sparse():\n    \"\"\"Check that we get the same results for dense and sparse implementation.\"\"\"\n    rng = np.random.RandomState(0)\n    X = rng.randn(5, 5)\n    X_dense = X.T @ X\n    X_sparse = _convert_container(X_dense, \"sparse_csr\")\n\n    mr_graph_dense = mutual_reachability_graph(X_dense, min_samples=3)\n    mr_graph_sparse = mutual_reachability_graph(X_sparse, min_samples=3)\n\n    assert_allclose(mr_graph_dense, mr_graph_sparse.toarray())\n\n\n@pytest.mark.parametrize(\"array_type\", [\"array\", \"sparse_csr\"])\n@pytest.mark.parametrize(\"dtype\", [np.float32, np.float64])\ndef test_mutual_reachability_graph_preserves_dtype(array_type, dtype):\n    \"\"\"Check that the computation preserve dtype thanks to fused types.\"\"\"\n    rng = np.random.RandomState(0)\n    X = rng.randn(10, 10)\n    X = (X.T @ X).astype(dtype)\n    np.fill_diagonal(X, 0.0)\n    X = _convert_container(X, array_type)\n\n    assert X.dtype == dtype\n    mr_graph = mutual_reachability_graph(X)\n    assert mr_graph.dtype == dtype\n"
  },
  "requirements": null
}