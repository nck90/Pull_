{
  "repo_name": "AntonOsika/gpt-engineer",
  "repo_url": "https://github.com/AntonOsika/gpt-engineer",
  "description": "CLI platform to experiment with codegen. Precursor to: https://lovable.dev",
  "stars": 53430,
  "language": "Python",
  "created_at": "2023-04-29T12:52:15Z",
  "updated_at": "2025-03-19T07:05:32Z",
  "files": {
    "scripts/test_api.py": "\"\"\"This is just a demo to test api.py.\"\"\"\n\nfrom time import sleep\n\nimport requests\n\n\ndef post_data(url, extra_arguments):\n    \"\"\"\n    Make an HTTP POST request with extra_arguments as data.\n\n    Parameters\n    ----------\n     url : str\n        The URL to which the POST request should be sent.\n    extra_arguments : dict\n        A dictionary of data that needs to be sent in the POST request.\n\n    Returns\n    -------\n    response\n        The response from the server.\n    \"\"\"\n\n    response = requests.post(url, json=extra_arguments)\n    return response\n\n\nif __name__ == \"__main__\":\n    URL_BASE = \"http://127.0.0.1:8000\"\n\n    arguments = {\n        \"input\": \"We are writing snake in python. MVC components split \\\n        in separate files. Keyboard control.\",  # our prompt\n        \"additional_input\": {\"improve_option\": False},\n    }\n\n    # create a task\n    response = post_data(f\"{URL_BASE}/agent/tasks\", arguments)\n    print(response.json())\n    task_id = response.json()[\"task_id\"]\n\n    sleep(1)  # this is not needed\n\n    # execute the step for our task\n    response = post_data(f\"{URL_BASE}/agent/tasks/{task_id}/steps\", {})\n    print(response.json())\n",
    "tests/applications/cli/test_cli_agent.py": "import os\nimport tempfile\n\nimport pytest\n\nfrom langchain.schema import AIMessage\n\nfrom gpt_engineer.applications.cli.cli_agent import CliAgent\nfrom gpt_engineer.core.default.disk_execution_env import DiskExecutionEnv\nfrom gpt_engineer.core.default.disk_memory import DiskMemory\n\n# from gpt_engineer.core.default.git_version_manager import GitVersionManager\nfrom gpt_engineer.core.default.paths import ENTRYPOINT_FILE, memory_path\nfrom gpt_engineer.core.files_dict import FilesDict\nfrom gpt_engineer.core.prompt import Prompt\nfrom gpt_engineer.tools.custom_steps import clarified_gen, lite_gen\nfrom tests.mock_ai import MockAI\n\n\ndef test_init_standard_config(monkeypatch):\n    monkeypatch.setattr(\"builtins.input\", lambda _: \"y\")\n    temp_dir = tempfile.mkdtemp()\n    memory = DiskMemory(memory_path(temp_dir))\n    execution_env = DiskExecutionEnv()\n    mock_ai = MockAI(\n        [\n            AIMessage(\n                \"hello_world.py\\n```\\nwith open('output.txt', 'w') as file:\\n    file.write('Hello World!')\\n```\"\n            ),\n            AIMessage(\"```run.sh\\npython3 hello_world.py\\n```\"),\n        ],\n    )\n    cli_agent = CliAgent.with_default_config(memory, execution_env, ai=mock_ai)\n    outfile = \"output.txt\"\n    os.path.join(temp_dir, outfile)\n    code = cli_agent.init(\n        Prompt(\n            f\"Make a program that prints 'Hello World!' to a file called '{outfile}'\"\n        )\n    )\n\n    env = DiskExecutionEnv()\n    env.upload(code).run(f\"bash {ENTRYPOINT_FILE}\")\n    code = env.download()\n\n    assert outfile in code\n    assert code[outfile] == \"Hello World!\"\n\n\ndef test_init_lite_config(monkeypatch):\n    monkeypatch.setattr(\"builtins.input\", lambda _: \"y\")\n    temp_dir = tempfile.mkdtemp()\n    memory = DiskMemory(memory_path(temp_dir))\n    # version_manager = GitVersionManager(temp_dir)\n    execution_env = DiskExecutionEnv()\n    mock_ai = MockAI(\n        [\n            AIMessage(\n                \"hello_world.py\\n```\\nwith open('output.txt', 'w') as file:\\n    file.write('Hello World!')\\n```\"\n            ),\n            AIMessage(\"```run.sh\\npython3 hello_world.py\\n```\"),\n        ],\n    )\n    cli_agent = CliAgent.with_default_config(\n        memory, execution_env, ai=mock_ai, code_gen_fn=lite_gen\n    )\n    outfile = \"output.txt\"\n    os.path.join(temp_dir, outfile)\n    code = cli_agent.init(\n        Prompt(\n            f\"Make a program that prints 'Hello World!' to a file called '{outfile}'\"\n        )\n    )\n\n    env = DiskExecutionEnv()\n    env.upload(code).run(f\"bash {ENTRYPOINT_FILE}\")\n    code = env.download()\n\n    assert outfile in code\n    assert code[outfile].strip() == \"Hello World!\"\n\n\ndef test_init_clarified_gen_config(monkeypatch):\n    monkeypatch.setattr(\"builtins.input\", lambda _: \"y\")\n    temp_dir = tempfile.mkdtemp()\n    memory = DiskMemory(memory_path(temp_dir))\n    execution_env = DiskExecutionEnv()\n    mock_ai = MockAI(\n        [\n            AIMessage(\"nothing to clarify\"),\n            AIMessage(\n                \"hello_world.py\\n```\\nwith open('output.txt', 'w') as file:\\n    file.write('Hello World!')\\n```\"\n            ),\n            AIMessage(\"```run.sh\\npython3 hello_world.py\\n```\"),\n        ],\n    )\n    cli_agent = CliAgent.with_default_config(\n        memory, execution_env, ai=mock_ai, code_gen_fn=clarified_gen\n    )\n    outfile = \"output.txt\"\n    code = cli_agent.init(\n        Prompt(\n            f\"Make a program that prints 'Hello World!' to a file called '{outfile} either using python or javascript'\"\n        )\n    )\n\n    env = DiskExecutionEnv()\n    env.upload(code).run(f\"bash {ENTRYPOINT_FILE}\")\n    code = env.download()\n\n    assert outfile in code\n    assert code[outfile].strip() == \"Hello World!\"\n\n\ndef test_improve_standard_config(monkeypatch):\n    monkeypatch.setattr(\"builtins.input\", lambda _: \"y\")\n    temp_dir = tempfile.mkdtemp()\n    code = FilesDict(\n        {\n            \"main.py\": \"def write_hello_world_to_file(filename):\\n    \\\"\\\"\\\"\\n    Writes 'Hello World!' to the specified file.\\n    \\n    :param filename: The name of the file to write to.\\n    \\\"\\\"\\\"\\n    with open(filename, 'w') as file:\\n        file.write('Hello World!')\\n\\nif __name__ == \\\"__main__\\\":\\n    output_filename = 'output.txt'\\n    write_hello_world_to_file(output_filename)\",\n            \"requirements.txt\": \"# No dependencies required\",\n            \"run.sh\": \"python3 main.py\\n\",\n        }\n    )\n    memory = DiskMemory(memory_path(temp_dir))\n    # version_manager = GitVersionManager(temp_dir)\n    execution_env = DiskExecutionEnv()\n    mock_ai = MockAI(\n        [\n            AIMessage(\n                \"```diff\\n--- main.py\\n+++ main.py\\n@@ -7,3 +7,3 @@\\n     with open(filename, 'w') as file:\\n-        file.write('Hello World!')\\n+        file.write('!dlroW olleH')\\n```\"\n            )\n        ]\n    )\n    cli_agent = CliAgent.with_default_config(memory, execution_env, ai=mock_ai)\n\n    code = cli_agent.improve(\n        code,\n        Prompt(\n            \"Change the program so that it prints '!dlroW olleH' instead of 'Hello World!'\"\n        ),\n    )\n\n    env = DiskExecutionEnv()\n    env.upload(code).run(f\"bash {ENTRYPOINT_FILE}\")\n    code = env.download()\n\n    outfile = \"output.txt\"\n    assert outfile in code\n    assert code[outfile] == \"!dlroW olleH\"\n\n\nif __name__ == \"__main__\":\n    pytest.main()\n",
    "tests/applications/cli/test_collect.py": "\"\"\"\nTests the collect_learnings function in the cli/collect module.\n\"\"\"\n\nimport pytest\n\n# def test_collect_learnings(monkeypatch):\n#     monkeypatch.setattr(rudder_analytics, \"track\", MagicMock())\n#\n#     model = \"test_model\"\n#     temperature = 0.5\n#     steps = [simple_gen]\n#     dbs = FileRepositories(\n#         OnDiskRepository(\"/tmp\"),\n#         OnDiskRepository(\"/tmp\"),\n#         OnDiskRepository(\"/tmp\"),\n#         OnDiskRepository(\"/tmp\"),\n#         OnDiskRepository(\"/tmp\"),\n#         OnDiskRepository(\"/tmp\"),\n#         OnDiskRepository(\"/tmp\"),\n#     )\n#     dbs.input = {\n#         \"prompt\": \"test prompt\\n with newlines\",\n#         \"feedback\": \"test feedback\",\n#     }\n#     code = \"this is output\\n\\nit contains code\"\n#     dbs.logs = {steps[0].__name__: json.dumps([{\"role\": \"system\", \"content\": code}])}\n#     dbs.memory = {\"all_output.txt\": \"test workspace\\n\" + code}\n#\n#     collect_learnings(model, temperature, steps, dbs)\n#\n#     learnings = extract_learning(\n#         model, temperature, steps, dbs, steps_file_hash=steps_file_hash()\n#     )\n#     assert rudder_analytics.track.call_count == 1\n#     assert rudder_analytics.track.call_args[1][\"event\"] == \"learning\"\n#     a = {\n#         k: v\n#         for k, v in rudder_analytics.track.call_args[1][\"properties\"].items()\n#         if k != \"timestamp\"\n#     }\n#     b = {k: v for k, v in learnings.to_dict().items() if k != \"timestamp\"}\n#     assert a == b\n#\n#     assert json.dumps(code) in learnings.logs\n#     assert code in learnings.workspace\n\n\nif __name__ == \"__main__\":\n    pytest.main([\"-v\"])\n",
    "tests/applications/cli/test_collection_consent.py": "\"\"\"\nTests for the revised data collection consent mechanism in the cli/learning module.\n\"\"\"\n\nfrom pathlib import Path\nfrom unittest.mock import patch\n\nimport pytest\n\nfrom gpt_engineer.applications.cli.learning import (\n    ask_collection_consent,\n    check_collection_consent,\n)\n\n\n# Use a fixture to clean up created files after each test\n@pytest.fixture\ndef cleanup():\n    yield\n    if Path(\".gpte_consent\").exists():\n        Path(\".gpte_consent\").unlink()\n\n\n\"\"\"\nTest the following 4 scenarios for check_collection_consent():\n    * The .gpte_consent file exists and its content is \"true\".\n    * The .gpte_consent file exists but its content is not \"true\".\n    * The .gpte_consent file does not exist and the user gives consent when asked.\n    * The .gpte_consent file does not exist and the user does not give consent when asked.\n\"\"\"\n\n\ndef test_check_consent_file_exists_and_true(cleanup):\n    Path(\".gpte_consent\").write_text(\"true\")\n    assert check_collection_consent() is True\n\n\ndef test_check_consent_file_exists_and_false(cleanup):\n    Path(\".gpte_consent\").write_text(\"false\")\n    with patch(\"builtins.input\", side_effect=[\"n\"]):\n        assert check_collection_consent() is False\n\n\ndef test_check_consent_file_not_exists_and_user_says_yes(cleanup):\n    with patch(\"builtins.input\", side_effect=[\"y\"]):\n        assert check_collection_consent() is True\n    assert Path(\".gpte_consent\").exists()\n    assert Path(\".gpte_consent\").read_text() == \"true\"\n\n\ndef test_check_consent_file_not_exists_and_user_says_no(cleanup):\n    with patch(\"builtins.input\", side_effect=[\"n\"]):\n        assert check_collection_consent() is False\n    assert not Path(\".gpte_consent\").exists()\n\n\n\"\"\"\nTest the following 4 scenarios for ask_collection_consent():\n    1. The user immediately gives consent with \"y\":\n        * The .gpte_consent file is created with content \"true\".\n        * The function returns True.\n    2. The user immediately denies consent with \"n\":\n        * The .gpte_consent file is not created.\n        * The function returns False.\n    3. The user first provides an invalid response, then gives consent with \"y\":\n        * The user is re-prompted after the invalid input.\n        * The .gpte_consent file is created with content \"true\".\n        * The function returns True.\n    4. The user first provides an invalid response, then denies consent with \"n\":\n        * The user is re-prompted after the invalid input.\n        * The .gpte_consent file is not created.\n        * The function returns False.\n\"\"\"\n\n\ndef test_ask_collection_consent_yes(cleanup):\n    with patch(\"builtins.input\", side_effect=[\"y\"]):\n        result = ask_collection_consent()\n    assert Path(\".gpte_consent\").exists()\n    assert Path(\".gpte_consent\").read_text() == \"true\"\n    assert result is True\n\n\ndef test_ask_collection_consent_no(cleanup):\n    with patch(\"builtins.input\", side_effect=[\"n\"]):\n        result = ask_collection_consent()\n    assert not Path(\".gpte_consent\").exists()\n    assert result is False\n\n\ndef test_ask_collection_consent_invalid_then_yes(cleanup):\n    with patch(\"builtins.input\", side_effect=[\"invalid\", \"y\"]):\n        result = ask_collection_consent()\n    assert Path(\".gpte_consent\").exists()\n    assert Path(\".gpte_consent\").read_text() == \"true\"\n    assert result is True\n\n\ndef test_ask_collection_consent_invalid_then_no(cleanup):\n    with patch(\"builtins.input\", side_effect=[\"invalid\", \"n\"]):\n        result = ask_collection_consent()\n    assert not Path(\".gpte_consent\").exists()\n    assert result is False\n",
    "tests/applications/cli/test_learning.py": "from unittest import mock\n\nfrom gpt_engineer.applications.cli import learning\nfrom gpt_engineer.applications.cli.learning import Learning\nfrom gpt_engineer.core.default.disk_memory import DiskMemory\nfrom gpt_engineer.core.prompt import Prompt\n\n\ndef test_human_review_input_no_concent_returns_none():\n    with mock.patch.object(learning, \"check_collection_consent\", return_value=False):\n        result = learning.human_review_input()\n\n    assert result is None\n\n\ndef test_human_review_input_consent_code_ran_no_comments():\n    with (\n        mock.patch.object(learning, \"check_collection_consent\", return_value=True),\n        mock.patch(\"builtins.input\", return_value=\"y\"),\n    ):\n        result = learning.human_review_input()\n\n    assert result.raw == \"y, y, \"\n    assert result.ran is True\n    assert result.works is None\n    assert result.comments == \"\"\n\n\ndef test_human_review_input_consent_code_ran_not_perfect_but_useful_no_comments():\n    with (\n        mock.patch.object(learning, \"check_collection_consent\", return_value=True),\n        mock.patch(\"builtins.input\", side_effect=[\"y\", \"n\", \"y\", \"\"]),\n    ):\n        result = learning.human_review_input()\n\n    assert result.raw == \"y, n, y\"\n    assert result.ran is True\n    assert result.works is True\n    assert result.comments == \"\"\n\n\ndef test_check_collection_consent_yes():\n    gpte_consent_mock = mock.Mock()\n    gpte_consent_mock.exists.return_value = True\n    gpte_consent_mock.read_text.return_value = \"true\"\n\n    with mock.patch.object(learning, \"Path\", return_value=gpte_consent_mock):\n        result = learning.check_collection_consent()\n\n    assert result is True\n\n\ndef test_check_collection_consent_no_ask_collection_consent():\n    with mock.patch.object(learning, \"Path\") as gpte_consent_mock:\n        gpte_consent_mock.exists.return_value = True\n        gpte_consent_mock.read_text.return_value = \"false\"\n\n        with mock.patch.object(learning, \"ask_collection_consent\", return_value=True):\n            result = learning.check_collection_consent()\n\n    assert result is True\n\n\ndef test_ask_collection_consent_yes():\n    with mock.patch(\"builtins.input\", return_value=\"y\"):\n        result = learning.ask_collection_consent()\n\n    assert result is True\n\n\ndef test_ask_collection_consent_no():\n    with mock.patch(\"builtins.input\", return_value=\"n\"):\n        result = learning.ask_collection_consent()\n\n    assert result is False\n\n\ndef test_extract_learning():\n    review = learning.Review(\n        raw=\"y, n, y\",\n        ran=True,\n        works=True,\n        perfect=False,\n        comments=\"The code is not perfect\",\n    )\n    memory = mock.Mock(spec=DiskMemory)\n    memory.to_json.return_value = {\"prompt\": \"prompt\"}\n\n    result = learning.extract_learning(\n        Prompt(\"prompt\"),\n        \"model_name\",\n        0.01,\n        (\"prompt_tokens\", \"completion_tokens\"),\n        memory,\n        review,\n    )\n\n    assert isinstance(result, Learning)\n\n\ndef test_get_session():\n    with mock.patch.object(learning, \"Path\") as path_mock:\n        # can be better tested with pyfakefs.\n        path_mock.return_value.__truediv__.return_value.exists.return_value = False\n\n        with mock.patch.object(learning, \"random\") as random_mock:\n            random_mock.randint.return_value = 42\n            result = learning.get_session()\n\n        assert result == \"42\"\n",
    "tests/applications/cli/test_main.py": "import dataclasses\nimport functools\nimport inspect\nimport os\nimport shutil\nimport tempfile\n\nfrom argparse import Namespace\nfrom unittest.mock import patch\n\nimport pytest\nimport typer\n\nimport gpt_engineer.applications.cli.main as main\n\nfrom gpt_engineer.applications.cli.main import load_prompt\nfrom gpt_engineer.core.default.disk_memory import DiskMemory\nfrom gpt_engineer.core.prompt import Prompt\n\n\n@functools.wraps(dataclasses.make_dataclass)\ndef dcommand(typer_f, **kwargs):\n    required = True\n\n    def field_desc(name, param):\n        nonlocal required\n\n        t = param.annotation or \"typing.Any\"\n        if param.default.default is not ...:\n            required = False\n            return name, t, dataclasses.field(default=param.default.default)\n\n        if not required:\n            raise ValueError(\"Required value after optional\")\n\n        return name, t\n\n    kwargs.setdefault(\"cls_name\", typer_f.__name__)\n\n    params = inspect.signature(typer_f).parameters\n    kwargs[\"fields\"] = [field_desc(k, v) for k, v in params.items()]\n\n    @functools.wraps(typer_f)\n    def dcommand_decorator(function_or_class):\n        assert callable(function_or_class)\n\n        ka = dict(kwargs)\n        ns = Namespace(**(ka.pop(\"namespace\", None) or {}))\n        if isinstance(function_or_class, type):\n            ka[\"bases\"] = *ka.get(\"bases\", ()), function_or_class\n        else:\n            ns.__call__ = function_or_class\n\n        ka[\"namespace\"] = vars(ns)\n        return dataclasses.make_dataclass(**ka)\n\n    return dcommand_decorator\n\n\n@dcommand(main.main)\nclass DefaultArgumentsMain:\n    def __call__(self):\n        attribute_dict = vars(self)\n        main.main(**attribute_dict)\n\n\ndef input_generator():\n    yield \"y\"  # First response\n    while True:\n        yield \"n\"  # Subsequent responses\n\n\nprompt_text = \"Make a python program that writes 'hello' to a file called 'output.txt'\"\n\n\nclass TestMain:\n    #  Runs gpt-engineer cli interface for many parameter configurations, BUT DOES NOT CODEGEN! Only testing cli.\n    def test_default_settings_generate_project(self, tmp_path, monkeypatch):\n        p = tmp_path / \"projects/example\"\n        p.mkdir(parents=True)\n        (p / \"prompt\").write_text(prompt_text)\n        args = DefaultArgumentsMain(str(p), llm_via_clipboard=True, no_execution=True)\n        args()\n\n    #  Runs gpt-engineer with improve mode and improves an existing project in the specified path.\n    def test_improve_existing_project(self, tmp_path, monkeypatch):\n        p = tmp_path / \"projects/example\"\n        p.mkdir(parents=True)\n        (p / \"prompt\").write_text(prompt_text)\n        args = DefaultArgumentsMain(\n            str(p), improve_mode=True, llm_via_clipboard=True, no_execution=True\n        )\n        args()\n\n    #  Runs gpt-engineer with improve mode and improves an existing project in the specified path, with skip_file_selection\n    def test_improve_existing_project_skip_file_selection(self, tmp_path, monkeypatch):\n        p = tmp_path / \"projects/example\"\n        p.mkdir(parents=True)\n        (p / \"prompt\").write_text(prompt_text)\n        args = DefaultArgumentsMain(\n            str(p),\n            improve_mode=True,\n            llm_via_clipboard=True,\n            no_execution=True,\n            skip_file_selection=True,\n        )\n        args()\n        assert args.skip_file_selection, \"Skip_file_selection not set\"\n\n    #  Runs gpt-engineer with improve mode and improves an existing project in the specified path, with skip_file_selection\n    def test_improve_existing_project_diff_timeout(self, tmp_path, monkeypatch):\n        p = tmp_path / \"projects/example\"\n        p.mkdir(parents=True)\n        (p / \"prompt\").write_text(prompt_text)\n        args = DefaultArgumentsMain(\n            str(p),\n            improve_mode=True,\n            llm_via_clipboard=True,\n            no_execution=True,\n            diff_timeout=99,\n        )\n        args()\n        assert args.diff_timeout == 99, \"Diff timeout not set\"\n\n        # def improve_generator():\n        #     yield \"y\"\n        #     while True:\n        #         yield \"n\"  # Subsequent responses\n        #\n        # gen = improve_generator()\n        # monkeypatch.setattr(\"builtins.input\", lambda _: next(gen))\n        # p = tmp_path / \"projects/example\"\n        # p.mkdir(parents=True)\n        # (p / \"prompt\").write_text(prompt_text)\n        # (p / \"main.py\").write_text(\"The program will be written in this file\")\n        # meta_p = p / META_DATA_REL_PATH\n        # meta_p.mkdir(parents=True)\n        # (meta_p / \"file_selection.toml\").write_text(\n        #     \"\"\"\n        # [files]\n        # \"main.py\" = \"selected\"\n        #             \"\"\"\n        # )\n        # os.environ[\"GPTE_TEST_MODE\"] = \"True\"\n        # simplified_main(str(p), \"improve\")\n        # DiskExecutionEnv(path=p)\n        # del os.environ[\"GPTE_TEST_MODE\"]\n\n    #  Runs gpt-engineer with lite mode and generates a project with only the main prompt.\n    def test_lite_mode_generate_project(self, tmp_path, monkeypatch):\n        p = tmp_path / \"projects/example\"\n        p.mkdir(parents=True)\n        (p / \"prompt\").write_text(prompt_text)\n        args = DefaultArgumentsMain(\n            str(p), lite_mode=True, llm_via_clipboard=True, no_execution=True\n        )\n        args()\n\n    #  Runs gpt-engineer with clarify mode and generates a project after discussing the specification with the AI.\n    def test_clarify_mode_generate_project(self, tmp_path, monkeypatch):\n        p = tmp_path / \"projects/example\"\n        p.mkdir(parents=True)\n        (p / \"prompt\").write_text(prompt_text)\n        args = DefaultArgumentsMain(\n            str(p), clarify_mode=True, llm_via_clipboard=True, no_execution=True\n        )\n        args()\n\n    #  Runs gpt-engineer with self-heal mode and generates a project after discussing the specification with the AI and self-healing the code.\n    def test_self_heal_mode_generate_project(self, tmp_path, monkeypatch):\n        p = tmp_path / \"projects/example\"\n        p.mkdir(parents=True)\n        (p / \"prompt\").write_text(prompt_text)\n        args = DefaultArgumentsMain(\n            str(p), self_heal_mode=True, llm_via_clipboard=True, no_execution=True\n        )\n        args()\n\n    def test_clarify_lite_improve_mode_generate_project(self, tmp_path, monkeypatch):\n        p = tmp_path / \"projects/example\"\n        p.mkdir(parents=True)\n        (p / \"prompt\").write_text(prompt_text)\n        args = DefaultArgumentsMain(\n            str(p),\n            improve_mode=True,\n            lite_mode=True,\n            clarify_mode=True,\n            llm_via_clipboard=True,\n            no_execution=True,\n        )\n        pytest.raises(typer.Exit, args)\n\n    #  Tests the creation of a log file in improve mode.\n\n\nclass TestLoadPrompt:\n    #  Load prompt from existing file in input_repo\n    def test_load_prompt_existing_file(self):\n        with tempfile.TemporaryDirectory() as tmp_dir:\n            input_repo = DiskMemory(tmp_dir)\n            prompt_file = \"prompt.txt\"\n            prompt_content = \"This is the prompt\"\n            input_repo[prompt_file] = prompt_content\n\n            improve_mode = False\n            image_directory = \"\"\n\n            result = load_prompt(input_repo, improve_mode, prompt_file, image_directory)\n\n            assert isinstance(result, Prompt)\n            assert result.text == prompt_content\n            assert result.image_urls is None\n\n    #  Prompt file does not exist in input_repo, and improve_mode is False\n    def test_load_prompt_no_file_improve_mode_false(self):\n        with tempfile.TemporaryDirectory() as tmp_dir:\n            input_repo = DiskMemory(tmp_dir)\n            prompt_file = \"prompt.txt\"\n\n            improve_mode = False\n            image_directory = \"\"\n\n            with patch(\n                \"builtins.input\",\n                return_value=\"What application do you want gpt-engineer to generate?\",\n            ):\n                result = load_prompt(\n                    input_repo, improve_mode, prompt_file, image_directory\n                )\n\n            assert isinstance(result, Prompt)\n            assert (\n                result.text == \"What application do you want gpt-engineer to generate?\"\n            )\n            assert result.image_urls is None\n\n    #  Prompt file is a directory\n    def test_load_prompt_directory_file(self):\n        with tempfile.TemporaryDirectory() as tmp_dir:\n            input_repo = DiskMemory(tmp_dir)\n            prompt_file = os.path.join(tmp_dir, \"prompt\")\n\n            os.makedirs(os.path.join(tmp_dir, prompt_file))\n\n            improve_mode = False\n            image_directory = \"\"\n\n            with pytest.raises(ValueError):\n                load_prompt(input_repo, improve_mode, prompt_file, image_directory)\n\n    #  Prompt file is empty\n    def test_load_prompt_empty_file(self):\n        with tempfile.TemporaryDirectory() as tmp_dir:\n            input_repo = DiskMemory(tmp_dir)\n            prompt_file = \"prompt.txt\"\n            input_repo[prompt_file] = \"\"\n\n            improve_mode = False\n            image_directory = \"\"\n\n            with patch(\n                \"builtins.input\",\n                return_value=\"What application do you want gpt-engineer to generate?\",\n            ):\n                result = load_prompt(\n                    input_repo, improve_mode, prompt_file, image_directory\n                )\n\n            assert isinstance(result, Prompt)\n            assert (\n                result.text == \"What application do you want gpt-engineer to generate?\"\n            )\n            assert result.image_urls is None\n\n    #  image_directory does not exist in input_repo\n    def test_load_prompt_no_image_directory(self):\n        with tempfile.TemporaryDirectory() as tmp_dir:\n            input_repo = DiskMemory(tmp_dir)\n            prompt_file = \"prompt.txt\"\n            prompt_content = \"This is the prompt\"\n            input_repo[prompt_file] = prompt_content\n\n            improve_mode = False\n            image_directory = \"tests/test_data\"\n            shutil.copytree(image_directory, os.path.join(tmp_dir, image_directory))\n\n            result = load_prompt(input_repo, improve_mode, prompt_file, image_directory)\n\n            assert isinstance(result, Prompt)\n            assert result.text == prompt_content\n            assert \"mona_lisa.jpg\" in result.image_urls\n\n\n#     def test_log_creation_in_improve_mode(self, tmp_path, monkeypatch):\n#         def improve_generator():\n#             yield \"y\"\n#             while True:\n#                 yield \"n\"  # Subsequent responses\n#\n#         gen = improve_generator()\n#         monkeypatch.setattr(\"builtins.input\", lambda _: next(gen))\n#         p = tmp_path / \"projects/example\"\n#         p.mkdir(parents=True)\n#         (p / \"prompt\").write_text(prompt_text)\n#         (p / \"main.py\").write_text(\"The program will be written in this file\")\n#         meta_p = p / META_DATA_REL_PATH\n#         meta_p.mkdir(parents=True)\n#         (meta_p / \"file_selection.toml\").write_text(\n#             \"\"\"\n#         [files]\n#         \"main.py\" = \"selected\"\n#                     \"\"\"\n#         )\n#         os.environ[\"GPTE_TEST_MODE\"] = \"True\"\n#         simplified_main(str(p), \"improve\")\n#         DiskExecutionEnv(path=p)\n#         assert (\n#             (p / f\".gpteng/memory/{DEBUG_LOG_FILE}\").read_text().strip()\n#             == \"\"\"UPLOADED FILES:\n# ```\n# File: main.py\n# 1 The program will be written in this file\n#\n# ```\n# PROMPT:\n# Make a python program that writes 'hello' to a file called 'output.txt'\n# CONSOLE OUTPUT:\"\"\"\n#         )\n#         del os.environ[\"GPTE_TEST_MODE\"]\n#\n#     def test_log_creation_in_improve_mode_with_failing_diff(\n#         self, tmp_path, monkeypatch\n#     ):\n#         def improve_generator():\n#             yield \"y\"\n#             while True:\n#                 yield \"n\"  # Subsequent responses\n#\n#         def mock_salvage_correct_hunks(\n#             messages: List, files_dict: FilesDict, error_message: List\n#         ) -> FilesDict:\n#             # create a falling diff\n#             messages[\n#                 -1\n#             ].content = \"\"\"To create a Python program that writes 'hello' to a file called 'output.txt', we will need to perform the following steps:\n#\n# 1. Open the file 'output.txt' in write mode.\n# 2. Write the string 'hello' to the file.\n# 3. Close the file to ensure the data is written and the file is not left open.\n#\n# Here is the implementation of the program in the `main.py` file:\n#\n# ```diff\n# --- main.py\n# +++ main.py\n# @@ -0,0 +1,9 @@\n# -create falling diff\n# ```\n#\n# This concludes a fully working implementation.\"\"\"\n#             # Call the original function with modified messages or define your own logic\n#             return salvage_correct_hunks(messages, files_dict, error_message)\n#\n#         gen = improve_generator()\n#         monkeypatch.setattr(\"builtins.input\", lambda _: next(gen))\n#         monkeypatch.setattr(\n#             \"gpt_engineer.core.default.steps.salvage_correct_hunks\",\n#             mock_salvage_correct_hunks,\n#         )\n#         p = tmp_path / \"projects/example\"\n#         p.mkdir(parents=True)\n#         (p / \"prompt\").write_text(prompt_text)\n#         (p / \"main.py\").write_text(\"The program will be written in this file\")\n#         meta_p = p / META_DATA_REL_PATH\n#         meta_p.mkdir(parents=True)\n#         (meta_p / \"file_selection.toml\").write_text(\n#             \"\"\"\n#         [files]\n#         \"main.py\" = \"selected\"\n#                     \"\"\"\n#         )\n#         os.environ[\"GPTE_TEST_MODE\"] = \"True\"\n#         simplified_main(str(p), \"improve\")\n#         DiskExecutionEnv(path=p)\n#         assert (\n#             (p / f\".gpteng/memory/{DEBUG_LOG_FILE}\").read_text().strip()\n#             == \"\"\"UPLOADED FILES:\n# ```\n# File: main.py\n# 1 The program will be written in this file\n#\n# ```\n# PROMPT:\n# Make a python program that writes 'hello' to a file called 'output.txt'\n# CONSOLE OUTPUT:\n# Invalid hunk: @@ -0,0 +1,9 @@\n# -create falling diff\n#\n# Invalid hunk: @@ -0,0 +1,9 @@\n# -create falling diff\"\"\"\n#         )\n#         del os.environ[\"GPTE_TEST_MODE\"]\n#\n#     def test_log_creation_in_improve_mode_with_unexpected_exceptions(\n#         self, tmp_path, monkeypatch\n#     ):\n#         def improve_generator():\n#             yield \"y\"\n#             while True:\n#                 yield \"n\"  # Subsequent responses\n#\n#         def mock_salvage_correct_hunks(\n#             messages: List, files_dict: FilesDict, error_message: List\n#         ) -> FilesDict:\n#             raise Exception(\"Mock exception in salvage_correct_hunks\")\n#\n#         gen = improve_generator()\n#         monkeypatch.setattr(\"builtins.input\", lambda _: next(gen))\n#         monkeypatch.setattr(\n#             \"gpt_engineer.core.default.steps.salvage_correct_hunks\",\n#             mock_salvage_correct_hunks,\n#         )\n#         p = tmp_path / \"projects/example\"\n#         p.mkdir(parents=True)\n#         (p / \"prompt\").write_text(prompt_text)\n#         (p / \"main.py\").write_text(\"The program will be written in this file\")\n#         meta_p = p / META_DATA_REL_PATH\n#         meta_p.mkdir(parents=True)\n#         (meta_p / \"file_selection.toml\").write_text(\n#             \"\"\"\n#         [files]\n#         \"main.py\" = \"selected\"\n#                     \"\"\"\n#         )\n#         os.environ[\"GPTE_TEST_MODE\"] = \"True\"\n#         simplified_main(str(p), \"improve\")\n#         DiskExecutionEnv(path=p)\n#         assert (\n#             (p / f\".gpteng/memory/{DEBUG_LOG_FILE}\").read_text().strip()\n#             == \"\"\"UPLOADED FILES:\n# ```\n# File: main.py\n# 1 The program will be written in this file\n#\n# ```\n# PROMPT:\n# Make a python program that writes 'hello' to a file called 'output.txt'\n# CONSOLE OUTPUT:\n# Error while improving the project: Mock exception in salvage_correct_hunks\"\"\"\n#         )\n#         del os.environ[\"GPTE_TEST_MODE\"]\n",
    "tests/benchmark/test_BenchConfig.py": "# Generated by CodiumAI\n\nimport pytest\n\nfrom gpt_engineer.benchmark.bench_config import (\n    AppsConfig,\n    BenchConfig,\n    GptmeConfig,\n    MbppConfig,\n)\n\n\nclass TestBenchConfig:\n    #  Creating a BenchConfig object with default values should return an instance of BenchConfig with all attributes set to their default values.\n    def test_default_values(self):\n        config = BenchConfig()\n        assert isinstance(config.apps, AppsConfig)\n        assert isinstance(config.mbpp, MbppConfig)\n        assert isinstance(config.gptme, GptmeConfig)\n        assert config.apps.active is True\n        assert config.apps.test_start_index == 0\n        assert config.apps.test_end_index == 1\n        assert config.apps.train_start_index == 0\n        assert config.apps.train_end_index == 0\n        assert config.mbpp.active is True\n        assert config.mbpp.test_len == 1\n        assert config.mbpp.train_len == 0\n        assert config.gptme.active is True\n\n    #  Creating a BenchConfig object with specific values should return an instance of BenchConfig with the specified attributes set to the specified values.\n    def test_specific_values(self):\n        config = BenchConfig(\n            apps=AppsConfig(\n                active=False,\n                test_start_index=1,\n                test_end_index=2,\n                train_start_index=3,\n                train_end_index=4,\n            ),\n            mbpp=MbppConfig(active=False, test_len=5, train_len=6),\n            gptme=GptmeConfig(active=False),\n        )\n        assert isinstance(config.apps, AppsConfig)\n        assert isinstance(config.mbpp, MbppConfig)\n        assert isinstance(config.gptme, GptmeConfig)\n        assert config.apps.active is False\n        assert config.apps.test_start_index == 1\n        assert config.apps.test_end_index == 2\n        assert config.apps.train_start_index == 3\n        assert config.apps.train_end_index == 4\n        assert config.mbpp.active is False\n        assert config.mbpp.test_len == 5\n        assert config.mbpp.train_len == 6\n        assert config.gptme.active is False\n\n    #  Calling the from_dict method with a valid dictionary should return an instance of BenchConfig with attributes set according to the values in the dictionary.\n    def test_from_dict_valid_dict(self):\n        config_dict = {\n            \"apps\": {\n                \"active\": False,\n                \"test_start_index\": 1,\n                \"test_end_index\": 2,\n                \"train_start_index\": 3,\n                \"train_end_index\": 4,\n            },\n            \"mbpp\": {\"active\": False, \"test_len\": 5, \"train_len\": 6},\n            \"gptme\": {\"active\": False},\n        }\n        config = BenchConfig.from_dict(config_dict)\n        assert isinstance(config.apps, AppsConfig)\n        assert isinstance(config.mbpp, MbppConfig)\n        assert isinstance(config.gptme, GptmeConfig)\n        assert config.apps.active is False\n        assert config.apps.test_start_index == 1\n        assert config.apps.test_end_index == 2\n        assert config.apps.train_start_index == 3\n        assert config.apps.train_end_index == 4\n        assert config.mbpp.active is False\n        assert config.mbpp.test_len == 5\n        assert config.mbpp.train_len == 6\n        assert config.gptme.active is False\n\n    #  Calling the from_toml method with an invalid path to a TOML file should raise an appropriate exception.\n    def test_from_toml_invalid_path(self):\n        config_file = \"invalid_config.toml\"\n        with pytest.raises(Exception):\n            BenchConfig.from_toml(config_file)\n"
  },
  "requirements": null
}