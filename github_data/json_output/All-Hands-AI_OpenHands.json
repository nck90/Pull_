{
  "repo_name": "All-Hands-AI/OpenHands",
  "repo_url": "https://github.com/All-Hands-AI/OpenHands",
  "description": "ðŸ™Œ OpenHands: Code Less, Make More",
  "stars": 50853,
  "language": "Python",
  "created_at": "2024-03-13T03:33:31Z",
  "updated_at": "2025-03-19T06:51:30Z",
  "files": {
    "evaluation/benchmarks/testgeneval/compute_readability.py": "import math\n\n\ndef total_byte_entropy_stats(python_code):\n    # Count the occurrence of each byte (character for simplicity)\n    byte_counts = {}\n    for byte in python_code.encode('utf-8'):\n        byte_counts[byte] = byte_counts.get(byte, 0) + 1\n\n    total_bytes = sum(byte_counts.values())\n    entropy = -sum(\n        (count / total_bytes) * math.log2(count / total_bytes)\n        for count in byte_counts.values()\n    )\n\n    return {'total_byte_entropy': entropy}\n\n\ndef average_nulls_stats(tree, num_lines):\n    total_nulls = 0\n    nulls_per_line = {}  # Dictionary to count nulls per line\n\n    def traverse(node):\n        nonlocal total_nulls\n        if node.type == 'null_literal':\n            total_nulls += 1\n            line_number = node.start_point[0]  # Get line number\n            if line_number in nulls_per_line:\n                nulls_per_line[line_number] += 1\n            else:\n                nulls_per_line[line_number] = 1\n        for child in node.children:\n            traverse(child)\n\n    traverse(tree.root_node)\n\n    # Calculate average nulls per line\n    avg_nulls = total_nulls / num_lines if num_lines > 0 else 0\n\n    # Calculate max nulls on any line\n    max_nulls_on_any_line = max(nulls_per_line.values()) if nulls_per_line else 0\n\n    return {\n        'avg_nulls': avg_nulls,\n        'total_nulls': total_nulls,\n        'max_nulls': max_nulls_on_any_line,\n        'has_nulls': 1 if total_nulls > 0 else 0,\n    }\n\n\ndef arithmetic_operations_stats(tree, num_lines):\n    # Dictionary to hold counts of each arithmetic operation\n    op_counts = {'+': 0, '-': 0, '*': 0, '/': 0, '%': 0}\n    total_ops = 0\n\n    # Function to traverse the AST and update operation counts\n    def traverse(node):\n        nonlocal total_ops\n        if node.type == 'binary_expression' or node.type == 'update_expression':\n            for child in node.children:\n                if child.type == 'operator':\n                    op = child.text.decode('utf8')\n                    if op in op_counts:\n                        op_counts[op] += 1\n                        total_ops += 1\n        else:\n            for child in node.children:\n                traverse(child)\n\n    traverse(tree.root_node)\n\n    return {\n        'total_arithmetic_operations': total_ops,\n        'avg_arithmetic_operations': total_ops / num_lines,\n    }\n\n\ndef numbers_floats_stats(tree, num_lines):\n    total_numbers = 0\n    total_floats = 0\n\n    def traverse(node):\n        nonlocal total_numbers, total_floats\n        if node.type in ['integer_literal', 'decimal_literal']:\n            total_numbers += 1\n            if (\n                '.' in node.text.decode('utf8')\n                or 'e' in node.text.decode('utf8').lower()\n            ):\n                total_floats += 1\n        for child in node.children:\n            traverse(child)\n\n    traverse(tree.root_node)\n    return {'total_numbers': total_numbers, 'total_floats': total_floats}\n\n\ndef code_stats(python_code):\n    lines = python_code.strip().split('\\n')\n    total_line_length = sum(len(line) for line in lines)\n    max_line_length = max(len(line) for line in lines)\n    return {\n        'total_line_length': total_line_length,\n        'max_line_length': max_line_length,\n        'avg_characters': total_line_length / len(lines),\n    }\n\n\ndef assertions_stats(tree, num_lines):\n    total_assertions = 0\n\n    def traverse(node):\n        nonlocal total_assertions\n        if node.type == 'assert_statement':\n            total_assertions += 1\n        for child in node.children:\n            traverse(child)\n\n    traverse(tree.root_node)\n    return {\n        'total_assertions': total_assertions,\n        'total_has_assertions': 1 if total_assertions > 0 else 0,\n    }\n\n\ndef class_instances_stats(tree, num_lines):\n    total_class_instances = 0\n\n    def traverse(node):\n        nonlocal total_class_instances\n        if node.type == 'object_creation_expression':\n            total_class_instances += 1\n        for child in node.children:\n            traverse(child)\n\n    traverse(tree.root_node)\n    return {'total_class_instances': total_class_instances}\n\n\ndef has_execeptions(tree, num_lines):\n    total_has_exceptions = 0\n\n    def traverse(node):\n        nonlocal total_has_exceptions\n        if node.type == 'try_statement':\n            total_has_exceptions += 1\n        for child in node.children:\n            traverse(child)\n\n    traverse(tree.root_node)\n    return {'total_has_exceptions': 1 if total_has_exceptions > 0 else 0}\n\n\ndef distinct_methods_stats(tree, num_lines):\n    method_names = set()\n    total_nodes = 0\n\n    def traverse(node):\n        nonlocal total_nodes\n        if node.type == 'method_declaration':\n            for child in node.children:\n                if child.type == 'identifier':\n                    method_names.add(child.text.decode('utf8'))\n                    break\n        total_nodes += 1\n        for child in node.children:\n            traverse(child)\n\n    traverse(tree.root_node)\n    total_distinct_methods = len(method_names)\n    total_method_ratio = (\n        total_distinct_methods / (total_nodes - total_distinct_methods)\n        if total_nodes > total_distinct_methods\n        else 0\n    )\n\n    return {\n        'total_distinct_methods': total_distinct_methods,\n        'total_method_ratio': total_method_ratio,\n    }\n\n\ndef loops_stats(tree, num_lines):\n    \"\"\"\n    Calculate the average number of loops.\n    \"\"\"\n    total_loops = 0\n\n    def traverse(node):\n        nonlocal total_loops\n        if node.type in ['for_statement', 'while_statement', 'do_statement']:\n            total_loops += 1\n        for child in node.children:\n            traverse(child)\n\n    traverse(tree.root_node)\n    avg_loops = total_loops / num_lines\n    return {'avg_loops': avg_loops}\n\n\ndef branches_stats(tree, num_lines):\n    \"\"\"\n    Calculate the average number of branches (conditional statements).\n    \"\"\"\n    total_branches = 0\n\n    def traverse(node):\n        nonlocal total_branches\n        if node.type in ['if_statement', 'switch_statement']:\n            total_branches += 1\n        for child in node.children:\n            traverse(child)\n\n    traverse(tree.root_node)\n    # Assuming each branch is its own, this might need refinement based on definition\n    avg_branches = total_branches / num_lines\n    return {'avg_branches': avg_branches}\n\n\ndef string_stats(tree, num_lines):\n    string_literals = []\n\n    # Function to traverse the AST and collect string literals\n    def traverse(node):\n        if node.type == 'string_literal':\n            # Extracting the string literal, excluding the quotation marks\n            literal_text = node.text.decode('utf8')[1:-1]\n            string_literals.append(literal_text)\n        for child in node.children:\n            traverse(child)\n\n    traverse(tree.root_node)\n\n    # Calculate the average string length\n    total_length = sum(len(s) for s in string_literals)\n    avg_length = total_length / num_lines\n    return {'avg_str_length': avg_length}\n\n\ndef identifier_stats(tree, num_lines):\n    root_node = tree.root_node\n    identifier_counts = {}  # Dictionary to count occurrences of each identifier\n    total_nodes = 0  # Counter for all nodes\n\n    # Function to recursively count identifiers and all nodes, gathering their stats\n    def count(node):\n        nonlocal identifier_counts, total_nodes\n        iden_count = 0\n        max_length = 0\n        total_nodes += 1  # Increment total nodes for every node visited\n        if node.type == 'identifier':\n            identifier = node.text.decode('utf8')  # Assuming UTF-8 encoding\n            iden_count += 1\n            identifier_counts[identifier] = identifier_counts.get(identifier, 0) + 1\n            iden_length = len(identifier)\n            if iden_length > max_length:\n                max_length = iden_length\n        for child in node.children:\n            child_count, child_max_length = count(child)\n            iden_count += child_count\n            if child_max_length > max_length:\n                max_length = child_max_length\n        return iden_count, max_length\n\n    total_identifiers, max_identifier_length = count(root_node)\n    total_unique_identifiers = len(identifier_counts)\n    total_identifier_length = sum(len(k) * v for k, v in identifier_counts.items())\n    avg_identifier_length = total_identifier_length / num_lines\n\n    # Calculate the identifier ratio as total identifiers over total nodes\n    identifier_ratio = total_identifiers / total_nodes if total_nodes > 0 else 0\n\n    return {\n        'total_identifiers': total_identifiers,\n        'total_identifier_length': total_identifier_length,\n        'max_identifier_length': max_identifier_length,\n        'avg_identifier_length': avg_identifier_length,\n        'total_unique_identifiers': total_unique_identifiers,\n        'total_identifier_ratio': identifier_ratio,  # Include the new ratio in the returned dictionary\n        'total_nodes': total_nodes,  # Include total node count for reference or further calculations\n    }\n\n\ndef compute_regression(results):\n    components = {\n        'total_line_length': -0.0001,\n        'max_line_length': -0.0021,\n        'total_identifiers': 0.0076,\n        'total_identifier_length': -0.0004,\n        'max_identifier_length': -0.0067,\n        'avg_identifier_length': -0.005,\n        'avg_arithmetic_operations': 0.0225,\n        'avg_branches': 0.9886,\n        'avg_loops': 0.1572,\n        'total_assertions': 0.0119,\n        'total_has_assertions': -0.0147,\n        'avg_characters': 0.1242,\n        'total_class_instances': -0.043,\n        'total_distinct_methods': -0.0127,\n        'avg_str_length': 0.0026,\n        'total_has_exceptions': 0.1206,\n        'total_unique_identifiers': -0.019,\n        'max_nulls': -0.0712,\n        'total_numbers': -0.0078,\n        'avg_nulls': 0.1444,\n        'total_identifier_ratio': 0.334,\n        'total_method_ratio': 0.0406,\n        'total_floats': -0.0174,\n        'total_byte_entropy': -0.3917,\n    }\n    test_score = 0\n\n    for component in components:\n        test_score += components[component] * results[component]\n\n    test_score += 5.7501\n    return test_score\n\n\ndef compute_readability(python_code):\n    # Create parser and set up language\n    import tree_sitter_python\n    from tree_sitter import Language, Parser\n\n    parser = Parser(Language(tree_sitter_python.language()))\n\n    results = code_stats(python_code)\n\n    num_lines = len(python_code.strip().split('\\n'))\n    results.update(total_byte_entropy_stats(python_code))\n\n    tree = parser.parse(bytes(python_code, 'utf8'))\n\n    results.update(identifier_stats(tree, num_lines))\n    results.update(loops_stats(tree, num_lines))\n    results.update(branches_stats(tree, num_lines))\n    results.update(distinct_methods_stats(tree, num_lines))\n    results.update(has_execeptions(tree, num_lines))\n    results.update(class_instances_stats(tree, num_lines))\n    results.update(assertions_stats(tree, num_lines))\n    results.update(numbers_floats_stats(tree, num_lines))\n    results.update(average_nulls_stats(tree, num_lines))\n    results.update(arithmetic_operations_stats(tree, num_lines))\n    results.update(string_stats(tree, num_lines))\n\n    score = compute_regression(results)\n    return score\n",
    "evaluation/benchmarks/testgeneval/constants.py": "from enum import Enum\nfrom pathlib import Path\nfrom typing import Any, TypedDict\n\n# Constants - Evaluation Log Directories\nBASE_IMAGE_BUILD_DIR = Path('logs/build_images/base')\nENV_IMAGE_BUILD_DIR = Path('logs/build_images/env')\nINSTANCE_IMAGE_BUILD_DIR = Path('logs/build_images/instances')\nRUN_EVALUATION_LOG_DIR = Path('logs/run_evaluation')\n\n\n# Constants - Task Instance Class\nclass TestGenEvalInstance(TypedDict):\n    repo: str\n    base_commit: str\n    version: str\n    instance_id: str\n    patch: str\n    test_patch: str\n    preds_context: dict\n    code_src: str\n    test_src: str\n    code_file: str\n    test_file: str\n    local_imports: list\n    id: str\n    baseline_covs: dict\n\n\nclass TestStatus(Enum):\n    FAILED = 'FAILED'\n    PASSED = 'PASSED'\n    SKIPPED = 'SKIPPED'\n    ERROR = 'ERROR'\n    XFAIL = 'XFAIL'\n\n\nFAIL_TO_PASS = 'FAIL_TO_PASS'\nFAIL_TO_FAIL = 'FAIL_TO_FAIL'\nPASS_TO_PASS = 'PASS_TO_PASS'\nPASS_TO_FAIL = 'PASS_TO_FAIL'\n\nTEST_PYTEST = 'coverage run -m pytest --no-header -rA --tb=no -p no:cacheprovider'\nTEST_PYTEST_VERBOSE = 'coverage run -m pytest -rA --tb=long -p no:cacheprovider'\nTEST_ASTROPY_PYTEST = (\n    'coverage run -m pytest -rA -vv -o console_output_style=classic --tb=no'\n)\nTEST_DJANGO = (\n    'coverage run ./tests/runtests.py --verbosity 2 --settings=test_sqlite --parallel 1'\n)\nTEST_DJANGO_NO_PARALLEL = 'coverage run ./tests/runtests.py --verbosity 2'\nTEST_SEABORN = 'coverage run -m pytest --no-header -rA'\nTEST_SEABORN_VERBOSE = 'coverage run -m pytest -rA --tb=long'\nTEST_PYTEST = 'coverage run -m pytest -rA'\nTEST_PYTEST_VERBOSE = 'coverage run -m pytest -rA --tb=long'\nTEST_SPHINX = 'tox --current-env -epy39 -v --'\nTEST_SYMPY = \"PYTHONWARNINGS='ignore::UserWarning,ignore::SyntaxWarning' coverage run bin/test -C --verbose\"\nTEST_SYMPY_VERBOSE = 'coverage run bin/test -C --verbose'\n\n\n# Constants - Installation Specifications\nSPECS_SKLEARN = {\n    k: {\n        'python': '3.6',\n        'packages': 'numpy scipy cython pytest pandas matplotlib',\n        'install': 'python -m pip install -v --no-use-pep517 --no-build-isolation -e .',\n        'pip_packages': [\n            'cython',\n            'numpy==1.19.2',\n            'setuptools',\n            'scipy==1.5.2',\n        ],\n        'test_cmd': TEST_PYTEST,\n    }\n    for k in ['0.20', '0.21', '0.22']\n}\nSPECS_SKLEARN.update(\n    {\n        k: {\n            'python': '3.9',\n            'packages': \"'numpy==1.19.2' 'scipy==1.5.2' 'cython==3.0.10' pytest 'pandas<2.0.0' 'matplotlib<3.9.0' setuptools pytest joblib threadpoolctl\",\n            'install': 'python -m pip install -v --no-use-pep517 --no-build-isolation -e .',\n            'pip_packages': ['cython', 'setuptools', 'numpy', 'scipy'],\n            'test_cmd': TEST_PYTEST,\n        }\n        for k in ['1.3', '1.4', '1.5', '1.6']\n    }\n)\n\nSPECS_FLASK = {\n    '2.0': {\n        'python': '3.9',\n        'packages': 'requirements.txt',\n        'install': 'python -m pip install -e .',\n        'pip_packages': [\n            'setuptools==70.0.0',\n            'Werkzeug==2.3.7',\n            'Jinja2==3.0.1',\n            'itsdangerous==2.1.2',\n            'click==8.0.1',\n            'MarkupSafe==2.1.3',\n        ],\n        'test_cmd': TEST_PYTEST,\n    },\n    '2.1': {\n        'python': '3.10',\n        'packages': 'requirements.txt',\n        'install': 'python -m pip install -e .',\n        'pip_packages': [\n            'click==8.1.3',\n            'itsdangerous==2.1.2',\n            'Jinja2==3.1.2',\n            'MarkupSafe==2.1.1',\n            'Werkzeug==2.3.7',\n        ],\n        'test_cmd': TEST_PYTEST,\n    },\n}\nSPECS_FLASK.update(\n    {\n        k: {\n            'python': '3.11',\n            'packages': 'requirements.txt',\n            'install': 'python -m pip install -e .',\n            'pip_packages': [\n                'click==8.1.3',\n                'itsdangerous==2.1.2',\n                'Jinja2==3.1.2',\n                'MarkupSafe==2.1.1',\n                'Werkzeug==2.3.7',\n            ],\n            'test_cmd': TEST_PYTEST,\n        }\n        for k in ['2.2', '2.3', '3.0', '3.1']\n    }\n)\n\nSPECS_DJANGO = {\n    k: {\n        'python': '3.5',\n        'packages': 'requirements.txt',\n        'pre_install': [\n            'apt-get update && apt-get install -y locales',\n            \"echo 'en_US UTF-8' > /etc/locale.gen\",\n            'locale-gen en_US.UTF-8',\n        ],\n        'install': 'python setup.py install',\n        'pip_packages': ['setuptools'],\n        'eval_commands': [\n            'export LANG=en_US.UTF-8',\n            'export LC_ALL=en_US.UTF-8',\n            'export PYTHONIOENCODING=utf8',\n            'export LANGUAGE=en_US:en',\n        ],\n        'test_cmd': TEST_DJANGO,\n    }\n    for k in ['1.7', '1.8', '1.9', '1.10', '1.11', '2.0', '2.1', '2.2']\n}\nSPECS_DJANGO.update(\n    {\n        k: {\n            'python': '3.5',\n            'install': 'python setup.py install',\n            'test_cmd': TEST_DJANGO,\n        }\n        for k in ['1.4', '1.5', '1.6']\n    }\n)\nSPECS_DJANGO.update(\n    {\n        k: {\n            'python': '3.6',\n            'packages': 'requirements.txt',\n            'install': 'python -m pip install -e .',\n            'eval_commands': [\n                \"sed -i '/en_US.UTF-8/s/^# //g' /etc/locale.gen && locale-gen\",\n                'export LANG=en_US.UTF-8',\n                'export LANGUAGE=en_US:en',\n                'export LC_ALL=en_US.UTF-8',\n            ],\n            'test_cmd': TEST_DJANGO,\n        }\n        for k in ['3.0', '3.1', '3.2']\n    }\n)\nSPECS_DJANGO.update(\n    {\n        k: {\n            'python': '3.8',\n            'packages': 'requirements.txt',\n            'install': 'python -m pip install -e .',\n            'test_cmd': TEST_DJANGO,\n        }\n        for k in ['4.0']\n    }\n)\nSPECS_DJANGO.update(\n    {\n        k: {\n            'python': '3.9',\n            'packages': 'requirements.txt',\n            'install': 'python -m pip install -e .',\n            'test_cmd': TEST_DJANGO,\n        }\n        for k in ['4.1', '4.2']\n    }\n)\nSPECS_DJANGO.update(\n    {\n        k: {\n            'python': '3.11',\n            'packages': 'requirements.txt',\n            'install': 'python -m pip install -e .',\n            'test_cmd': TEST_DJANGO,\n        }\n        for k in ['5.0', '5.1', '5.2']\n    }\n)\nSPECS_DJANGO['1.9']['test_cmd'] = TEST_DJANGO_NO_PARALLEL\n\nSPECS_REQUESTS = {\n    k: {\n        'python': '3.9',\n        'packages': 'pytest',\n        'install': 'python -m pip install .',\n        'test_cmd': TEST_PYTEST,\n    }\n    for k in ['0.7', '0.8', '0.9', '0.11', '0.13', '0.14', '1.1', '1.2', '2.0', '2.2']\n    + ['2.3', '2.4', '2.5', '2.7', '2.8', '2.9', '2.10', '2.11', '2.12', '2.17']\n    + ['2.18', '2.19', '2.22', '2.26', '2.25', '2.27', '2.31', '3.0']\n}\n\nSPECS_SEABORN = {\n    k: {\n        'python': '3.9',\n        'install': 'python -m pip install -e .',\n        'pip_packages': [\n            'contourpy==1.1.0',\n            'cycler==0.11.0',\n            'fonttools==4.42.1',\n            'importlib-resources==6.0.1',\n            'kiwisolver==1.4.5',\n            'matplotlib==3.7.2',\n            'numpy==1.25.2',\n            'packaging==23.1',\n            'pandas==1.3.5',  # 2.0.3\n            'pillow==10.0.0',\n            'pyparsing==3.0.9',\n            'pytest',\n            'python-dateutil==2.8.2',\n            'pytz==2023.3.post1',\n            'scipy==1.11.2',\n            'six==1.16.0',\n            'tzdata==2023.1',\n            'zipp==3.16.2',\n        ],\n        'test_cmd': TEST_SEABORN,\n    }\n    for k in ['0.11']\n}\nSPECS_SEABORN.update(\n    {\n        k: {\n            'python': '3.9',\n            'install': 'python -m pip install -e .[dev]',\n            'pip_packages': [\n                'contourpy==1.1.0',\n                'cycler==0.11.0',\n                'fonttools==4.42.1',\n                'importlib-resources==6.0.1',\n                'kiwisolver==1.4.5',\n                'matplotlib==3.7.2',\n                'numpy==1.25.2',\n                'packaging==23.1',\n                'pandas==2.0.0',\n                'pillow==10.0.0',\n                'pyparsing==3.0.9',\n                'pytest',\n                'python-dateutil==2.8.2',\n                'pytz==2023.3.post1',\n                'scipy==1.11.2',\n                'six==1.16.0',\n                'tzdata==2023.1',\n                'zipp==3.16.2',\n            ],\n            'test_cmd': TEST_SEABORN,\n        }\n        for k in ['0.12', '0.13', '0.14']\n    }\n)\n\nSPECS_PYTEST = {\n    k: {\n        'python': '3.9',\n        'install': 'python -m pip install -e .',\n        'test_cmd': TEST_PYTEST,\n    }\n    for k in [\n        '4.4',\n        '4.5',\n        '4.6',\n        '5.0',\n        '5.1',\n        '5.2',\n        '5.3',\n        '5.4',\n        '6.0',\n        '6.2',\n        '6.3',\n        '7.0',\n        '7.1',\n        '7.2',\n        '7.4',\n        '8.0',\n        '8.1',\n        '8.2',\n        '8.3',\n        '8.4',\n    ]\n}\nSPECS_PYTEST['4.4']['pip_packages'] = [\n    'atomicwrites==1.4.1',\n    'attrs==23.1.0',\n    'more-itertools==10.1.0',\n    'pluggy==0.13.1',\n    'py==1.11.0',\n    'setuptools==68.0.0',\n    'six==1.16.0',\n]\nSPECS_PYTEST['4.5']['pip_packages'] = [\n    'atomicwrites==1.4.1',\n    'attrs==23.1.0',\n    'more-itertools==10.1.0',\n    'pluggy==0.11.0',\n    'py==1.11.0',\n    'setuptools==68.0.0',\n    'six==1.16.0',\n    'wcwidth==0.2.6',\n]\nSPECS_PYTEST['4.6']['pip_packages'] = [\n    'atomicwrites==1.4.1',\n    'attrs==23.1.0',\n    'more-itertools==10.1.0',\n    'packaging==23.1',\n    'pluggy==0.13.1',\n    'py==1.11.0',\n    'six==1.16.0',\n    'wcwidth==0.2.6',\n]\nfor k in ['5.0', '5.1', '5.2']:\n    SPECS_PYTEST[k]['pip_packages'] = [\n        'atomicwrites==1.4.1',\n        'attrs==23.1.0',\n        'more-itertools==10.1.0',\n        'packaging==23.1',\n        'pluggy==0.13.1',\n        'py==1.11.0',\n        'wcwidth==0.2.6',\n    ]\nSPECS_PYTEST['5.3']['pip_packages'] = [\n    'attrs==23.1.0',\n    'more-itertools==10.1.0',\n    'packaging==23.1',\n    'pluggy==0.13.1',\n    'py==1.11.0',\n    'wcwidth==0.2.6',\n]\nSPECS_PYTEST['5.4']['pip_packages'] = [\n    'py==1.11.0',\n    'packaging==23.1',\n    'attrs==23.1.0',\n    'more-itertools==10.1.0',\n    'pluggy==0.13.1',\n]\nSPECS_PYTEST['6.0']['pip_packages'] = [\n    'attrs==23.1.0',\n    'iniconfig==2.0.0',\n    'more-itertools==10.1.0',\n    'packaging==23.1',\n    'pluggy==0.13.1',\n    'py==1.11.0',\n    'toml==0.10.2',\n]\nfor k in ['6.2', '6.3']:\n    SPECS_PYTEST[k]['pip_packages'] = [\n        'attrs==23.1.0',\n        'iniconfig==2.0.0',\n        'packaging==23.1',\n        'pluggy==0.13.1',\n        'py==1.11.0',\n        'toml==0.10.2',\n    ]\nSPECS_PYTEST['7.0']['pip_packages'] = [\n    'attrs==23.1.0',\n    'iniconfig==2.0.0',\n    'packaging==23.1',\n    'pluggy==0.13.1',\n    'py==1.11.0',\n]\nfor k in ['7.1', '7.2']:\n    SPECS_PYTEST[k]['pip_packages'] = [\n        'attrs==23.1.0',\n        'iniconfig==2.0.0',\n        'packaging==23.1',\n        'pluggy==0.13.1',\n        'py==1.11.0',\n        'tomli==2.0.1',\n    ]\nfor k in ['7.4', '8.0', '8.1', '8.2', '8.3', '8.4']:\n    SPECS_PYTEST[k]['pip_packages'] = [\n        'iniconfig==2.0.0',\n        'packaging==23.1',\n        'pluggy==1.3.0',\n        'exceptiongroup==1.1.3',\n        'tomli==2.0.1',\n    ]\n\nSPECS_MATPLOTLIB = {\n    k: {\n        'python': '3.11',\n        'packages': 'environment.yml',\n        'install': 'python -m pip install -e .',\n        'pre_install': [\n            'apt-get -y update && apt-get -y upgrade && DEBIAN_FRONTEND=noninteractive apt-get install -y imagemagick ffmpeg texlive texlive-latex-extra texlive-fonts-recommended texlive-xetex texlive-luatex cm-super dvipng'\n        ],\n        'pip_packages': [\n            'contourpy==1.1.0',\n            'cycler==0.11.0',\n            'fonttools==4.42.1',\n            'ghostscript',\n            'kiwisolver==1.4.5',\n            'numpy==1.25.2',\n            'packaging==23.1',\n            'pillow==10.0.0',\n            'pikepdf',\n            'pyparsing==3.0.9',\n            'python-dateutil==2.8.2',\n            'six==1.16.0',\n            'setuptools==68.1.2',\n            'setuptools-scm==7.1.0',\n            'typing-extensions==4.7.1',\n        ],\n        'test_cmd': TEST_PYTEST,\n    }\n    for k in ['3.5', '3.6', '3.7', '3.8', '3.9']\n}\nSPECS_MATPLOTLIB.update(\n    {\n        k: {\n            'python': '3.8',\n            'packages': 'requirements.txt',\n            'install': 'python -m pip install -e .',\n            'pre_install': [\n                'apt-get -y update && apt-get -y upgrade && DEBIAN_FRONTEND=noninteractive apt-get install -y imagemagick ffmpeg libfreetype6-dev pkg-config texlive texlive-latex-extra texlive-fonts-recommended texlive-xetex texlive-luatex cm-super'\n            ],\n            'pip_packages': ['pytest', 'ipython'],\n            'test_cmd': TEST_PYTEST,\n        }\n        for k in ['3.1', '3.2', '3.3', '3.4']\n    }\n)\nSPECS_MATPLOTLIB.update(\n    {\n        k: {\n            'python': '3.7',\n            'packages': 'requirements.txt',\n            'install': 'python -m pip install -e .',\n            'pre_install': [\n                'apt-get -y update && apt-get -y upgrade && apt-get install -y imagemagick ffmpeg libfreetype6-dev pkg-config'\n            ],\n            'pip_packages': ['pytest'],\n            'test_cmd': TEST_PYTEST,\n        }\n        for k in ['3.0']\n    }\n)\nSPECS_MATPLOTLIB.update(\n    {\n        k: {\n            'python': '3.5',\n            'install': 'python setup.py build; python setup.py install',\n            'pre_install': [\n                'apt-get -y update && apt-get -y upgrade && && apt-get install -y imagemagick ffmpeg'\n            ],\n            'pip_packages': ['pytest'],\n            'execute_test_as_nonroot': True,\n            'test_cmd': TEST_PYTEST,\n        }\n        for k in ['2.0', '2.1', '2.2', '1.0', '1.1', '1.2', '1.3', '1.4', '1.5']\n    }\n)\nfor k in ['3.8', '3.9']:\n    SPECS_MATPLOTLIB[k]['install'] = (\n        'python -m pip install --no-build-isolation -e \".[dev]\"'\n    )\n\nSPECS_SPHINX = {\n    k: {\n        'python': '3.9',\n        'pip_packages': ['tox==4.16.0', 'tox-current-env==0.0.11'],\n        'install': 'python -m pip install -e .[test]',\n        'pre_install': [\"sed -i 's/pytest/pytest -rA/' tox.ini\"],\n        'test_cmd': TEST_SPHINX,\n    }\n    for k in ['1.5', '1.6', '1.7', '1.8', '2.0', '2.1', '2.2', '2.3', '2.4', '3.0']\n    + ['3.1', '3.2', '3.3', '3.4', '3.5', '4.0', '4.1', '4.2', '4.3', '4.4']\n    + ['4.5', '5.0', '5.1', '5.2', '5.3', '6.0', '6.2', '7.0', '7.1', '7.2']\n    + ['7.3', '7.4', '8.0', '8.1']\n}\nfor k in ['3.0', '3.1', '3.2', '3.3', '3.4', '3.5', '4.0', '4.1', '4.2', '4.3', '4.4']:\n    SPECS_SPHINX[k]['pre_install'].extend(\n        [\n            \"sed -i 's/Jinja2>=2.3/Jinja2<3.0/' setup.py\",\n            \"sed -i 's/sphinxcontrib-applehelp/sphinxcontrib-applehelp<=1.0.7/' setup.py\",\n            \"sed -i 's/sphinxcontrib-devhelp/sphinxcontrib-devhelp<=1.0.5/' setup.py\",\n            \"sed -i 's/sphinxcontrib-qthelp/sphinxcontrib-qthelp<=1.0.6/' setup.py\",\n            \"sed -i 's/alabaster>=0.7,<0.8/alabaster>=0.7,<0.7.12/' setup.py\",\n            \"sed -i \\\"s/'packaging',/'packaging', 'markupsafe<=2.0.1',/\\\" setup.py\",\n        ]\n    )\n    if k in ['4.2', '4.3', '4.4']:\n        SPECS_SPHINX[k]['pre_install'].extend(\n            [\n                \"sed -i 's/sphinxcontrib-htmlhelp>=2.0.0/sphinxcontrib-htmlhelp>=2.0.0,<=2.0.4/' setup.py\",\n                \"sed -i 's/sphinxcontrib-serializinghtml>=1.1.5/sphinxcontrib-serializinghtml>=1.1.5,<=1.1.9/' setup.py\",\n            ]\n        )\n    elif k == '4.1':\n        SPECS_SPHINX[k]['pre_install'].extend(\n            [\n                (\n                    \"grep -q 'sphinxcontrib-htmlhelp>=2.0.0' setup.py && \"\n                    \"sed -i 's/sphinxcontrib-htmlhelp>=2.0.0/sphinxcontrib-htmlhelp>=2.0.0,<=2.0.4/' setup.py || \"\n                    \"sed -i 's/sphinxcontrib-htmlhelp/sphinxcontrib-htmlhelp<=2.0.4/' setup.py\"\n                ),\n                (\n                    \"grep -q 'sphinxcontrib-serializinghtml>=1.1.5' setup.py && \"\n                    \"sed -i 's/sphinxcontrib-serializinghtml>=1.1.5/sphinxcontrib-serializinghtml>=1.1.5,<=1.1.9/' setup.py || \"\n                    \"sed -i 's/sphinxcontrib-serializinghtml/sphinxcontrib-serializinghtml<=1.1.9/' setup.py\"\n                ),\n            ]\n        )\n    else:\n        SPECS_SPHINX[k]['pre_install'].extend(\n            [\n                \"sed -i 's/sphinxcontrib-htmlhelp/sphinxcontrib-htmlhelp<=2.0.4/' setup.py\",\n                \"sed -i 's/sphinxcontrib-serializinghtml/sphinxcontrib-serializinghtml<=1.1.9/' setup.py\",\n            ]\n        )\nfor k in ['7.2', '7.3', '7.4', '8.0', '8.1']:\n    SPECS_SPHINX[k]['pre_install'] += ['apt-get update && apt-get install -y graphviz']\nfor k in ['8.0', '8.1']:\n    SPECS_SPHINX[k]['python'] = '3.10'\n\nSPECS_ASTROPY = {\n    k: {\n        'python': '3.9',\n        'install': 'python -m pip install -e .[test] --verbose',\n        'pip_packages': [\n            'attrs==23.1.0',\n            'exceptiongroup==1.1.3',\n            'execnet==2.0.2',\n            'hypothesis==6.82.6',\n            'iniconfig==2.0.0',\n            'numpy==1.25.2',\n            'packaging==23.1',\n            'pluggy==1.3.0',\n            'psutil==5.9.5',\n            'pyerfa==2.0.0.3',\n            'pytest-arraydiff==0.5.0',\n            'pytest-astropy-header==0.2.2',\n            'pytest-astropy==0.10.0',\n            'pytest-cov==4.1.0',\n            'pytest-doctestplus==1.0.0',\n            'pytest-filter-subpackage==0.1.2',\n            'pytest-mock==3.11.1',\n            'pytest-openfiles==0.5.0',\n            'pytest-remotedata==0.4.0',\n            'pytest-xdist==3.3.1',\n            'pytest==7.4.0',\n            'PyYAML==6.0.1',\n            'setuptools==68.0.0',\n            'sortedcontainers==2.4.0',\n            'tomli==2.0.1',\n        ],\n        'test_cmd': TEST_PYTEST,\n    }\n    for k in ['3.0', '3.1', '3.2', '4.1', '4.2', '4.3', '5.0', '5.1', '5.2', 'v5.3']\n}\nSPECS_ASTROPY.update(\n    {\n        k: {\n            'python': '3.6',\n            'install': 'python -m pip install -e .[test] --verbose',\n            'packages': 'setuptools==38.2.4',\n            'pip_packages': [\n                'attrs==17.3.0',\n                'exceptiongroup==0.0.0a0',\n                'execnet==1.5.0',\n                'hypothesis==3.44.2',\n                'cython==0.27.3',\n                'jinja2==2.10',\n                'MarkupSafe==1.0',\n                'numpy==1.16.0',\n                'packaging==16.8',\n                'pluggy==0.6.0',\n                'psutil==5.4.2',\n                'pyerfa==1.7.0',\n                'pytest-arraydiff==0.1',\n                'pytest-astropy-header==0.1',\n                'pytest-astropy==0.2.1',\n                'pytest-cov==2.5.1',\n                'pytest-doctestplus==0.1.2',\n                'pytest-filter-subpackage==0.1',\n                'pytest-forked==0.2',\n                'pytest-mock==1.6.3',\n                'pytest-openfiles==0.2.0',\n                'pytest-remotedata==0.2.0',\n                'pytest-xdist==1.20.1',\n                'pytest==3.3.1',\n                'PyYAML==3.12',\n                'sortedcontainers==1.5.9',\n                'tomli==0.2.0',\n            ],\n            'test_cmd': TEST_ASTROPY_PYTEST,\n        }\n        for k in ['0.1', '0.2', '0.3', '0.4', '1.1', '1.2', '1.3']\n    }\n)\nfor k in ['4.1', '4.2', '4.3', '5.0', '5.1', '5.2', 'v5.3']:\n    SPECS_ASTROPY[k]['pre_install'] = [\n        'sed -i \\'s/requires = \\\\[\"setuptools\",/requires = \\\\[\"setuptools==68.0.0\",/\\' pyproject.toml'\n    ]\nfor k in ['v5.3']:\n    SPECS_ASTROPY[k]['python'] = '3.10'\n\nSPECS_SYMPY = {\n    k: {\n        'python': '3.9',\n        'packages': 'mpmath flake8',\n        'pip_packages': ['mpmath==1.3.0', 'flake8-comprehensions'],\n        'install': 'python -m pip install -e .',\n        'test_cmd': TEST_SYMPY,\n    }\n    for k in ['0.7', '1.0', '1.1', '1.10', '1.11', '1.12', '1.2', '1.4', '1.5', '1.6']\n    + ['1.7', '1.8', '1.9']\n    + ['1.10', '1.11', '1.12', '1.13', '1.14']\n}\nSPECS_SYMPY.update(\n    {\n        k: {\n            'python': '3.9',\n            'packages': 'requirements.txt',\n            'install': 'python -m pip install -e .',\n            'pip_packages': ['mpmath==1.3.0'],\n            'test_cmd': TEST_SYMPY,\n        }\n        for k in ['1.13', '1.14']\n    }\n)\n\nSPECS_PYLINT = {\n    k: {\n        'python': '3.9',\n        'packages': 'requirements.txt',\n        'install': 'python -m pip install -e .',\n        'test_cmd': TEST_PYTEST,\n    }\n    for k in [\n        '2.10',\n        '2.11',\n        '2.13',\n        '2.14',\n        '2.15',\n        '2.16',\n        '2.17',\n        '2.8',\n        '2.9',\n        '3.0',\n        '3.1',\n        '3.2',\n        '3.3',\n        '4.0',\n    ]\n}\nSPECS_PYLINT['2.8']['pip_packages'] = ['pyenchant==3.2']\nSPECS_PYLINT['2.8']['pre_install'] = [\n    'apt-get update && apt-get install -y libenchant-2-dev hunspell-en-us'\n]\nSPECS_PYLINT.update(\n    {\n        k: {\n            **SPECS_PYLINT[k],\n            'pip_packages': ['astroid==3.0.0a6', 'setuptools'],\n        }\n        for k in ['3.0', '3.1', '3.2', '3.3', '4.0']\n    }\n)\nfor v in ['2.14', '2.15', '2.17', '3.0', '3.1', '3.2', '3.3', '4.0']:\n    SPECS_PYLINT[v]['nano_cpus'] = int(2e9)\n\nSPECS_XARRAY = {\n    k: {\n        'python': '3.10',\n        'packages': 'environment.yml',\n        'install': 'python -m pip install -e .',\n        'pip_packages': [\n            'numpy==1.23.0',\n            'packaging==23.1',\n            'pandas==1.5.3',\n            'pytest==7.4.0',\n            'python-dateutil==2.8.2',\n            'pytz==2023.3',\n            'six==1.16.0',\n            'scipy==1.11.1',\n            'setuptools==68.0.0',\n            'dask==2022.8.1',\n        ],\n        'no_use_env': True,\n        'test_cmd': TEST_PYTEST,\n    }\n    for k in [\n        '0.12',\n        '0.18',\n        '0.19',\n        '0.20',\n        '2022.03',\n        '2022.06',\n        '2022.09',\n        '2023.07',\n        '2024.05',\n    ]\n}\n\nSPECS_SQLFLUFF = {\n    k: {\n        'python': '3.9',\n        'packages': 'requirements.txt',\n        'install': 'python -m pip install -e .',\n        'test_cmd': TEST_PYTEST,\n    }\n    for k in [\n        '0.10',\n        '0.11',\n        '0.12',\n        '0.13',\n        '0.4',\n        '0.5',\n        '0.6',\n        '0.8',\n        '0.9',\n        '1.0',\n        '1.1',\n        '1.2',\n        '1.3',\n        '1.4',\n        '2.0',\n        '2.1',\n        '2.2',\n    ]\n}\n\nSPECS_DBT_CORE = {\n    k: {\n        'python': '3.9',\n        'packages': 'requirements.txt',\n        'install': 'python -m pip install -e .',\n    }\n    for k in [\n        '0.13',\n        '0.14',\n        '0.15',\n        '0.16',\n        '0.17',\n        '0.18',\n        '0.19',\n        '0.20',\n        '0.21',\n        '1.0',\n        '1.1',\n        '1.2',\n        '1.3',\n        '1.4',\n        '1.5',\n        '1.6',\n        '1.7',\n    ]\n}\n\nSPECS_PYVISTA = {\n    k: {\n        'python': '3.9',\n        'install': 'python -m pip install -e .',\n        'pip_packages': ['pytest'],\n        'test_cmd': TEST_PYTEST,\n    }\n    for k in ['0.20', '0.21', '0.22', '0.23']\n}\nSPECS_PYVISTA.update(\n    {\n        k: {\n            'python': '3.9',\n            'packages': 'requirements.txt',\n            'install': 'python -m pip install -e .',\n            'pip_packages': ['pytest'],\n            'test_cmd': TEST_PYTEST,\n        }\n        for k in [\n            '0.24',\n            '0.25',\n            '0.26',\n            '0.27',\n            '0.28',\n            '0.29',\n            '0.30',\n            '0.31',\n            '0.32',\n            '0.33',\n            '0.34',\n            '0.35',\n            '0.36',\n            '0.37',\n            '0.38',\n            '0.39',\n            '0.40',\n            '0.41',\n            '0.42',\n            '0.43',\n        ]\n    }\n)\n\nSPECS_ASTROID = {\n    k: {\n        'python': '3.9',\n        'install': 'python -m pip install -e .',\n        'pip_packages': ['pytest'],\n        'test_cmd': TEST_PYTEST,\n    }\n    for k in [\n        '2.10',\n        '2.12',\n        '2.13',\n        '2.14',\n        '2.15',\n        '2.16',\n        '2.5',\n        '2.6',\n        '2.7',\n        '2.8',\n        '2.9',\n        '3.0',\n    ]\n}\n\nSPECS_MARSHMALLOW = {\n    k: {\n        'python': '3.9',\n        'install': \"python -m pip install -e '.[dev]'\",\n        'test_cmd': TEST_PYTEST,\n    }\n    for k in [\n        '2.18',\n        '2.19',\n        '2.20',\n        '3.0',\n        '3.1',\n        '3.10',\n        '3.11',\n        '3.12',\n        '3.13',\n        '3.15',\n        '3.16',\n        '3.19',\n        '3.2',\n        '3.4',\n        '3.8',\n        '3.9',\n    ]\n}\n\nSPECS_PVLIB = {\n    k: {\n        'python': '3.9',\n        'install': 'python -m pip install -e .[all]',\n        'packages': 'pandas scipy',\n        'pip_packages': ['jupyter', 'ipython', 'matplotlib', 'pytest', 'flake8'],\n        'test_cmd': TEST_PYTEST,\n    }\n    for k in ['0.1', '0.2', '0.3', '0.4', '0.5', '0.6', '0.7', '0.8', '0.9']\n}\n\nSPECS_PYDICOM = {\n    k: {\n        'python': '3.6',\n        'install': 'python -m pip install -e .',\n        'packages': 'numpy',\n        'pip_packages': ['pytest'],\n        'test_cmd': TEST_PYTEST,\n    }\n    for k in [\n        '1.0',\n        '1.1',\n        '1.2',\n        '1.3',\n        '1.4',\n        '2.0',\n        '2.1',\n        '2.2',\n        '2.3',\n        '2.4',\n        '3.0',\n    ]\n}\nSPECS_PYDICOM.update({k: {**SPECS_PYDICOM[k], 'python': '3.8'} for k in ['1.4', '2.0']})\nSPECS_PYDICOM.update({k: {**SPECS_PYDICOM[k], 'python': '3.9'} for k in ['2.1', '2.2']})\nSPECS_PYDICOM.update({k: {**SPECS_PYDICOM[k], 'python': '3.10'} for k in ['2.3']})\nSPECS_PYDICOM.update(\n    {k: {**SPECS_PYDICOM[k], 'python': '3.11'} for k in ['2.4', '3.0']}\n)\n\nSPECS_HUMANEVAL = {k: {'python': '3.9', 'test_cmd': 'python'} for k in ['1.0']}\n\n# Constants - Task Instance Instllation Environment\nMAP_REPO_VERSION_TO_SPECS: dict[str, dict[str, Any]] = {\n    'astropy/astropy': SPECS_ASTROPY,\n    'dbt-labs/dbt-core': SPECS_DBT_CORE,\n    'django/django': SPECS_DJANGO,\n    'matplotlib/matplotlib': SPECS_MATPLOTLIB,\n    'marshmallow-code/marshmallow': SPECS_MARSHMALLOW,\n    'mwaskom/seaborn': SPECS_SEABORN,\n    'pallets/flask': SPECS_FLASK,\n    'psf/requests': SPECS_REQUESTS,\n    'pvlib/pvlib-python': SPECS_PVLIB,\n    'pydata/xarray': SPECS_XARRAY,\n    'pydicom/pydicom': SPECS_PYDICOM,\n    'pylint-dev/astroid': SPECS_ASTROID,\n    'pylint-dev/pylint': SPECS_PYLINT,\n    'pytest-dev/pytest': SPECS_PYTEST,\n    'pyvista/pyvista': SPECS_PYVISTA,\n    'scikit-learn/scikit-learn': SPECS_SKLEARN,\n    'sphinx-doc/sphinx': SPECS_SPHINX,\n    'sqlfluff/sqlfluff': SPECS_SQLFLUFF,\n    'swe-bench/humaneval': SPECS_HUMANEVAL,\n    'sympy/sympy': SPECS_SYMPY,\n}\n\n# Constants - Repository Specific Installation Instructions\nMAP_REPO_TO_INSTALL = {}\n\n\n# Constants - Task Instance Requirements File Paths\nMAP_REPO_TO_REQS_PATHS = {\n    'dbt-labs/dbt-core': ['dev-requirements.txt', 'dev_requirements.txt'],\n    'django/django': ['tests/requirements/py3.txt'],\n    'matplotlib/matplotlib': [\n        'requirements/dev/dev-requirements.txt',\n        'requirements/testing/travis_all.txt',\n    ],\n    'pallets/flask': ['requirements/dev.txt'],\n    'pylint-dev/pylint': ['requirements_test.txt'],\n    'pyvista/pyvista': ['requirements_test.txt', 'requirements.txt'],\n    'sqlfluff/sqlfluff': ['requirements_dev.txt'],\n    'sympy/sympy': ['requirements-dev.txt', 'requirements-test.txt'],\n}\n\n\n# Constants - Task Instance environment.yml File Paths\nMAP_REPO_TO_ENV_YML_PATHS = {\n    'matplotlib/matplotlib': ['environment.yml'],\n    'pydata/xarray': ['ci/requirements/environment.yml', 'environment.yml'],\n}\n\n# Constants - Evaluation Keys\nKEY_INSTANCE_ID = 'instance_id'\nKEY_MODEL = 'model_name_or_path'\nKEY_PREDICTION = 'model_patch'\n\n\n# Constants - Logging\nAPPLY_PATCH_FAIL = '>>>>> Patch Apply Failed'\nAPPLY_PATCH_PASS = '>>>>> Applied Patch'\nINSTALL_FAIL = '>>>>> Init Failed'\nINSTALL_PASS = '>>>>> Init Succeeded'\nINSTALL_TIMEOUT = '>>>>> Init Timed Out'\nRESET_FAILED = '>>>>> Reset Failed'\nTESTS_ERROR = '>>>>> Tests Errored'\nTESTS_FAILED = '>>>>> Some Tests Failed'\nTESTS_PASSED = '>>>>> All Tests Passed'\nTESTS_TIMEOUT = '>>>>> Tests Timed Out'\nCOVERAGE_PREFIX = '>>>>> Coverage'\nTESTS_SUFFIX = '>>>>> Tests Finished'\n\n\n# Constants - Patch Types\nclass PatchType(Enum):\n    PATCH_GOLD = 'gold'\n    PATCH_PRED = 'pred'\n    PATCH_PRED_TRY = 'pred_try'\n    PATCH_PRED_MINIMAL = 'pred_minimal'\n    PATCH_PRED_MINIMAL_TRY = 'pred_minimal_try'\n    PATCH_TEST = 'test'\n\n    def __str__(self):\n        return self.value\n\n\n# Constants - Miscellaneous\nNON_TEST_EXTS = [\n    '.json',\n    '.png',\n    'csv',\n    '.txt',\n    '.md',\n    '.jpg',\n    '.jpeg',\n    '.pkl',\n    '.yml',\n    '.yaml',\n    '.toml',\n]\nSWE_BENCH_URL_RAW = 'https://raw.githubusercontent.com/'\nUSE_X86 = {\n    'astropy__astropy-7973',\n    'django__django-10087',\n    'django__django-10097',\n    'django__django-10213',\n    'django__django-10301',\n    'django__django-10316',\n    'django__django-10426',\n    'django__django-11383',\n    'django__django-12185',\n    'django__django-12497',\n    'django__django-13121',\n    'django__django-13417',\n    'django__django-13431',\n    'django__django-13447',\n    'django__django-14155',\n    'django__django-14164',\n    'django__django-14169',\n    'django__django-14170',\n    'django__django-15180',\n    'django__django-15199',\n    'django__django-15280',\n    'django__django-15292',\n    'django__django-15474',\n    'django__django-15682',\n    'django__django-15689',\n    'django__django-15695',\n    'django__django-15698',\n    'django__django-15781',\n    'django__django-15925',\n    'django__django-15930',\n    'django__django-5158',\n    'django__django-5470',\n    'django__django-7188',\n    'django__django-7475',\n    'django__django-7530',\n    'django__django-8326',\n    'django__django-8961',\n    'django__django-9003',\n    'django__django-9703',\n    'django__django-9871',\n    'matplotlib__matplotlib-13983',\n    'matplotlib__matplotlib-13984',\n    'matplotlib__matplotlib-13989',\n    'matplotlib__matplotlib-14043',\n    'matplotlib__matplotlib-14471',\n    'matplotlib__matplotlib-22711',\n    'matplotlib__matplotlib-22719',\n    'matplotlib__matplotlib-22734',\n    'matplotlib__matplotlib-22767',\n    'matplotlib__matplotlib-22815',\n    'matplotlib__matplotlib-22835',\n    'matplotlib__matplotlib-22865',\n    'matplotlib__matplotlib-22871',\n    'matplotlib__matplotlib-22883',\n    'matplotlib__matplotlib-22926',\n    'matplotlib__matplotlib-22929',\n    'matplotlib__matplotlib-22931',\n    'matplotlib__matplotlib-22945',\n    'matplotlib__matplotlib-22991',\n    'matplotlib__matplotlib-23031',\n    'matplotlib__matplotlib-23047',\n    'matplotlib__matplotlib-23049',\n    'matplotlib__matplotlib-23057',\n    'matplotlib__matplotlib-23088',\n    'matplotlib__matplotlib-23111',\n    'matplotlib__matplotlib-23140',\n    'matplotlib__matplotlib-23174',\n    'matplotlib__matplotlib-23188',\n    'matplotlib__matplotlib-23198',\n    'matplotlib__matplotlib-23203',\n    'matplotlib__matplotlib-23266',\n    'matplotlib__matplotlib-23267',\n    'matplotlib__matplotlib-23288',\n    'matplotlib__matplotlib-23299',\n    'matplotlib__matplotlib-23314',\n    'matplotlib__matplotlib-23332',\n    'matplotlib__matplotlib-23348',\n    'matplotlib__matplotlib-23412',\n    'matplotlib__matplotlib-23476',\n    'matplotlib__matplotlib-23516',\n    'matplotlib__matplotlib-23562',\n    'matplotlib__matplotlib-23563',\n    'matplotlib__matplotlib-23573',\n    'matplotlib__matplotlib-23740',\n    'matplotlib__matplotlib-23742',\n    'matplotlib__matplotlib-23913',\n    'matplotlib__matplotlib-23964',\n    'matplotlib__matplotlib-23987',\n    'matplotlib__matplotlib-24013',\n    'matplotlib__matplotlib-24026',\n    'matplotlib__matplotlib-24088',\n    'matplotlib__matplotlib-24111',\n    'matplotlib__matplotlib-24149',\n    'matplotlib__matplotlib-24177',\n    'matplotlib__matplotlib-24189',\n    'matplotlib__matplotlib-24224',\n    'matplotlib__matplotlib-24250',\n    'matplotlib__matplotlib-24257',\n    'matplotlib__matplotlib-24265',\n    'matplotlib__matplotlib-24334',\n    'matplotlib__matplotlib-24362',\n    'matplotlib__matplotlib-24403',\n    'matplotlib__matplotlib-24431',\n    'matplotlib__matplotlib-24538',\n    'matplotlib__matplotlib-24570',\n    'matplotlib__matplotlib-24604',\n    'matplotlib__matplotlib-24619',\n    'matplotlib__matplotlib-24627',\n    'matplotlib__matplotlib-24637',\n    'matplotlib__matplotlib-24691',\n    'matplotlib__matplotlib-24749',\n    'matplotlib__matplotlib-24768',\n    'matplotlib__matplotlib-24849',\n    'matplotlib__matplotlib-24870',\n    'matplotlib__matplotlib-24912',\n    'matplotlib__matplotlib-24924',\n    'matplotlib__matplotlib-24970',\n    'matplotlib__matplotlib-24971',\n    'matplotlib__matplotlib-25027',\n    'matplotlib__matplotlib-25052',\n    'matplotlib__matplotlib-25079',\n    'matplotlib__matplotlib-25085',\n    'matplotlib__matplotlib-25122',\n    'matplotlib__matplotlib-25126',\n    'matplotlib__matplotlib-25129',\n    'matplotlib__matplotlib-25238',\n    'matplotlib__matplotlib-25281',\n    'matplotlib__matplotlib-25287',\n    'matplotlib__matplotlib-25311',\n    'matplotlib__matplotlib-25332',\n    'matplotlib__matplotlib-25334',\n    'matplotlib__matplotlib-25340',\n    'matplotlib__matplotlib-25346',\n    'matplotlib__matplotlib-25404',\n    'matplotlib__matplotlib-25405',\n    'matplotlib__matplotlib-25425',\n    'matplotlib__matplotlib-25430',\n    'matplotlib__matplotlib-25433',\n    'matplotlib__matplotlib-25442',\n    'matplotlib__matplotlib-25479',\n    'matplotlib__matplotlib-25498',\n    'matplotlib__matplotlib-25499',\n    'matplotlib__matplotlib-25515',\n    'matplotlib__matplotlib-25547',\n    'matplotlib__matplotlib-25551',\n    'matplotlib__matplotlib-25565',\n    'matplotlib__matplotlib-25624',\n    'matplotlib__matplotlib-25631',\n    'matplotlib__matplotlib-25640',\n    'matplotlib__matplotlib-25651',\n    'matplotlib__matplotlib-25667',\n    'matplotlib__matplotlib-25712',\n    'matplotlib__matplotlib-25746',\n    'matplotlib__matplotlib-25772',\n    'matplotlib__matplotlib-25775',\n    'matplotlib__matplotlib-25779',\n    'matplotlib__matplotlib-25785',\n    'matplotlib__matplotlib-25794',\n    'matplotlib__matplotlib-25859',\n    'matplotlib__matplotlib-25960',\n    'matplotlib__matplotlib-26011',\n    'matplotlib__matplotlib-26020',\n    'matplotlib__matplotlib-26024',\n    'matplotlib__matplotlib-26078',\n    'matplotlib__matplotlib-26089',\n    'matplotlib__matplotlib-26101',\n    'matplotlib__matplotlib-26113',\n    'matplotlib__matplotlib-26122',\n    'matplotlib__matplotlib-26160',\n    'matplotlib__matplotlib-26184',\n    'matplotlib__matplotlib-26208',\n    'matplotlib__matplotlib-26223',\n    'matplotlib__matplotlib-26232',\n    'matplotlib__matplotlib-26249',\n    'matplotlib__matplotlib-26278',\n    'matplotlib__matplotlib-26285',\n    'matplotlib__matplotlib-26291',\n    'matplotlib__matplotlib-26300',\n    'matplotlib__matplotlib-26311',\n    'matplotlib__matplotlib-26341',\n    'matplotlib__matplotlib-26342',\n    'matplotlib__matplotlib-26399',\n    'matplotlib__matplotlib-26466',\n    'matplotlib__matplotlib-26469',\n    'matplotlib__matplotlib-26472',\n    'matplotlib__matplotlib-26479',\n    'matplotlib__matplotlib-26532',\n    'pydata__xarray-2905',\n    'pydata__xarray-2922',\n    'pydata__xarray-3095',\n    'pydata__xarray-3114',\n    'pydata__xarray-3151',\n    'pydata__xarray-3156',\n    'pydata__xarray-3159',\n    'pydata__xarray-3239',\n    'pydata__xarray-3302',\n    'pydata__xarray-3305',\n    'pydata__xarray-3338',\n    'pydata__xarray-3364',\n    'pydata__xarray-3406',\n    'pydata__xarray-3520',\n    'pydata__xarray-3527',\n    'pydata__xarray-3631',\n    'pydata__xarray-3635',\n    'pydata__xarray-3637',\n    'pydata__xarray-3649',\n    'pydata__xarray-3677',\n    'pydata__xarray-3733',\n    'pydata__xarray-3812',\n    'pydata__xarray-3905',\n    'pydata__xarray-3976',\n    'pydata__xarray-3979',\n    'pydata__xarray-3993',\n    'pydata__xarray-4075',\n    'pydata__xarray-4094',\n    'pydata__xarray-4098',\n    'pydata__xarray-4182',\n    'pydata__xarray-4184',\n    'pydata__xarray-4248',\n    'pydata__xarray-4339',\n    'pydata__xarray-4356',\n    'pydata__xarray-4419',\n    'pydata__xarray-4423',\n    'pydata__xarray-4442',\n    'pydata__xarray-4493',\n    'pydata__xarray-4510',\n    'pydata__xarray-4629',\n    'pydata__xarray-4683',\n    'pydata__xarray-4684',\n    'pydata__xarray-4687',\n    'pydata__xarray-4695',\n    'pydata__xarray-4750',\n    'pydata__xarray-4758',\n    'pydata__xarray-4759',\n    'pydata__xarray-4767',\n    'pydata__xarray-4802',\n    'pydata__xarray-4819',\n    'pydata__xarray-4827',\n    'pydata__xarray-4879',\n    'pydata__xarray-4911',\n    'pydata__xarray-4939',\n    'pydata__xarray-4940',\n    'pydata__xarray-4966',\n    'pydata__xarray-4994',\n    'pydata__xarray-5033',\n    'pydata__xarray-5126',\n    'pydata__xarray-5131',\n    'pydata__xarray-5180',\n    'pydata__xarray-5187',\n    'pydata__xarray-5233',\n    'pydata__xarray-5362',\n    'pydata__xarray-5365',\n    'pydata__xarray-5455',\n    'pydata__xarray-5580',\n    'pydata__xarray-5662',\n    'pydata__xarray-5682',\n    'pydata__xarray-5731',\n    'pydata__xarray-6135',\n    'pydata__xarray-6386',\n    'pydata__xarray-6394',\n    'pydata__xarray-6400',\n    'pydata__xarray-6461',\n    'pydata__xarray-6548',\n    'pydata__xarray-6598',\n    'pydata__xarray-6599',\n    'pydata__xarray-6601',\n    'pydata__xarray-6721',\n    'pydata__xarray-6744',\n    'pydata__xarray-6798',\n    'pydata__xarray-6804',\n    'pydata__xarray-6823',\n    'pydata__xarray-6857',\n    'pydata__xarray-6882',\n    'pydata__xarray-6889',\n    'pydata__xarray-6938',\n    'pydata__xarray-6971',\n    'pydata__xarray-6992',\n    'pydata__xarray-6999',\n    'pydata__xarray-7003',\n    'pydata__xarray-7019',\n    'pydata__xarray-7052',\n    'pydata__xarray-7089',\n    'pydata__xarray-7101',\n    'pydata__xarray-7105',\n    'pydata__xarray-7112',\n    'pydata__xarray-7120',\n    'pydata__xarray-7147',\n    'pydata__xarray-7150',\n    'pydata__xarray-7179',\n    'pydata__xarray-7203',\n    'pydata__xarray-7229',\n    'pydata__xarray-7233',\n    'pydata__xarray-7347',\n    'pydata__xarray-7391',\n    'pydata__xarray-7393',\n    'pydata__xarray-7400',\n    'pydata__xarray-7444',\n    'pytest-dev__pytest-10482',\n    'scikit-learn__scikit-learn-10198',\n    'scikit-learn__scikit-learn-10297',\n    'scikit-learn__scikit-learn-10306',\n    'scikit-learn__scikit-learn-10331',\n    'scikit-learn__scikit-learn-10377',\n    'scikit-learn__scikit-learn-10382',\n    'scikit-learn__scikit-learn-10397',\n    'scikit-learn__scikit-learn-10427',\n    'scikit-learn__scikit-learn-10428',\n    'scikit-learn__scikit-learn-10443',\n    'scikit-learn__scikit-learn-10452',\n    'scikit-learn__scikit-learn-10459',\n    'scikit-learn__scikit-learn-10471',\n    'scikit-learn__scikit-learn-10483',\n    'scikit-learn__scikit-learn-10495',\n    'scikit-learn__scikit-learn-10508',\n    'scikit-learn__scikit-learn-10558',\n    'scikit-learn__scikit-learn-10577',\n    'scikit-learn__scikit-learn-10581',\n    'scikit-learn__scikit-learn-10687',\n    'scikit-learn__scikit-learn-10774',\n    'scikit-learn__scikit-learn-10777',\n    'scikit-learn__scikit-learn-10803',\n    'scikit-learn__scikit-learn-10844',\n    'scikit-learn__scikit-learn-10870',\n    'scikit-learn__scikit-learn-10881',\n    'scikit-learn__scikit-learn-10899',\n    'scikit-learn__scikit-learn-10908',\n    'scikit-learn__scikit-learn-10913',\n    'scikit-learn__scikit-learn-10949',\n    'scikit-learn__scikit-learn-10982',\n    'scikit-learn__scikit-learn-10986',\n    'scikit-learn__scikit-learn-11040',\n    'scikit-learn__scikit-learn-11042',\n    'scikit-learn__scikit-learn-11043',\n    'scikit-learn__scikit-learn-11151',\n    'scikit-learn__scikit-learn-11160',\n    'scikit-learn__scikit-learn-11206',\n    'scikit-learn__scikit-learn-11235',\n    'scikit-learn__scikit-learn-11243',\n    'scikit-learn__scikit-learn-11264',\n    'scikit-learn__scikit-learn-11281',\n    'scikit-learn__scikit-learn-11310',\n    'scikit-learn__scikit-learn-11315',\n    'scikit-learn__scikit-learn-11333',\n    'scikit-learn__scikit-learn-11346',\n    'scikit-learn__scikit-learn-11391',\n    'scikit-learn__scikit-learn-11496',\n    'scikit-learn__scikit-learn-11542',\n    'scikit-learn__scikit-learn-11574',\n    'scikit-learn__scikit-learn-11578',\n    'scikit-learn__scikit-learn-11585',\n    'scikit-learn__scikit-learn-11596',\n    'scikit-learn__scikit-learn-11635',\n    'scikit-learn__scikit-learn-12258',\n    'scikit-learn__scikit-learn-12421',\n    'scikit-learn__scikit-learn-12443',\n    'scikit-learn__scikit-learn-12462',\n    'scikit-learn__scikit-learn-12471',\n    'scikit-learn__scikit-learn-12486',\n    'scikit-learn__scikit-learn-12557',\n    'scikit-learn__scikit-learn-12583',\n    'scikit-learn__scikit-learn-12585',\n    'scikit-learn__scikit-learn-12625',\n    'scikit-learn__scikit-learn-12626',\n    'scikit-learn__scikit-learn-12656',\n    'scikit-learn__scikit-learn-12682',\n    'scikit-learn__scikit-learn-12704',\n    'scikit-learn__scikit-learn-12733',\n    'scikit-learn__scikit-learn-12758',\n    'scikit-learn__scikit-learn-12760',\n    'scikit-learn__scikit-learn-12784',\n    'scikit-learn__scikit-learn-12827',\n    'scikit-learn__scikit-learn-12834',\n    'scikit-learn__scikit-learn-12860',\n    'scikit-learn__scikit-learn-12908',\n    'scikit-learn__scikit-learn-12938',\n    'scikit-learn__scikit-learn-12961',\n    'scikit-learn__scikit-learn-12973',\n    'scikit-learn__scikit-learn-12983',\n    'scikit-learn__scikit-learn-12989',\n    'scikit-learn__scikit-learn-13010',\n    'scikit-learn__scikit-learn-13013',\n    'scikit-learn__scikit-learn-13017',\n    'scikit-learn__scikit-learn-13046',\n    'scikit-learn__scikit-learn-13087',\n    'scikit-learn__scikit-learn-13124',\n    'scikit-learn__scikit-learn-13135',\n    'scikit-learn__scikit-learn-13142',\n    'scikit-learn__scikit-learn-13143',\n    'scikit-learn__scikit-learn-13157',\n    'scikit-learn__scikit-learn-13165',\n    'scikit-learn__scikit-learn-13174',\n    'scikit-learn__scikit-learn-13221',\n    'scikit-learn__scikit-learn-13241',\n    'scikit-learn__scikit-learn-13253',\n    'scikit-learn__scikit-learn-13280',\n    'scikit-learn__scikit-learn-13283',\n    'scikit-learn__scikit-learn-13302',\n    'scikit-learn__scikit-learn-13313',\n    'scikit-learn__scikit-learn-13328',\n    'scikit-learn__scikit-learn-13333',\n    'scikit-learn__scikit-learn-13363',\n    'scikit-learn__scikit-learn-13368',\n    'scikit-learn__scikit-learn-13392',\n    'scikit-learn__scikit-learn-13436',\n    'scikit-learn__scikit-learn-13439',\n    'scikit-learn__scikit-learn-13447',\n    'scikit-learn__scikit-learn-13454',\n    'scikit-learn__scikit-learn-13467',\n    'scikit-learn__scikit-learn-13472',\n    'scikit-learn__scikit-learn-13485',\n    'scikit-learn__scikit-learn-13496',\n    'scikit-learn__scikit-learn-13497',\n    'scikit-learn__scikit-learn-13536',\n    'scikit-learn__scikit-learn-13549',\n    'scikit-learn__scikit-learn-13554',\n    'scikit-learn__scikit-learn-13584',\n    'scikit-learn__scikit-learn-13618',\n    'scikit-learn__scikit-learn-13620',\n    'scikit-learn__scikit-learn-13628',\n    'scikit-learn__scikit-learn-13641',\n    'scikit-learn__scikit-learn-13704',\n    'scikit-learn__scikit-learn-13726',\n    'scikit-learn__scikit-learn-13779',\n    'scikit-learn__scikit-learn-13780',\n    'scikit-learn__scikit-learn-13828',\n    'scikit-learn__scikit-learn-13864',\n    'scikit-learn__scikit-learn-13877',\n    'scikit-learn__scikit-learn-13910',\n    'scikit-learn__scikit-learn-13915',\n    'scikit-learn__scikit-learn-13933',\n    'scikit-learn__scikit-learn-13960',\n    'scikit-learn__scikit-learn-13974',\n    'scikit-learn__scikit-learn-13983',\n    'scikit-learn__scikit-learn-14012',\n    'scikit-learn__scikit-learn-14024',\n    'scikit-learn__scikit-learn-14053',\n    'scikit-learn__scikit-learn-14067',\n    'scikit-learn__scikit-learn-14087',\n    'scikit-learn__scikit-learn-14092',\n    'scikit-learn__scikit-learn-14114',\n    'scikit-learn__scikit-learn-14125',\n    'scikit-learn__scikit-learn-14141',\n    'scikit-learn__scikit-learn-14237',\n    'scikit-learn__scikit-learn-14309',\n    'scikit-learn__scikit-learn-14430',\n    'scikit-learn__scikit-learn-14450',\n    'scikit-learn__scikit-learn-14458',\n    'scikit-learn__scikit-learn-14464',\n    'scikit-learn__scikit-learn-14496',\n    'scikit-learn__scikit-learn-14520',\n    'scikit-learn__scikit-learn-14544',\n    'scikit-learn__scikit-learn-14591',\n    'scikit-learn__scikit-learn-14629',\n    'scikit-learn__scikit-learn-14704',\n    'scikit-learn__scikit-learn-14706',\n    'scikit-learn__scikit-learn-14710',\n    'scikit-learn__scikit-learn-14732',\n    'scikit-learn__scikit-learn-14764',\n    'scikit-learn__scikit-learn-14806',\n    'scikit-learn__scikit-learn-14869',\n    'scikit-learn__scikit-learn-14878',\n    'scikit-learn__scikit-learn-14890',\n    'scikit-learn__scikit-learn-14894',\n    'scikit-learn__scikit-learn-14898',\n    'scikit-learn__scikit-learn-14908',\n    'scikit-learn__scikit-learn-14983',\n    'scikit-learn__scikit-learn-14999',\n    'scikit-learn__scikit-learn-15028',\n    'scikit-learn__scikit-learn-15084',\n    'scikit-learn__scikit-learn-15086',\n    'scikit-learn__scikit-learn-15094',\n    'scikit-learn__scikit-learn-15096',\n    'scikit-learn__scikit-learn-15100',\n    'scikit-learn__scikit-learn-15119',\n    'scikit-learn__scikit-learn-15120',\n    'scikit-learn__scikit-learn-15138',\n    'scikit-learn__scikit-learn-15393',\n    'scikit-learn__scikit-learn-15495',\n    'scikit-learn__scikit-learn-15512',\n    'scikit-learn__scikit-learn-15524',\n    'scikit-learn__scikit-learn-15535',\n    'scikit-learn__scikit-learn-15625',\n    'scikit-learn__scikit-learn-3840',\n    'scikit-learn__scikit-learn-7760',\n    'scikit-learn__scikit-learn-8554',\n    'scikit-learn__scikit-learn-9274',\n    'scikit-learn__scikit-learn-9288',\n    'scikit-learn__scikit-learn-9304',\n    'scikit-learn__scikit-learn-9775',\n    'scikit-learn__scikit-learn-9939',\n    'sphinx-doc__sphinx-11311',\n    'sphinx-doc__sphinx-7910',\n    'sympy__sympy-12812',\n    'sympy__sympy-14248',\n    'sympy__sympy-15222',\n    'sympy__sympy-19201',\n}\n\nMUTATION_TEMPLATE = \"\"\"[cosmic-ray]\nmodule-path = \"{source_fp}\"\ntimeout = {timeout}\nexcluded-modules = []\ntest-command = \"{test_cmd}\"\n\n[cosmic-ray.distributor]\nname = \"local\"\n\"\"\"\n\nMUTATION_TIMEOUT = 3600\nMUTATION_BUFFER = 500\n\nUPDATE_TOX = r\"\"\"\nadd_coverage_tox() {\n  local config_file=\"$1\"\n\n  # Check if the file exists\n  if [[ ! -f \"$config_file\" ]]; then\n    echo \"Error: File $config_file does not exist.\"\n    return 1\n  fi\n\n  # Check if the [testenv] section exists\n  if ! grep -q \"^\\[testenv\\]\" \"$config_file\"; then\n    echo \"Error: [testenv] section not found in $config_file.\"\n    return 1\n  fi\n\n  # Modify the commands in the [testenv] section\n  awk '\n    BEGIN { in_testenv = 0; commands_modified = 0 }\n    /^\\[testenv\\]/ { in_testenv = 1 }\n    in_testenv && /^commands\\s*=/ {\n      # Start modifying commands\n      print $0\n      while (getline > 0) {\n        if ($0 ~ /^[[:space:]]*$/) break # Stop at the next empty line\n        if ($0 ~ /pytest/ && $0 !~ /--cov/) {\n          # Add --cov to pytest commands\n          sub(/pytest/, \"pytest --cov sphinx\", $0)\n        }\n        print \"    \" $0\n      }\n      # Add coverage json command after pytest\n      if (commands_modified == 0) {\n        print \"    coverage json -o coverage.json\"\n        commands_modified = 1\n      }\n      next\n    }\n    { print }\n  ' \"$config_file\" > \"${config_file}.tmp\" && mv \"${config_file}.tmp\" \"$config_file\"\n\n  echo \">> Coverage commands successfully added to $config_file.\"\n}\n\"\"\"\n",
    "evaluation/benchmarks/testgeneval/eval_infer.py": "import os\nimport tempfile\nimport time\nfrom functools import partial\n\nimport pandas as pd\nfrom report_utils import (\n    check_coverage,\n    check_mutation,\n    count_methods,\n    get_lines_of_code,\n)\n\nfrom evaluation.benchmarks.testgeneval.compute_readability import compute_readability\nfrom evaluation.benchmarks.testgeneval.constants import (\n    COVERAGE_PREFIX,\n    MUTATION_BUFFER,\n    MUTATION_TEMPLATE,\n    MUTATION_TIMEOUT,\n    TESTS_SUFFIX,\n)\nfrom evaluation.benchmarks.testgeneval.metrics import (\n    bleu,\n    edit_sim,\n    exact_match,\n    rouge_l,\n)\nfrom evaluation.benchmarks.testgeneval.pygments_utils import tokenize_code\nfrom evaluation.benchmarks.testgeneval.run_infer import get_instance_docker_image\nfrom evaluation.benchmarks.testgeneval.test_filter import filter_tests\nfrom evaluation.benchmarks.testgeneval.test_spec import (\n    TestGenEvalInstance,\n    TestSpec,\n    make_test_spec,\n)\nfrom evaluation.benchmarks.testgeneval.utils import load_testgeneval_dataset\nfrom evaluation.utils.shared import (\n    EvalMetadata,\n    EvalOutput,\n    prepare_dataset,\n    reset_logger_for_multiprocessing,\n    run_evaluation,\n)\nfrom openhands.core.config import AppConfig, SandboxConfig, get_parser\nfrom openhands.core.logger import openhands_logger as logger\nfrom openhands.core.main import create_runtime\nfrom openhands.events.action import CmdRunAction\nfrom openhands.events.observation import CmdOutputObservation\nfrom openhands.utils.async_utils import call_async_from_sync\n\nDOCKER_IMAGE_PREFIX = os.environ.get('EVAL_DOCKER_IMAGE_PREFIX', 'docker.io/kdjain/')\nlogger.info(f'Using docker image prefix: {DOCKER_IMAGE_PREFIX}')\n\n\ndef get_config(instance: pd.Series) -> AppConfig:\n    base_container_image = get_instance_docker_image(instance['instance_id_swebench'])\n    assert (\n        base_container_image\n    ), f\"Invalid container image for instance {instance['instance_id_swebench']}.\"\n    logger.info(f'Using instance container image: {base_container_image}.')\n    return AppConfig(\n        run_as_openhands=False,\n        runtime=os.environ.get('RUNTIME', 'eventstream'),\n        sandbox=SandboxConfig(\n            base_container_image=base_container_image,\n            use_host_network=False,\n            timeout=1800,\n            api_key=os.environ.get('ALLHANDS_API_KEY'),\n            remote_runtime_api_url=os.environ.get(\n                'SANDBOX_REMOTE_RUNTIME_API_URL', 'http://localhost:8000'\n            ),\n        ),\n        workspace_base=None,\n        workspace_mount_path=None,\n    )\n\n\ndef compute_lexical_metrics(pred_suite, gold_suite):\n    pred_loc = get_lines_of_code(pred_suite)\n    gold_loc = get_lines_of_code(gold_suite)\n    pred_methods = count_methods(pred_suite)\n    gold_methods = count_methods(gold_suite)\n    readability_pred = compute_readability(pred_suite)\n    readability_gold = compute_readability(gold_suite)\n\n    preds = tokenize_code(pred_suite)\n    golds = tokenize_code(gold_suite)\n\n    return {\n        'pred_loc': pred_loc,\n        'gold_loc': gold_loc,\n        'pred_readability': readability_pred,\n        'gold_readability': readability_gold,\n        'pred_methods': pred_methods,\n        'gold_methods': gold_methods,\n        'bleu': bleu(preds, golds),\n        'xmatch': exact_match(preds, golds),\n        'edit_sim': edit_sim(preds, golds),\n        'rouge_f': rouge_l(golds, preds)['f'],\n        'rouge_p': rouge_l(golds, preds)['p'],\n        'rouge_r': rouge_l(golds, preds)['r'],\n    }\n\n\ndef run_command(runtime, command, timeout=600):\n    action = CmdRunAction(command=command)\n    action.set_hard_timeout(timeout)\n    logger.info(action, extra={'msg_type': 'ACTION'})\n    obs = runtime.run_action(action)\n    logger.info(obs, extra={'msg_type': 'OBSERVATION'})\n    assert obs.exit_code == 0\n    return obs\n\n\ndef run_tests(runtime, instance, test_script, log_file='/tmp/test_output.log'):\n    action = CmdRunAction(command=f'bash {test_script} > {log_file} 2>&1 & echo $!')\n    action.set_hard_timeout(60)\n    obs = runtime.run_action(action)\n\n    assert isinstance(obs, CmdOutputObservation), 'Failed to start test script.'\n    pid = obs.content.split()[-1].strip()\n    logger.info(f'[{instance.instance_id}] Test process started with PID: {pid}')\n\n    start_time = time.time()\n    timeout = 1800\n    while True:\n        elapsed_time = time.time() - start_time\n        if elapsed_time > timeout:\n            logger.info(f'[{instance.instance_id}] Test process timed out.')\n            instance['test_result']['report']['test_timeout'] = True\n            break\n\n        check_action = CmdRunAction(command=f'ps -p {pid} > /dev/null; echo $?')\n        check_obs = runtime.run_action(check_action)\n        if (\n            isinstance(check_obs, CmdOutputObservation)\n            and len(check_obs.content.split()) > 0\n            and check_obs.content.split()[-1].strip() == '1'\n        ):\n            logger.info(f'[{instance.instance_id}] Test process completed.')\n            break\n        time.sleep(30)\n\n    test_action = CmdRunAction(command=f'cat {log_file}')\n    test_action.set_hard_timeout(300)\n    test_obs = runtime.run_action(test_action)\n    assert isinstance(test_obs, CmdOutputObservation), 'Failed to retrieve test output.'\n    return test_obs.exit_code, test_obs.content, elapsed_time\n\n\ndef run_mutation_testing(\n    runtime, instance, mutation_script, log_file='/tmp/mutation_output.log'\n):\n    action = CmdRunAction(command=f'bash {mutation_script} > {log_file} 2>&1 & echo $!')\n    action.set_hard_timeout(60)\n    obs = runtime.run_action(action)\n\n    assert isinstance(obs, CmdOutputObservation), 'Failed to start test script.'\n    pid = obs.content.split()[-1].strip()\n    logger.info(f'[{instance.instance_id}] Mutation process started with PID: {pid}')\n\n    start_time = time.time()\n    timeout = 4000\n    while True:\n        elapsed_time = time.time() - start_time\n        if elapsed_time > timeout:\n            logger.info(f'[{instance.instance_id}] Mutation process timed out.')\n            instance['test_result']['report']['mutation_timeout'] = True\n            break\n\n        check_action = CmdRunAction(command=f'ps -p {pid} > /dev/null; echo $?')\n        check_obs = runtime.run_action(check_action)\n        if (\n            isinstance(check_obs, CmdOutputObservation)\n            and len(check_obs.content.split()) > 0\n            and check_obs.content.split()[-1].strip() == '1'\n        ):\n            logger.info(f'[{instance.instance_id}] Mutation process completed.')\n            break\n        time.sleep(30)\n\n    assert isinstance(obs, CmdOutputObservation), 'Failed to run mutation script.'\n    mutation_action = CmdRunAction(command=f'cat {log_file}')\n    mutation_action.set_hard_timeout(300)\n    mutation_obs = runtime.run_action(mutation_action)\n    assert isinstance(\n        mutation_obs, CmdOutputObservation\n    ), 'Failed to retrieve mutation output.'\n    return mutation_obs.exit_code, mutation_obs.content\n\n\ndef grade_test_output(\n    test_suite: str, instance: pd.Series, test_output: str, test_spec: TestSpec, runtime\n):\n    \"\"\"\n    Two-pass test grading with short-circuiting:\n    1. Run all tests to identify passing/failing tests\n    2. If no failing tests, evaluate coverage immediately\n    3. Otherwise, run only passing tests for coverage analysis\n    \"\"\"\n    unit_test_output, coverage_output = '', ''\n    if TESTS_SUFFIX in test_output:\n        unit_test_output = test_output.split(TESTS_SUFFIX)[0]\n\n    if not unit_test_output:\n        return (\n            False,\n            0,\n            '',\n            '',\n            {\n                'total_tests': 0,\n                'passing_tests': 0,\n                'failing_tests': 0,\n                'any_pass': False,\n                'all_pass': False,\n                'passing_test_names': [],\n                'failing_test_names': [],\n            },\n        )\n\n    logger.info('Calling filter unit tests')\n    filtered_content, passing_tests, failing_tests = filter_tests(\n        test_suite, unit_test_output, test_spec.repo\n    )\n\n    total_tests = len(passing_tests) + len(failing_tests)\n    test_stats = {\n        'total_tests': total_tests,\n        'passing_tests': len(passing_tests),\n        'failing_tests': len(failing_tests),\n        'any_pass': len(passing_tests) > 0,\n        'all_pass': len(failing_tests) == 0 and total_tests > 0,\n        'passing_test_names': passing_tests,\n        'failing_test_names': failing_tests,\n    }\n\n    if not passing_tests:\n        return False, 0, unit_test_output, coverage_output, test_stats\n\n    # If all tests pass, evaluate coverage immediately\n    if not failing_tests:\n        coverage = 0\n        cov_success = False\n        if COVERAGE_PREFIX in test_output:\n            coverage_output = test_output.split(COVERAGE_PREFIX)[1]\n            _, coverage = check_coverage(coverage_output, test_spec.code_file)\n            cov_success = True\n        # test_stats['filtered_suite'] = test_suite\n        return cov_success, coverage, unit_test_output, coverage_output, test_stats\n\n    cov_success = False\n    coverage = 0\n    # Second pass - run coverage on passing tests\n    if filtered_content:\n        with tempfile.TemporaryDirectory() as temp_dir:\n            test_suite_path = os.path.join(temp_dir, 'test_suite.py')\n            with open(test_suite_path, 'w') as f:\n                f.write(filtered_content)\n            runtime.copy_to(test_suite_path, '/tmp')\n\n        run_command(runtime, f'cp /tmp/test_suite.py /testbed/{test_spec.test_file}')\n        _, test_output_second_pass, _ = run_tests(runtime, instance, '/tmp/test.sh')\n\n        coverage, coverage_output, unit_test_output = 0, '', test_output_second_pass\n\n        if COVERAGE_PREFIX in test_output_second_pass:\n            coverage_output = test_output_second_pass.split(COVERAGE_PREFIX)[1]\n            unit_test_output = test_output_second_pass.split(TESTS_SUFFIX)[0]\n            _, coverage = check_coverage(coverage_output, test_spec.code_file)\n            cov_success = True\n\n    # test_stats['filtered_suite'] = filtered_content\n    return cov_success, coverage, unit_test_output, coverage_output, test_stats\n\n\ndef process_instance(\n    instance: pd.Series,\n    metadata: EvalMetadata,\n    reset_logger: bool = True,\n    log_dir: str | None = None,\n) -> EvalOutput:\n    \"\"\"\n    Evaluate agent performance on a TestGenEval problem instance.\n\n    Note that this signature differs from the expected input to `run_evaluation`. Use\n    `functools.partial` to provide optional arguments before passing to the evaluation harness.\n\n    Args:\n        log_dir (str | None, default=None): Path to directory where log files will be written. Must\n        be provided if `reset_logger` is set.\n\n    Raises:\n        AssertionError: if the `reset_logger` flag is set without a provided log directory.\n    \"\"\"\n    if reset_logger:\n        assert (\n            log_dir is not None\n        ), \"Can't reset logger without a provided log directory.\"\n        os.makedirs(log_dir, exist_ok=True)\n        reset_logger_for_multiprocessing(logger, instance.instance_id, log_dir)\n    else:\n        logger.info(f'Starting evaluation for instance {instance.instance_id}.')\n\n    config = get_config(instance)\n    id = instance.instance_id\n    logger.info(f'Starting evaluation for instance {id}.')\n\n    instance['test_result']['id'] = id\n    instance['test_result']['report'] = {\n        'test_output': '',\n        # 'coverage_output': '',\n        # 'mutation_output': '',\n        'empty_generation': False,\n        'error_eval': False,\n        'all_tests_pass': False,\n        'tests_pass': False,\n        'test_timeout': False,\n        'mutation_timeout': False,\n        'coverage_success': False,\n        'mutation_success': False,\n        'coverage': 0,\n        'mutation_score': 0,\n        'mutation_error_interval': -1,\n        'num_mutants': -1,\n    }\n\n    instance['test_result']['lexical'] = {\n        'pred_loc': -1,\n        'gold_loc': -1,\n        'pred_readability': -1,\n        'gold_readability': -1,\n        'pred_methods': -1,\n        'gold_methods': -1,\n        'bleu': -1,\n        'xmatch': -1,\n        'edit_sim': -1,\n        'rouge_f': -1,\n        'rouge_p': -1,\n        'rouge_r': -1,\n    }\n\n    if instance['test_suite'] == '' or instance['test_suite'] is None:\n        instance['test_result']['report']['empty_generation'] = True\n        return EvalOutput(\n            instance_id=instance.instance_id, test_result=instance['test_result']\n        )\n\n    if not args.skip_lexical:\n        lexical_metrics = compute_lexical_metrics(\n            instance['test_suite'], instance['instance']['test_src']\n        )\n        instance['test_result']['lexical'] = lexical_metrics\n\n    test_suite = instance['test_suite']\n    test_spec: TestSpec = instance['test_spec']\n    runtime = create_runtime(config)\n    call_async_from_sync(runtime.connect)\n    with tempfile.TemporaryDirectory() as temp_dir:\n        test_suite_path = os.path.join(temp_dir, 'test_suite.py')\n        with open(test_suite_path, 'w') as f:\n            f.write(test_suite)\n        runtime.copy_to(test_suite_path, '/tmp')\n\n        test_script_path = os.path.join(temp_dir, 'test.sh')\n        with open(test_script_path, 'w') as f:\n            f.write(test_spec.test_script)\n        runtime.copy_to(test_script_path, '/tmp')\n\n        mutation_script_path = os.path.join(temp_dir, 'mutation.sh')\n        with open(mutation_script_path, 'w') as f:\n            f.write(test_spec.mutation_script)\n        runtime.copy_to(mutation_script_path, '/tmp')\n\n    try:\n        run_command(runtime, 'chmod +x /tmp/test.sh /tmp/mutation.sh')\n        run_command(runtime, f'cp /tmp/test_suite.py /testbed/{test_spec.test_file}')\n\n        # First pass - run all tests\n        _, test_output, test_time = run_tests(runtime, instance, '/tmp/test.sh')\n\n        # Grade tests with two-pass approach\n        coverage_success, coverage, unit_test_output, coverage_output, test_stats = (\n            grade_test_output(test_suite, instance, test_output, test_spec, runtime)\n        )\n\n        # Update report with test statistics\n        instance['test_result']['report'].update(\n            {\n                'test_output': unit_test_output,\n                # 'coverage_output': coverage_output,\n                'tests_pass': test_stats['any_pass'],  # Changed to use any_pass\n                'all_tests_pass': test_stats['all_pass'],  # Added all_pass metric\n                'coverage_success': coverage_success,\n                'coverage': coverage if coverage_success else 0,\n                'test_stats': test_stats,\n            }\n        )\n\n        # Only run mutation testing if we have passing tests and coverage\n        if (\n            not args.skip_mutation\n            and coverage_success\n            and test_stats['any_pass']\n            and coverage > 0\n        ):\n            mutation_timeout = max(10, 1.5 * test_time)\n            mutation_toml = MUTATION_TEMPLATE.format(\n                test_cmd=test_spec.test_cmd,\n                source_fp=test_spec.code_file,\n                timeout=mutation_timeout,\n            )\n\n            with tempfile.TemporaryDirectory() as temp_dir:\n                mutation_toml_path = os.path.join(temp_dir, 'mutation.toml')\n                with open(mutation_toml_path, 'w') as f:\n                    f.write(mutation_toml)\n                runtime.copy_to(mutation_toml_path, '/tmp')\n\n            run_command(runtime, 'cp /tmp/mutation.toml /testbed/mutation.toml')\n\n            mutation_code, mutation_output = run_mutation_testing(\n                runtime, instance, '/tmp/mutation.sh'\n            )\n            # instance['test_result']['report']['mutation_output'] = mutation_output\n            if mutation_output and mutation_code == 0:\n                (\n                    mutation_success,\n                    num_mutants,\n                    mutation_score,\n                    mutation_confidence_interval,\n                ) = check_mutation(mutation_output)\n                instance['test_result']['report']['num_mutants'] = num_mutants\n                instance['test_result']['report']['mutation_success'] = mutation_success\n                instance['test_result']['report']['mutation_score'] = mutation_score\n                instance['test_result']['report']['mutation_error_interval'] = (\n                    mutation_confidence_interval\n                )\n\n        return EvalOutput(\n            instance_id=instance.instance_id, test_result=instance['test_result']\n        )\n    except Exception as e:\n        logger.error(f'Error processing instance {instance.instance_id}: {e}')\n        raise RuntimeError(\n            instance.instance_id,\n            'Unexpected output...',\n            logger,\n        )\n\n    finally:\n        runtime.close()\n\n\ndef count_and_log_fields(evaluated_predictions, fields, key):\n    \"\"\"\n    Count and log the sum of specified fields in the evaluated predictions,\n    ignoring fields with a value of -1. If all values for a field are -1,\n    return -1.\n\n    :param evaluated_predictions: DataFrame containing evaluation results\n    :param fields: List of field names to count\n    :param key: Key to access the field values ('report' or 'lexical')\n    \"\"\"\n\n    def count_field(row, field):\n        value = row['test_result'][key][field]\n        return (\n            value if value != -1 else None\n        )  # Ignore -1 fields by treating them as None\n\n    for field in fields:\n        # Extract the valid values for the field, ignoring -1\n        valid_values = evaluated_predictions.apply(\n            count_field, args=(field,), axis=1\n        ).dropna()\n\n        if valid_values.empty:  # If all values are -1\n            logger.info(f'# {field}: -1 (All values are -1)')\n        else:\n            count = valid_values.sum()  # Sum of valid values\n            length = len(valid_values)  # Count of valid entries\n            logger.info(f'# {field}: {length}. ({count / length:.2f})')\n\n\nif __name__ == '__main__':\n    parser = get_parser()\n    parser.add_argument(\n        '--input-file', type=str, required=True, help='Path to input predictions file'\n    )\n    parser.add_argument(\n        '--dataset',\n        type=str,\n        default='kjain14/testgeneval',\n        help='Dataset to evaluate on',\n    )\n    parser.add_argument(\n        '--split', type=str, default='test', help='Split to evaluate on'\n    )\n    parser.add_argument(\n        '--skip_mutation', action='store_true', help='Skip mutation testing'\n    )\n    parser.add_argument(\n        '--skip_lexical', action='store_true', help='Skip lexical metrics'\n    )\n    parser.add_argument(\n        '--mutation_timeout',\n        type=int,\n        default=MUTATION_TIMEOUT,\n        help='Mutation timeout',\n    )\n    parser.add_argument(\n        '--mutation_buffer',\n        type=int,\n        default=MUTATION_BUFFER,\n        help='Mutation buffer',\n    )\n    args, _ = parser.parse_known_args()\n\n    dataset: list[TestGenEvalInstance] = load_testgeneval_dataset(\n        args.dataset, args.split\n    )\n\n    logger.info(\n        f'Loaded dataset {args.dataset} with split {args.split} to run inference on.'\n    )\n\n    # Load predictions\n    assert args.input_file.endswith('.jsonl'), 'Input file must be a jsonl file.'\n    predictions = pd.read_json(args.input_file, lines=True)\n    assert (\n        'instance_id' in predictions.columns\n    ), 'Input file must contain instance_id column.'\n\n    if 'test_suite' not in predictions.columns and (\n        'test_result' in predictions.columns\n        and 'test_suite' in predictions['test_result'].iloc(0)\n    ):\n        raise ValueError(\n            'Input file must contain test_suite column OR test_result column with test_suite field.'\n        )\n\n    if 'instance_id_swebench' not in predictions.columns:\n        predictions['instance_id_swebench'] = predictions['instance'].apply(\n            lambda x: x['instance_id_swebench']\n        )\n\n    if 'instance_id' not in predictions.columns and (\n        'instance_id' in predictions['instance'].iloc(0)\n    ):\n        raise ValueError(\n            'Input file must contain id column OR instance column with id field.'\n        )\n\n    if 'instance_id' not in predictions.columns:\n        predictions['instance_id'] = predictions['instance'].apply(\n            lambda x: x['instance_id']\n        )\n\n    if 'test_suite' not in predictions.columns:\n        predictions['test_suite'] = predictions['test_result'].apply(\n            lambda x: x['test_suite']\n        )\n\n    assert len(predictions['instance_id'].unique()) == len(\n        predictions\n    ), 'instance_id column must be unique.'\n\n    assert {'instance_id_swebench', 'test_suite', 'instance_id'}.issubset(\n        set(predictions.columns)\n    ), 'Input file must contain id, instance_id and test_suite columns.'\n\n    predictions['test_spec'] = predictions['instance'].apply(\n        lambda x: make_test_spec(x, args.mutation_timeout, args.mutation_buffer)\n    )\n\n    output_file = args.input_file.replace('.jsonl', '.testgeneval.jsonl')\n    instances = prepare_dataset(predictions, output_file, args.eval_n_limit)\n\n    # If possible, load the relevant metadata to avoid issues with `run_evaluation`.\n    metadata: EvalMetadata | None = None\n    metadata_filepath = os.path.join(os.path.dirname(args.input_file), 'metadata.json')\n    if os.path.exists(metadata_filepath):\n        with open(metadata_filepath, 'r') as metadata_file:\n            data = metadata_file.read()\n            metadata = EvalMetadata.model_validate_json(data)\n\n    # The evaluation harness constrains the signature of `process_instance_func` but we need to\n    # pass extra information. Build a new function object to avoid issues with multiprocessing.\n    process_instance_func = partial(\n        process_instance, log_dir=output_file.replace('.jsonl', '.logs')\n    )\n\n    run_evaluation(\n        instances,\n        metadata=None,\n        output_file=output_file,\n        num_workers=args.eval_num_workers,\n        process_instance_func=process_instance_func,\n    )\n\n    # Load evaluated predictions & print number of resolved predictions\n    evaluated_predictions = pd.read_json(output_file, lines=True)\n    report_fields = [\n        'coverage',\n        'mutation_score',\n        'tests_pass',\n        'all_tests_pass',\n        'empty_generation',\n        'coverage_success',\n        'test_timeout',\n        'error_eval',\n    ]\n    lexical_fields = [\n        'pred_loc',\n        'gold_loc',\n        'pred_methods',\n        'gold_methods',\n        'bleu',\n        'xmatch',\n        'edit_sim',\n        'rouge_f',\n        'rouge_p',\n        'rouge_r',\n    ]\n\n    # Log report and lexical fields\n    count_and_log_fields(evaluated_predictions, report_fields, key='report')\n    count_and_log_fields(evaluated_predictions, lexical_fields, key='lexical')\n",
    "evaluation/benchmarks/testgeneval/log_parsers.py": "import re\n\nfrom evaluation.benchmarks.testgeneval.constants import TestStatus\n\n\ndef parse_log_pytest(log: str) -> dict[str, str]:\n    \"\"\"\n    Parser for test logs generated with PyTest framework\n\n    Args:\n        log (str): log content\n    Returns:\n        dict: test case to test status mapping\n    \"\"\"\n    test_status_map = {}\n    for line in log.split('\\n'):\n        if any([line.startswith(x.value) for x in TestStatus]):\n            # Additional parsing for FAILED status\n            if line.startswith(TestStatus.FAILED.value):\n                line = line.replace(' - ', ' ')\n            test_case = line.split()\n            if len(test_case) <= 1:\n                continue\n            test_status_map[test_case[1]] = test_case[0]\n    return test_status_map\n\n\ndef parse_log_pytest_options(log: str) -> dict[str, str]:\n    \"\"\"\n    Parser for test logs generated with PyTest framework with options\n\n    Args:\n        log (str): log content\n    Returns:\n        dict: test case to test status mapping\n    \"\"\"\n    option_pattern = re.compile(r'(.*?)\\[(.*)\\]')\n    test_status_map = {}\n    for line in log.split('\\n'):\n        if any([line.startswith(x.value) for x in TestStatus]):\n            # Additional parsing for FAILED status\n            if line.startswith(TestStatus.FAILED.value):\n                line = line.replace(' - ', ' ')\n            test_case = line.split()\n            if len(test_case) <= 1:\n                continue\n            has_option = option_pattern.search(test_case[1])\n            if has_option:\n                main, option = has_option.groups()\n                if (\n                    option.startswith('/')\n                    and not option.startswith('//')\n                    and '*' not in option\n                ):\n                    option = '/' + option.split('/')[-1]\n                test_name = f'{main}[{option}]'\n            else:\n                test_name = test_case[1]\n            test_status_map[test_name] = test_case[0]\n    return test_status_map\n\n\ndef parse_log_django(log: str) -> dict[str, str]:\n    \"\"\"\n    Parser for test logs generated with Django tester framework\n\n    Args:\n        log (str): log content\n    Returns:\n        dict: test case to test status mapping\n    \"\"\"\n    test_status_map = {}\n    lines = log.split('\\n')\n\n    prev_test = None\n    for line in lines:\n        line = line.strip()\n\n        # This isn't ideal but the test output spans multiple lines\n        if '--version is equivalent to version' in line:\n            test_status_map['--version is equivalent to version'] = (\n                TestStatus.PASSED.value\n            )\n\n        # Log it in case of error\n        if ' ... ' in line:\n            prev_test = line.split(' ... ')[0]\n\n        pass_suffixes = (' ... ok', ' ... OK', ' ...  OK')\n        for suffix in pass_suffixes:\n            if line.endswith(suffix):\n                # TODO: Temporary, exclusive fix for django__django-7188\n                # The proper fix should involve somehow getting the test results to\n                # print on a separate line, rather than the same line\n                if line.strip().startswith(\n                    'Applying sites.0002_alter_domain_unique...test_no_migrations'\n                ):\n                    line = line.split('...', 1)[-1].strip()\n                test = line.rsplit(suffix, 1)[0]\n                test_status_map[test] = TestStatus.PASSED.value\n                break\n        if ' ... skipped' in line:\n            test = line.split(' ... skipped')[0]\n            test_status_map[test] = TestStatus.SKIPPED.value\n        if line.endswith(' ... FAIL'):\n            test = line.split(' ... FAIL')[0]\n            test_status_map[test] = TestStatus.FAILED.value\n        if line.startswith('FAIL:'):\n            test = line.split()[1].strip()\n            test_status_map[test] = TestStatus.FAILED.value\n        if line.endswith(' ... ERROR'):\n            test = line.split(' ... ERROR')[0]\n            test_status_map[test] = TestStatus.ERROR.value\n        if line.startswith('ERROR:'):\n            test = line.split()[1].strip()\n            test_status_map[test] = TestStatus.ERROR.value\n\n        if line.lstrip().startswith('ok') and prev_test is not None:\n            # It means the test passed, but there's some additional output (including new lines)\n            # between \"...\" and \"ok\" message\n            test = prev_test\n            test_status_map[test] = TestStatus.PASSED.value\n\n    # TODO: This is very brittle, we should do better\n    # There's a bug in the django logger, such that sometimes a test output near the end gets\n    # interrupted by a particular long multiline print statement.\n    # We have observed this in one of 3 forms:\n    # - \"{test_name} ... Testing against Django installed in {*} silenced.\\nok\"\n    # - \"{test_name} ... Internal Server Error: \\/(.*)\\/\\nok\"\n    # - \"{test_name} ... System check identified no issues (0 silenced).\\nok\"\n    patterns = [\n        r'^(.*?)\\s\\.\\.\\.\\sTesting\\ against\\ Django\\ installed\\ in\\ ((?s:.*?))\\ silenced\\)\\.\\nok$',\n        r'^(.*?)\\s\\.\\.\\.\\sInternal\\ Server\\ Error:\\ \\/(.*)\\/\\nok$',\n        r'^(.*?)\\s\\.\\.\\.\\sSystem check identified no issues \\(0 silenced\\)\\nok$',\n    ]\n    for pattern in patterns:\n        for match in re.finditer(pattern, log, re.MULTILINE):\n            test_name = match.group(1)\n            test_status_map[test_name] = TestStatus.PASSED.value\n    return test_status_map\n\n\ndef parse_log_pytest_v2(log: str) -> dict[str, str]:\n    \"\"\"\n    Parser for test logs generated with PyTest framework (Later Version)\n\n    Args:\n        log (str): log content\n    Returns:\n        dict: test case to test status mapping\n    \"\"\"\n    test_status_map = {}\n    escapes = ''.join([chr(char) for char in range(1, 32)])\n    for line in log.split('\\n'):\n        line = re.sub(r'\\[(\\d+)m', '', line)\n        translator = str.maketrans('', '', escapes)\n        line = line.translate(translator)\n        if any([line.startswith(x.value) for x in TestStatus]):\n            if line.startswith(TestStatus.FAILED.value):\n                line = line.replace(' - ', ' ')\n            test_case = line.split()\n            if len(test_case) >= 2:\n                test_status_map[test_case[1]] = test_case[0]\n        # Support older pytest versions by checking if the line ends with the test status\n        elif any([line.endswith(x.value) for x in TestStatus]):\n            test_case = line.split()\n            if len(test_case) >= 2:\n                test_status_map[test_case[0]] = test_case[1]\n    return test_status_map\n\n\ndef parse_log_seaborn(log: str) -> dict[str, str]:\n    \"\"\"\n    Parser for test logs generated with seaborn testing framework\n\n    Args:\n        log (str): log content\n    Returns:\n        dict: test case to test status mapping\n    \"\"\"\n    test_status_map = {}\n    for line in log.split('\\n'):\n        if line.startswith(TestStatus.FAILED.value):\n            test_case = line.split()[1]\n            test_status_map[test_case] = TestStatus.FAILED.value\n        elif f' {TestStatus.PASSED.value} ' in line:\n            parts = line.split()\n            if parts[1] == TestStatus.PASSED.value:\n                test_case = parts[0]\n                test_status_map[test_case] = TestStatus.PASSED.value\n        elif line.startswith(TestStatus.PASSED.value):\n            parts = line.split()\n            test_case = parts[1]\n            test_status_map[test_case] = TestStatus.PASSED.value\n    return test_status_map\n\n\ndef parse_log_sympy(log: str) -> dict[str, str]:\n    \"\"\"\n    Parser for test logs generated with Sympy framework\n\n    Args:\n        log (str): log content\n    Returns:\n        dict: test case to test status mapping\n    \"\"\"\n    test_status_map = {}\n    pattern = r'(_*) (.*)\\.py:(.*) (_*)'\n    matches = re.findall(pattern, log)\n    for match in matches:\n        test_case = f'{match[1]}.py:{match[2]}'\n        test_status_map[test_case] = TestStatus.FAILED.value\n    for line in log.split('\\n'):\n        line = line.strip()\n        if line.startswith('test_'):\n            if line.endswith('[FAIL]') or line.endswith('[OK]'):\n                line = line[: line.rfind('[')]\n                line = line.strip()\n            if line.endswith(' E'):\n                test = line.split()[0]\n                test_status_map[test] = TestStatus.ERROR.value\n            if line.endswith(' F'):\n                test = line.split()[0]\n                test_status_map[test] = TestStatus.FAILED.value\n            if line.endswith(' ok'):\n                test = line.split()[0]\n                test_status_map[test] = TestStatus.PASSED.value\n    return test_status_map\n\n\ndef parse_log_matplotlib(log: str) -> dict[str, str]:\n    \"\"\"\n    Parser for test logs generated with PyTest framework\n\n    Args:\n        log (str): log content\n    Returns:\n        dict: test case to test status mapping\n    \"\"\"\n    test_status_map = {}\n    for line in log.split('\\n'):\n        line = line.replace('MouseButton.LEFT', '1')\n        line = line.replace('MouseButton.RIGHT', '3')\n        if any([line.startswith(x.value) for x in TestStatus]):\n            # Additional parsing for FAILED status\n            if line.startswith(TestStatus.FAILED.value):\n                line = line.replace(' - ', ' ')\n            test_case = line.split()\n            if len(test_case) <= 1:\n                continue\n            test_status_map[test_case[1]] = test_case[0]\n    return test_status_map\n\n\nparse_log_astroid = parse_log_pytest\nparse_log_flask = parse_log_pytest\nparse_log_marshmallow = parse_log_pytest\nparse_log_pvlib = parse_log_pytest\nparse_log_pyvista = parse_log_pytest\nparse_log_sqlfluff = parse_log_pytest\nparse_log_xarray = parse_log_pytest\n\nparse_log_pydicom = parse_log_pytest_options\nparse_log_requests = parse_log_pytest_options\nparse_log_pylint = parse_log_pytest_options\n\nparse_log_astropy = parse_log_pytest_v2\nparse_log_scikit = parse_log_pytest_v2\nparse_log_sphinx = parse_log_pytest_v2\n\n\nMAP_REPO_TO_PARSER = {\n    'astropy/astropy': parse_log_astropy,\n    'django/django': parse_log_django,\n    'marshmallow-code/marshmallow': parse_log_marshmallow,\n    'matplotlib/matplotlib': parse_log_matplotlib,\n    'mwaskom/seaborn': parse_log_seaborn,\n    'pallets/flask': parse_log_flask,\n    'psf/requests': parse_log_requests,\n    'pvlib/pvlib-python': parse_log_pvlib,\n    'pydata/xarray': parse_log_xarray,\n    'pydicom/pydicom': parse_log_pydicom,\n    'pylint-dev/astroid': parse_log_astroid,\n    'pylint-dev/pylint': parse_log_pylint,\n    'pytest-dev/pytest': parse_log_pytest,\n    'pyvista/pyvista': parse_log_pyvista,\n    'scikit-learn/scikit-learn': parse_log_scikit,\n    'sqlfluff/sqlfluff': parse_log_sqlfluff,\n    'sphinx-doc/sphinx': parse_log_sphinx,\n    'sympy/sympy': parse_log_sympy,\n}\n",
    "evaluation/benchmarks/testgeneval/metrics.py": "import sys\nfrom typing import Callable, Dict, List, Optional, Sequence, TypeVar, Union\n\nimport nltk\nimport numpy as np\nfrom fuzzywuzzy import fuzz\nfrom rouge import Rouge\n\n# increase recursion depth to ensure ROUGE can be calculated for long sentences\nif sys.getrecursionlimit() < 10_000:\n    sys.setrecursionlimit(10_000)\n\n\ndef bleu(gold: List[str], pred: List[str]) -> float:\n    \"\"\"\n    Calculate BLEU score, using smoothing method 2 with auto reweighting, in the range of 0~100.\n\n    :param gold: list of gold tokens\n    :param pred: list of predicted tokens\n    :return: BLEU score\n    \"\"\"\n    if len(pred) == 0 or len(gold) == 0:\n        return 0.0\n    return 100.0 * nltk.translate.bleu_score.sentence_bleu(\n        [gold],\n        pred,\n        smoothing_function=nltk.translate.bleu_score.SmoothingFunction().method2,\n        auto_reweigh=True,\n    )\n\n\ndef batch_bleu(golds: List[List[str]], preds: List[List[str]]) -> List[float]:\n    \"\"\"\n    Calculate BLEU score for a batch of sentences.\n\n    :param golds: list of gold sentences\n    :param preds: list of predicted sentences\n    :return: list of BLEU scores\n    \"\"\"\n    if len(golds) != len(preds):\n        raise ValueError('golds and preds must have the same length')\n    return [bleu(gold, pred) for gold, pred in zip(golds, preds)]\n\n\ndef corpus_bleu(golds: List[List[str]], preds: List[List[str]]) -> float:\n    \"\"\"\n    Calculate corpus-level BLEU score for a batch of sentences.\n\n    :param golds: list of gold sentences\n    :param preds: list of predicted sentences\n    :return: corpus-level BLEU score\n    \"\"\"\n    if len(golds) != len(preds):\n        raise ValueError('golds and preds must have the same length')\n    return 100.0 * nltk.translate.bleu_score.corpus_bleu(\n        [[gold] for gold in golds],\n        preds,\n        smoothing_function=nltk.translate.bleu_score.SmoothingFunction().method2,\n        auto_reweigh=True,\n    )\n\n\ndef edit_sim(\n    gold: Union[str, List[str]], pred: Union[str, List[str]], sep: str = ' '\n) -> float:\n    \"\"\"\n    Calculate char-level edit similarity, in the range of 0~100.\n\n    :param gold: gold sentence or list of gold tokens\n    :param pred: predicted sentence or list of predicted tokens\n    :param sep: separator between tokens\n    :return: char-level edit similarity\n    \"\"\"\n    if len(pred) == 0 or len(gold) == 0:\n        return 0.0\n    if isinstance(gold, list):\n        gold = sep.join(gold)\n    if isinstance(pred, list):\n        pred = sep.join(pred)\n    return fuzz.ratio(gold, pred)\n\n\ndef batch_edit_sim(\n    golds: List[Union[str, List[str]]],\n    preds: List[Union[str, List[str]]],\n    sep: str = ' ',\n) -> List[float]:\n    \"\"\"\n    Calculate char-level edit similarity for a batch of sentences.\n\n    :param golds: list of gold sentences\n    :param preds: list of predicted sentences\n    :param sep: separator between tokens\n    :return: list of char-level edit similarity\n    \"\"\"\n    if len(golds) != len(preds):\n        raise ValueError('golds and preds must have the same length')\n    return [edit_sim(gold, pred, sep) for gold, pred in zip(golds, preds)]\n\n\nT = TypeVar('T')\n\n\ndef exact_match(gold: T, pred: T) -> float:\n    \"\"\"\n    Calculate exact match accuracy, in the range of {0, 100}.\n\n    :param gold: gold sentence or list of gold tokens\n    :param pred: predicted sentence or list of predicted tokens\n    :return: exact match accuracy\n    \"\"\"\n    if len(pred) == 0 or len(gold) == 0:\n        return 0.0\n    return 100.0 if gold == pred else 0.0\n\n\ndef batch_exact_match(golds: List[T], preds: List[T]) -> List[float]:\n    \"\"\"\n    Calculate exact match accuracy for a batch of sentences.\n\n    :param golds: list of gold sentences\n    :param preds: list of predicted sentences\n    :return: list of exact match accuracy\n    \"\"\"\n    if len(golds) != len(preds):\n        raise ValueError('golds and preds must have the same length')\n    return [exact_match(gold, pred) for gold, pred in zip(golds, preds)]\n\n\ndef rouge_l(\n    gold: Union[str, List[str]], pred: Union[str, List[str]], sep: str = ' '\n) -> Dict[str, float]:\n    \"\"\"\n    Calculate ROUGE-L F1, precision, and recall scores, in the range of 0~100.\n\n    :param gold: gold sentence or list of gold tokens\n    :param pred: predicted sentence or list of predicted tokens\n    :return: {\"p\": precision, \"r\": recall, \"f\": F1}\n    \"\"\"\n    if len(pred) == 0 or len(gold) == 0:\n        return {'p': 0.0, 'r': 0.0, 'f': 0.0}\n    if isinstance(gold, list):\n        gold = sep.join(gold)\n    if isinstance(pred, list):\n        pred = sep.join(pred)\n    try:\n        rouge = Rouge()\n        scores = rouge.get_scores(hyps=pred, refs=gold, avg=True)\n        return {x: scores['rouge-l'][x] * 100.0 for x in ['p', 'r', 'f']}\n    except ValueError:\n        return {'p': 0.0, 'r': 0.0, 'f': 0.0}\n\n\ndef batch_rouge_l(\n    golds: List[Union[str, List[str]]],\n    preds: List[Union[str, List[str]]],\n    sep: str = ' ',\n) -> Dict[str, List[float]]:\n    \"\"\"\n    Calculate ROUGE-L F1, precision, and recall scores for a batch of sentences.\n\n    :param golds: list of gold sentences\n    :param preds: list of predicted sentences\n    :param sep: separator between tokens\n    :return: list of {\"p\": precision, \"r\": recall, \"f\": F1}\n    \"\"\"\n    if len(golds) != len(preds):\n        raise ValueError('golds and preds must have the same length')\n    scores = [rouge_l(gold, pred, sep) for gold, pred in zip(golds, preds)]\n    return {x: [score[x] for score in scores] for x in ['p', 'r', 'f']}\n\n\ndef accuracy(\n    gold: List[str],\n    pred: List[str],\n    ignore: Optional[Sequence[str]] = None,\n) -> float:\n    \"\"\"\n    Calculate token-level accuracy, in the range of 0~100.\n    If gold and pred are not the same length, the longer one would be truncated.\n\n    :param gold: list of gold tokens\n    :param pred: list of predicted tokens\n    :param ignore: list of (gold) tokens to ignore\n    :return: accuracy\n    \"\"\"\n    if len(pred) == 0 or len(gold) == 0:\n        return 0.0\n    if ignore is None:\n        ignore = []\n    i = 0\n    total = 0\n    match = 0\n    while i < len(gold) and i < len(pred):\n        if gold[i] in ignore:\n            i += 1\n            continue\n        total += 1\n        if gold[i] == pred[i]:\n            match += 1\n        i += 1\n\n    if total == 0:\n        return 0.0\n    return 100.0 * match / total\n\n\ndef batch_accuracy(\n    golds: List[List[str]],\n    preds: List[List[str]],\n    ignore: Optional[Sequence[str]] = None,\n) -> List[float]:\n    \"\"\"\n    Calculate token-level accuracy for a batch of sentences.\n\n    :param golds: list of gold sentences\n    :param preds: list of predicted sentences\n    :param ignore: list of (gold) tokens to ignore\n    :return: list of accuracy\n    \"\"\"\n    if len(golds) != len(preds):\n        raise ValueError('golds and preds must have the same length')\n    return [accuracy(gold, pred, ignore) for gold, pred in zip(golds, preds)]\n\n\ndef first_match_to_topk(\n    first_match_list: List[int], k_values: List[int]\n) -> Dict[int, List[float]]:\n    \"\"\"\n    Calculate top-k accuracy with the first match ranks (1-indexed).\n\n    :param first_match: first match ranks (1-indexed)\n    :param k_values: k values to consider\n    :return: a mapping from k to top-k accuracies (ranging from 0~100)\n    \"\"\"\n    return {k: [100.0 if x <= k else 0.0 for x in first_match_list] for k in k_values}\n\n\ndef pass_at_k(n: int, c: int, k: int) -> float:\n    \"\"\"\n    Sample pass@k metric according to the Codex paper, but in the scale of 0~100.\n    :param n: total number of samples\n    :param c: number of correct samples\n    :param k: k in pass@$k$\n    \"\"\"\n    if n < k or (n - c) < k:\n        # fallback to the (1 - (1-p)^k) formula\n        return (1 - (1 - (c / n)) ** k) * 100\n    else:\n        return (1.0 - np.prod(1.0 - k / np.arange(n - c + 1, n + 1)).item()) * 100\n\n\ndef self_bleu(samples: List[List[str]]) -> float:\n    \"\"\"\n    Calculate self-BLEU among the samples.\n    :param samples: the chosen m samples\n    :return: self-BLEU\n    \"\"\"\n    if len(samples) == 0:\n        return 100.0\n\n    scores = []\n    for i in range(len(samples)):\n        scores.append(\n            100.0\n            * nltk.translate.bleu_score.sentence_bleu(\n                [samples[j] for j in range(len(samples)) if j != i],\n                samples[i],\n                smoothing_function=nltk.translate.bleu_score.SmoothingFunction().method2,\n                auto_reweigh=True,\n            )\n        )\n    return np.mean(scores).item()\n\n\ndef self_edit_distance(samples: List[Union[str, List[str]]], sep=' ') -> float:\n    \"\"\"\n    Calculate self-edit-distance among the samples.\n    :param samples: the chosen m samples\n    :param sep: the separator between tokens\n    :return: self-edit-distance\n    \"\"\"\n    if len(samples) == 0:\n        return 0.0\n\n    scores = []\n    for i in range(len(samples)):\n        sample_i = samples[i]\n        if not isinstance(sample_i, str):\n            sample_i = sep.join(sample_i)\n        for j in range(len(samples)):\n            if i == j:\n                continue\n            sample_j = samples[j]\n            if not isinstance(sample_j, str):\n                sample_j = sep.join(sample_j)\n\n            scores.append(100 - fuzz.ratio(sample_i, sample_j))\n    return np.mean(scores).item()\n\n\nQUALITY_METRICS: Dict[str, Callable[[List[str], List[str]], float]] = {\n    'bleu': bleu,\n    'xmatch': exact_match,\n    'edit-sim': edit_sim,\n    'rouge-f': lambda g, p: rouge_l(g, p)['f'],\n    'rouge-p': lambda g, p: rouge_l(g, p)['p'],\n    'rouge-r': lambda g, p: rouge_l(g, p)['r'],\n}\n",
    "evaluation/benchmarks/testgeneval/prompt.py": "CODEACT_TESTGEN_PROMPT_OLD = \"\"\"Your goal is to generate a high-quality test suite (at least 20+ passing tests) for the code file: {code_file}. Output the test suite at {test_file}\\n'\n\n[current directory: /workspace/{workspace_dir_name}]\n\nIMPORTANT: You should ONLY interact with the environment provided to you AND NEVER ASK FOR HUMAN HELP\n\nIMPORTANT: Follow instructions, if you have < 80 tests you should generate more tests rather than trying to fix the ones you have.\n\nIMPORTANT: Code file to test:\n```python\n{code_src}\n```\n\nHere are additional imports that you may need:\n{imports}\n\nLook at code dependencies (NOT {code_file} since you already have contents) and test files you need context for to write a complete test suite.\n\nAim for 20+ test functions with asserts. Do not hestitate to use the Python interpreter to understand the input output behavior of the code you are testing.\n\nOutput your test suite at {test_file}. Each unit test must be a function starting with test_. Include all your test imports and setup before your first test. Do not include a main method to run the tests. Make sure to make it as comprehensive as possible, try to execute all the methods you saw.\n\nWhen you think you've successfully generated a test suite, run it on for the current project using {coverage_command}.\n\nIf you have few tests GENERATE MORE TESTS rather than trying to fix the ones you have (it is possible to filter out failing tests later).\n\nThen run coverage report -m --include {code_file} to see how well your test suite covers the code under test.\n\nWhen you are trying to improve coverage pick a part of the code that is not covered (indicated by lines on coverage report), examine the code and then\ntry to generate a test for it. Feel free to use a code interpreter to understand the input output behavior. ONLY add tests\nnot remove them.\n\nIf you are unable to see passing and failing tests, FIX YOUR IMPORTS to use the same style as other test files.\n\nYou should NOT modify any existing test case files. You SHOULD add new test in a NEW file to reproduce the issue.\n\nYou should NEVER use web browsing or any other web-based tools.\n\nYou should NEVER install new packages, use existing packages only.\n\nYou should ALWAYS use the default Python interpreter available in the <execute_bash> environment to run code related to the provided issue and/or repository.\n\nYou should ALWAYS use local imports DO NOT import the general library.\n\nWhen you think you have a fully adequate test suite, please run the following command: <execute_bash> exit </execute_bash>.\n\"\"\"\n\nCODEACT_TESTGEN_PROMPT = \"\"\"\nYour goal is to generate a comprehensive, **broad-coverage** test suite for the code below, ensuring you test as many lines and branches as possible on the first attempt.\n\nPlace your test suite in a new file named {test_file}.\n\nIMPORTANT REQUIREMENTS:\n1. **No external help or resources**â€”use only the snippet below.\n2. **Focus on breadth over depth**: cover all major functions, classes, and code paths early to minimize coverage iterations.\n3. Each test function must start with `test_` and use `assert` to verify behavior.\n4. Include only necessary imports (standard library or local).\n5. Do **not** modify existing test filesâ€”create a brand new one. No `main()` or other non-test code.\n6. Produce **at least 20 test functions**; if coverage is lacking, add more tests rather than removing or changing existing ones.\n7. Use the following commands to check coverage:\n   <execute_bash> {coverage_command} </execute_bash>\n   <execute_bash> coverage report -m --include {code_file} </execute_bash>\n   If lines remain uncovered, add new tests targeting them specifically.\n8. When you're satisfied with coverage, finalize by running:\n   <execute_bash> exit </execute_bash>\n\nBelow is the **complete code snippet** to test:\n\n<START_OF_CODE>\n{code_src}\n<END_OF_CODE>\n\nNOTE: if you are testing django, you must use from django.test import SimpleTestCase and class based tests (i.e. class TestSomething(SimpleTestCase)).\nNOTE: if there is an error executing tests you MUST fix it before exiting. DO NOT install new packages.\nNOTE: if outputting a revised test suite REPLACE {test_file} with the revised suite\n\n**Output the final test suite** (20+ tests) for {test_file} in a single code block, no extra commentary. MAKE SURE you run the tests and ensure you can see which tests passed and failed BEFORE exiting.\n\"\"\"\n\nCODEACT_TESTGEN_PROMPT_ITERATE = \"\"\"\nYour goal is to improve the test suite at {test_file} to achieve **broad-coverage** of the code below.\n\nFirst run the test suite.\n\nIf no tests run, then remove {test_file} and create {test_file} with a new suite.\n\nOtherwise, improve it aiming to improve code coverage.\n\nIMPORTANT REQUIREMENTS:\n1. Use the following commands to check coverage (RUN THIS FIRST):\n   <execute_bash> {coverage_command} </execute_bash>\n   <execute_bash> coverage report -m --include {code_file} </execute_bash>\n   If lines remain uncovered, add new tests targeting them specifically.\n2. **No external help or resources**â€”use only the snippet below.\n3. **Focus on breadth over depth**: cover all major functions, classes, and code paths early to minimize coverage iterations.\n4. Each test function must use `assert` to verify behavior.\n5. Include only necessary imports (standard library or local).\n6. Do **not** modify other test files in the repository. No `main()` or other non-test code.\n7. Produce **at least 20 test functions**; if coverage is lacking, add more tests rather than removing or changing existing ones.\n8. When you're satisfied with coverage, finalize by running:\n   <execute_bash> exit </execute_bash>\n\nBelow is the **complete code snippet** to test:\n\n<START_OF_CODE>\n{code_src}\n<END_OF_CODE>\n\nNOTE: if you are testing django, you must use from django.test import SimpleTestCase and class based tests (i.e. class TestSomething(SimpleTestCase)).\nNOTE: if there is an error executing tests you MUST fix it before exiting. DO NOT install new packages.\nNOTE: if outputting a revised test suite REPLACE {test_file} with the revised suite\n\n**Output the final test suite** (20+ tests) for {test_file} in a single code block, no extra commentary. MAKE SURE you run the tests and ensure you can see which tests passed and failed BEFORE exiting.\n\"\"\"\n",
    "evaluation/benchmarks/testgeneval/pygments_utils.py": "import re\n\nfrom pygments.lexers.python import PythonLexer\n\n\ndef tokenize_code(code):\n    lexer = PythonLexer()\n    tokens = process_pygments_tokens(lexer.get_tokens(code))\n    return tokens\n\n\ndef process_pygments_tokens(tokens):\n    new_tokens = []\n\n    for token in tokens:\n        if (\n            str(token[0]) == 'Token.Text'\n            and re.match(r'\\s+', token[1])\n            or str(token[0]) == 'Token.Text.Whitespace'\n        ):\n            continue\n        new_tokens.append(token[1])\n\n    new_tokens_final = []\n    i = 0\n    while i < len(new_tokens) - 2:\n        if (\n            new_tokens[i] == '\"'\n            and new_tokens[i + 1] == 'STR'\n            and new_tokens[i + 2] == '\"'\n        ):\n            new_tokens_final.append('\"STR\"')\n            i = i + 3\n        else:\n            new_tokens_final.append(new_tokens[i])\n            i = i + 1\n\n    for i in range(len(new_tokens) - 2, len(new_tokens)):\n        if i >= 0:\n            new_tokens_final.append(new_tokens[i])\n\n    return new_tokens_final\n",
    "evaluation/benchmarks/testgeneval/report_utils.py": "import json\nimport re\n\n\ndef check_coverage(coverage_output, code_file):\n    json_cov = json.loads(coverage_output)\n    if code_file in json_cov['files'].keys():\n        file_data = json_cov['files'][code_file]\n        return True, file_data['summary']['percent_covered']\n\n    return False, 0\n\n\ndef check_mutation(mutation_output):\n    if 'total jobs: ' in mutation_output:\n        num_mutants = int(mutation_output.split('total jobs: ')[1].split('\\n')[0])\n        final_conf = mutation_output.split('\\n')[-1]\n        if len(final_conf.strip().split(' ')) == 3:\n            low, val, high = final_conf.split(' ')\n            low = float(low)\n            val = float(val)\n            high = float(high)\n\n            confidence_range = high - val\n            mutation_score = 100 - val\n\n            return True, num_mutants, mutation_score, confidence_range\n\n    return False, -1, 0, -1\n\n\ndef count_methods(code_str):\n    \"\"\"\n    Counts the number of methods/functions in a given string of code.\n\n    Args:\n    code_str (str): A string containing code.\n\n    Returns:\n    int: The number of methods/functions found.\n    \"\"\"\n    # Regular expression to find Python function definitions\n    pattern = r'\\bdef\\b\\s+\\w+\\s*\\('\n    matches = re.findall(pattern, code_str)\n    return len(matches)\n\n\ndef get_lines_of_code(code_str):\n    \"\"\"\n    Extracts lines of code from a given string.\n\n    Args:\n    code_str (str): A string containing code.\n\n    Returns:\n    list: A list of lines of code.\n    \"\"\"\n    return len(code_str.strip().split('\\n'))\n",
    "evaluation/benchmarks/testgeneval/run_infer.py": "import asyncio\nimport json\nimport os\nimport tempfile\nimport time\nimport traceback\nfrom typing import Any\n\nimport numpy as np\nimport pandas as pd\nimport toml\nfrom datasets import load_dataset\n\nimport openhands.agenthub\nfrom evaluation.benchmarks.testgeneval.constants import MAP_REPO_VERSION_TO_SPECS\nfrom evaluation.benchmarks.testgeneval.prompt import (\n    CODEACT_TESTGEN_PROMPT,\n    CODEACT_TESTGEN_PROMPT_ITERATE,\n)\nfrom evaluation.benchmarks.testgeneval.utils import get_test_directives\nfrom evaluation.utils.shared import (\n    EvalException,\n    EvalMetadata,\n    EvalOutput,\n    assert_and_raise,\n    codeact_user_response,\n    get_metrics,\n    is_fatal_evaluation_error,\n    make_metadata,\n    prepare_dataset,\n    reset_logger_for_multiprocessing,\n    run_evaluation,\n    update_llm_config_for_completions_logging,\n)\nfrom openhands.controller.state.state import State\nfrom openhands.core.config import (\n    AgentConfig,\n    AppConfig,\n    SandboxConfig,\n    get_llm_config_arg,\n    get_parser,\n)\nfrom openhands.core.logger import openhands_logger as logger\nfrom openhands.core.main import create_runtime, run_controller\nfrom openhands.events.action import CmdRunAction, MessageAction\nfrom openhands.events.observation import CmdOutputObservation, ErrorObservation\nfrom openhands.events.serialization.event import event_to_dict\nfrom openhands.runtime.base import Runtime\nfrom openhands.utils.async_utils import call_async_from_sync\n\nRUN_WITH_BROWSING = os.environ.get('RUN_WITH_BROWSING', 'false').lower() == 'true'\n\nAGENT_CLS_TO_FAKE_USER_RESPONSE_FN = {\n    'CodeActAgent': codeact_user_response,\n}\n\n\ndef _preprocess_instance(d):\n    for key, value in d.items():\n        if isinstance(value, np.ndarray):\n            d[key] = value.tolist()\n    return d\n\n\ndef _get_swebench_workspace_dir_name(instance: pd.Series) -> str:\n    return f'{instance.repo}__{instance.version}'.replace('/', '__')\n\n\ndef get_instruction(instance: pd.Series, metadata: EvalMetadata):\n    # workspace_dir_name = _get_swebench_workspace_dir_name(instance)\n    # Prepare instruction\n    coverage_command = ' '.join(\n        [\n            MAP_REPO_VERSION_TO_SPECS[instance['repo']][instance['version']][\n                'test_cmd'\n            ],\n            *get_test_directives(instance),\n        ]\n    )\n\n    # Testing general agents\n    prompt_to_use = (\n        CODEACT_TESTGEN_PROMPT_ITERATE\n        if instance['full_pred'] is not None\n        else CODEACT_TESTGEN_PROMPT\n    )\n    instruction = prompt_to_use.format(\n        code_file=os.path.join('/testbed', instance.code_file),\n        test_file=os.path.join('/testbed', instance.test_file),\n        coverage_command=coverage_command,\n        code_src=instance['code_src'],\n        imports='\\n'.join(instance.local_imports),\n        workspace_dir_name=_get_swebench_workspace_dir_name(instance),\n    )\n\n    if RUN_WITH_BROWSING:\n        instruction += (\n            '<IMPORTANT!>\\n'\n            'You SHOULD NEVER attempt to browse the web. '\n            '</IMPORTANT!>\\n'\n        )\n\n    return instruction\n\n\n# TODO: migrate all swe-bench docker to ghcr.io/openhands\nDOCKER_IMAGE_PREFIX = os.environ.get('EVAL_DOCKER_IMAGE_PREFIX', 'docker.io/kdjain/')\nlogger.info(f'Using docker image prefix: {DOCKER_IMAGE_PREFIX}')\n\n\ndef get_instance_docker_image(instance_id: str) -> str:\n    image_name = 'sweb.eval.x86_64.' + instance_id\n    image_name = image_name.replace(\n        '__', '_s_'\n    )  # to comply with docker image naming convention\n    return DOCKER_IMAGE_PREFIX.rstrip('/') + '/' + image_name\n\n\ndef get_config(\n    instance: pd.Series,\n    metadata: EvalMetadata,\n) -> AppConfig:\n    # We use a different instance image for the each instance of TestGenEval\n    base_container_image = get_instance_docker_image(instance['instance_id_swebench'])\n    logger.info(\n        f'Using instance container image: {base_container_image}. '\n        f'Please make sure this image exists. '\n        f'Submit an issue on https://github.com/All-Hands-AI/OpenHands if you run into any issues.'\n    )\n\n    config = AppConfig(\n        default_agent=metadata.agent_class,\n        run_as_openhands=False,\n        max_iterations=metadata.max_iterations,\n        runtime=os.environ.get('RUNTIME', 'eventstream'),\n        sandbox=SandboxConfig(\n            base_container_image=base_container_image,\n            enable_auto_lint=True,\n            use_host_network=False,\n            # large enough timeout, since some testcases take very long to run\n            timeout=300,\n            # Add platform to the sandbox config to solve issue 4401\n            platform='linux/amd64',\n            api_key=os.environ.get('ALLHANDS_API_KEY', None),\n            remote_runtime_api_url=os.environ.get(\n                'SANDBOX_REMOTE_RUNTIME_API_URL', 'http://localhost:8000'\n            ),\n            keep_runtime_alive=False,\n            remote_runtime_init_timeout=3600,\n        ),\n        # do not mount workspace\n        workspace_base=None,\n        workspace_mount_path=None,\n    )\n    config.set_llm_config(\n        update_llm_config_for_completions_logging(\n            metadata.llm_config, metadata.eval_output_dir, instance['id']\n        )\n    )\n    agent_config = AgentConfig(\n        codeact_enable_jupyter=False,\n        codeact_enable_browsing=RUN_WITH_BROWSING,\n        codeact_enable_llm_editor=False,\n        condenser=metadata.condenser_config,\n        enable_prompt_extensions=False,\n    )\n    config.set_agent_config(agent_config)\n    return config\n\n\ndef initialize_runtime(\n    runtime: Runtime,\n    instance: pd.Series,  # this argument is not required\n):\n    \"\"\"Initialize the runtime for the agent.\n\n    This function is called before the runtime is used to run the agent.\n    \"\"\"\n    logger.info('-' * 30)\n    logger.info('BEGIN Runtime Initialization Fn')\n    logger.info('-' * 30)\n    workspace_dir_name = _get_swebench_workspace_dir_name(instance)\n    obs: CmdOutputObservation\n\n    instance['instance_id'] = instance['instance_id_swebench']\n\n    # Set instance id\n    action = CmdRunAction(\n        command=f\"\"\"echo 'export SWE_INSTANCE_ID={instance['instance_id_swebench']}' >> ~/.bashrc && echo 'export PIP_CACHE_DIR=~/.cache/pip' >> ~/.bashrc && echo \"alias git='git --no-pager'\" >> ~/.bashrc\"\"\"\n    )\n    action.set_hard_timeout(600)\n    logger.info(action, extra={'msg_type': 'ACTION'})\n    obs = runtime.run_action(action)\n    logger.info(obs, extra={'msg_type': 'OBSERVATION'})\n    assert_and_raise(\n        obs.exit_code == 0, f'Failed to export SWE_INSTANCE_ID: {str(obs)}'\n    )\n\n    action = CmdRunAction(command=\"\"\"export USER=$(whoami); echo USER=${USER} \"\"\")\n    action.set_hard_timeout(600)\n    logger.info(action, extra={'msg_type': 'ACTION'})\n    obs = runtime.run_action(action)\n    logger.info(obs, extra={'msg_type': 'OBSERVATION'})\n    assert_and_raise(obs.exit_code == 0, f'Failed to export USER: {str(obs)}')\n\n    # inject the init script\n    script_dir = os.path.dirname(__file__)\n\n    # inject the instance info\n    action = CmdRunAction(command='mkdir -p /swe_util/eval_data/instances')\n    action.set_hard_timeout(600)\n    logger.info(action, extra={'msg_type': 'ACTION'})\n    obs = runtime.run_action(action)\n    logger.info(obs, extra={'msg_type': 'OBSERVATION'})\n    assert_and_raise(\n        obs.exit_code == 0,\n        f'Failed to create /swe_util/eval_data/instances: {str(obs)}',\n    )\n\n    swe_instance_json_name = 'swe-bench-instance.json'\n    swe_prediction = 'test_suite.py'\n    with tempfile.TemporaryDirectory() as temp_dir:\n        # Construct the full path for the desired file name within the temporary directory\n        temp_file_path = os.path.join(temp_dir, swe_instance_json_name)\n        # Write to the file with the desired name within the temporary directory\n        with open(temp_file_path, 'w') as f:\n            if not isinstance(instance, dict):\n                preprocessed_instance = _preprocess_instance(instance.to_dict())\n                json.dump([preprocessed_instance], f)\n            else:\n                preprocessed_instance = _preprocess_instance(instance)\n                json.dump([preprocessed_instance], f)\n\n        # Copy the file to the desired location\n        runtime.copy_to(temp_file_path, '/swe_util/eval_data/instances/')\n\n        if instance['full_pred'] is not None:\n            temp_file_path_pred = os.path.join(temp_dir, swe_prediction)\n            with open(temp_file_path_pred, 'w') as f:\n                f.write(instance['full_pred'])\n\n            runtime.copy_to(temp_file_path_pred, '/tmp')\n\n            # Copy the file to the desired location\n            action = CmdRunAction(\n                command=f\"cp /tmp/test_suite.py /testbed/{instance['test_file']}\"\n            )\n            action.set_hard_timeout(600)\n            logger.info(action, extra={'msg_type': 'ACTION'})\n            obs = runtime.run_action(action)\n            logger.info(obs, extra={'msg_type': 'OBSERVATION'})\n            assert_and_raise(\n                obs.exit_code == 0, f'Failed to copy test file: {str(obs)}'\n            )\n\n            action = CmdRunAction(\n                command='git -C /testbed add . && git -C /testbed commit -m \"Add test file\"'\n            )\n            action.set_hard_timeout(600)\n            logger.info(action, extra={'msg_type': 'ACTION'})\n            obs = runtime.run_action(action)\n            logger.info(obs, extra={'msg_type': 'OBSERVATION'})\n            assert_and_raise(obs.exit_code == 0, f'Failed to cat ~/.bashrc: {str(obs)}')\n\n    # inject the instance swe entry\n    runtime.copy_to(\n        str(os.path.join(script_dir, 'scripts/setup/instance_swe_entry.sh')),\n        '/swe_util/',\n    )\n    action = CmdRunAction(command='cat ~/.bashrc')\n    action.set_hard_timeout(600)\n    logger.info(action, extra={'msg_type': 'ACTION'})\n    obs = runtime.run_action(action)\n    logger.info(obs, extra={'msg_type': 'OBSERVATION'})\n    assert_and_raise(obs.exit_code == 0, f'Failed to cat ~/.bashrc: {str(obs)}')\n\n    action = CmdRunAction(command='source ~/.bashrc')\n    action.set_hard_timeout(600)\n    logger.info(action, extra={'msg_type': 'ACTION'})\n    obs = runtime.run_action(action)\n    logger.info(obs, extra={'msg_type': 'OBSERVATION'})\n    if isinstance(obs, ErrorObservation):\n        logger.error(f'Failed to source ~/.bashrc: {str(obs)}')\n    assert_and_raise(obs.exit_code == 0, f'Failed to source ~/.bashrc: {str(obs)}')\n\n    action = CmdRunAction(command='source /swe_util/instance_swe_entry.sh')\n    action.set_hard_timeout(600)\n    logger.info(action, extra={'msg_type': 'ACTION'})\n    obs = runtime.run_action(action)\n    logger.info(obs, extra={'msg_type': 'OBSERVATION'})\n    assert_and_raise(\n        obs.exit_code == 0,\n        f'Failed to source /swe_util/instance_swe_entry.sh: {str(obs)}',\n    )\n\n    action = CmdRunAction(command=f'cd /workspace/{workspace_dir_name}')\n    action.set_hard_timeout(600)\n    logger.info(action, extra={'msg_type': 'ACTION'})\n    obs = runtime.run_action(action)\n    logger.info(obs, extra={'msg_type': 'OBSERVATION'})\n    assert_and_raise(\n        obs.exit_code == 0,\n        f'Failed to cd to /workspace/{workspace_dir_name}: {str(obs)}',\n    )\n\n    action = CmdRunAction(command='git reset --hard')\n    action.set_hard_timeout(600)\n    logger.info(action, extra={'msg_type': 'ACTION'})\n    obs = runtime.run_action(action)\n    logger.info(obs, extra={'msg_type': 'OBSERVATION'})\n    assert_and_raise(obs.exit_code == 0, f'Failed to git reset --hard: {str(obs)}')\n\n    action = CmdRunAction(\n        command='for remote_name in $(git remote); do git remote remove \"${remote_name}\"; done'\n    )\n    action.set_hard_timeout(600)\n    logger.info(action, extra={'msg_type': 'ACTION'})\n    obs = runtime.run_action(action)\n    logger.info(obs, extra={'msg_type': 'OBSERVATION'})\n    assert_and_raise(obs.exit_code == 0, f'Failed to remove git remotes: {str(obs)}')\n\n    logger.info('-' * 30)\n    logger.info('END Runtime Initialization Fn')\n    logger.info('-' * 30)\n\n\ndef complete_runtime(\n    runtime: Runtime,\n    instance: pd.Series,  # this argument is not required, but it is used to get the workspace_dir_name\n) -> dict[str, Any]:\n    \"\"\"Complete the runtime for the agent.\n\n    This function is called before the runtime is used to run the agent.\n    If you need to do something in the sandbox to get the correctness metric after\n    the agent has run, modify this function.\n    \"\"\"\n    try:\n        logger.info('-' * 30)\n        logger.info('BEGIN Runtime Completion Fn')\n        logger.info('-' * 30)\n        obs: CmdOutputObservation\n        workspace_dir_name = _get_swebench_workspace_dir_name(instance)\n\n        action = CmdRunAction(command=f'cd /workspace/{workspace_dir_name}')\n        action.set_hard_timeout(600)\n        logger.info(action, extra={'msg_type': 'ACTION'})\n        obs = runtime.run_action(action)\n        logger.info(obs, extra={'msg_type': 'OBSERVATION'})\n        assert_and_raise(\n            obs.exit_code == 0,\n            f'Failed to cd to /workspace/{workspace_dir_name}: {str(obs)}',\n        )\n\n        action = CmdRunAction(command=f'cat {instance.test_file}')\n        action.set_hard_timeout(600)\n        logger.info(action, extra={'msg_type': 'ACTION'})\n        obs = runtime.run_action(action)\n        logger.info(obs, extra={'msg_type': 'OBSERVATION'})\n        assert_and_raise(\n            obs.exit_code == 0,\n            f'Failed to find file: {instance.test_file} in /workspace/{workspace_dir_name}',\n        )\n\n        test_suite = obs.content.strip()\n    except Exception:\n        # Print stack trace\n        print('Skipping, exception in complete_runtime')\n        print(traceback.format_exc())\n        test_suite = instance['full_pred'] if instance['full_pred'] is not None else ''\n\n    # action = CmdRunAction(command='git add -A')\n    # action.set_hard_timeout(600)\n    # logger.info(action, extra={'msg_type': 'ACTION'})\n    # obs = runtime.run_action(action)\n    # logger.info(obs, extra={'msg_type': 'OBSERVATION'})\n    # assert_and_raise(obs.exit_code == 0, f'Failed to git add -A: {str(obs)}')\n\n    logger.info('-' * 30)\n    logger.info('END Runtime Completion Fn')\n    logger.info('-' * 30)\n    return {\n        'test_suite': test_suite,\n    }\n\n\ndef process_instance(\n    instance: pd.Series,\n    metadata: EvalMetadata,\n    reset_logger: bool = True,\n) -> EvalOutput:\n    config = get_config(instance, metadata)\n    start_time = time.time()  # Track start time\n\n    # Setup the logger properly, so you can run multi-processing to parallelize the evaluation\n    if reset_logger:\n        log_dir = os.path.join(metadata.eval_output_dir, 'infer_logs')\n        reset_logger_for_multiprocessing(logger, instance.id, log_dir)\n    else:\n        logger.info(f'Starting evaluation for instance {instance.id}.')\n\n    runtime = create_runtime(config)\n    call_async_from_sync(runtime.connect)\n\n    try:\n        initialize_runtime(runtime, instance)\n\n        instruction = get_instruction(instance, metadata)\n\n        # Here's how you can run the agent (similar to the `main` function) and get the final task state\n        state: State | None = asyncio.run(\n            run_controller(\n                config=config,\n                initial_user_action=MessageAction(content=instruction),\n                runtime=runtime,\n                fake_user_response_fn=AGENT_CLS_TO_FAKE_USER_RESPONSE_FN[\n                    metadata.agent_class\n                ],\n            )\n        )\n\n        # if fatal error, throw EvalError to trigger re-run\n        if is_fatal_evaluation_error(state.last_error):\n            raise EvalException('Fatal error detected: ' + state.last_error)\n\n        # ======= THIS IS SWE-Bench specific =======\n        return_val = complete_runtime(runtime, instance)\n        test_suite = return_val['test_suite']\n        logger.info(\n            f'Got test suite for instance {instance.instance_id}:\\n--------\\n{test_suite}\\n--------'\n        )\n    finally:\n        runtime.close()\n\n    end_time = time.time()\n    elapsed_time = end_time - start_time\n    logger.info(\n        f'Evaluation for instance {instance.instance_id} took {elapsed_time:.2f} seconds.'\n    )\n\n    # ==========================================\n\n    # ======= Attempt to evaluate the agent's edits =======\n    # we use eval_infer.sh to evaluate the agent's edits, not here\n    # because the agent may alter the environment / testcases\n    test_result = {\n        'test_suite': test_suite,\n        'elapsed_time': elapsed_time,\n    }\n\n    # If you are working on some simpler benchmark that only evaluates the final model output (e.g., in a MessageAction)\n    # You can simply get the LAST `MessageAction` from the returned `state.history` and parse it for evaluation.\n    if state is None:\n        raise ValueError('State should not be None.')\n\n    histories = [event_to_dict(event) for event in state.history]\n    metrics = get_metrics(state)\n\n    # Save the output\n    output = EvalOutput(\n        instance_id=instance.id,\n        instruction=instruction,\n        instance=_preprocess_instance(instance.to_dict()),  # SWE Bench specific\n        test_result=test_result,\n        metadata=metadata,\n        history=histories,\n        metrics=metrics,\n        error=state.last_error if state and state.last_error else None,\n    )\n    # print(output)\n    return output\n\n\ndef prepare_dataset_pre(dataset: pd.DataFrame, filter_column: str) -> pd.DataFrame:\n    file_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'config.toml')\n    if os.path.exists(file_path):\n        with open(file_path, 'r') as file:\n            data = toml.load(file)\n            if 'selected_ids' in data:\n                selected_ids = data['selected_ids']\n                logger.info(\n                    f'Filtering {len(selected_ids)} tasks from \"selected_ids\"...'\n                )\n                subset = dataset[dataset[filter_column].isin(selected_ids)]\n                logger.info(f'Retained {subset.shape[0]} tasks after filtering')\n\n                subset['instance_id_swebench'] = subset['instance_id']\n                subset['instance_id'] = subset['id']\n                return subset\n\n    dataset['instance_id_swebench'] = dataset['instance_id']\n    dataset['instance_id'] = dataset['id']\n    return dataset\n\n\nif __name__ == '__main__':\n    parser = get_parser()\n    parser.add_argument(\n        '--dataset',\n        type=str,\n        default='kjain/testgenevallite',\n        help='data set to evaluate on, either full-test or lite-test',\n    )\n    parser.add_argument(\n        '--split',\n        type=str,\n        default='test',\n        help='split to evaluate on',\n    )\n    parser.add_argument(\n        '--testfile_start',\n        action='store_true',\n        help='Whether to start from the 0 shot test file',\n    )\n\n    parser.add_argument(\n        '--zero_shot_path',\n        type=str,\n        help='Path to the zero shot test file predictions',\n    )\n    args, _ = parser.parse_known_args()\n\n    if args.testfile_start and not args.zero_shot_path:\n        raise ValueError(\n            'If you want to start from the 0 shot test file, you must provide the path to the zero shot test file predictions'\n        )\n\n    preds_map = {}\n    if args.testfile_start:\n        with open(args.zero_shot_path, 'r') as f:\n            for line in f:\n                pred = json.loads(line)\n                preds_map[pred['id']] = pred['preds']['full'][0]\n\n    # NOTE: It is preferable to load datasets from huggingface datasets and perform post-processing\n    # so we don't need to manage file uploading to OpenHands's repo\n    dataset = load_dataset(args.dataset, split=args.split)\n    logger.info(f'Loaded dataset {args.dataset} with split {args.split}')\n    testgeneval_filepairs = prepare_dataset_pre(dataset.to_pandas(), 'id')\n\n    llm_config = None\n    if args.llm_config:\n        llm_config = get_llm_config_arg(args.llm_config)\n        llm_config.log_completions = True\n        # modify_params must be False for evaluation purpose, for reproducibility and accurancy of results\n        llm_config.modify_params = False\n\n    if llm_config is None:\n        raise ValueError(f'Could not find LLM config: --llm_config {args.llm_config}')\n\n    details = {}\n    _agent_cls = openhands.agenthub.Agent.get_cls(args.agent_cls)\n\n    dataset_descrption = (\n        args.dataset.replace('/', '__') + '-' + args.split.replace('/', '__')\n    )\n    metadata = make_metadata(\n        llm_config,\n        dataset_descrption,\n        args.agent_cls,\n        args.max_iterations,\n        args.eval_note,\n        args.eval_output_dir,\n        details=details,\n    )\n\n    output_file = os.path.join(metadata.eval_output_dir, 'output.jsonl')\n    instances = prepare_dataset(testgeneval_filepairs, output_file, args.eval_n_limit)\n\n    if not instances.empty:\n        instances['full_pred'] = (\n            instances['instance_id']\n            .map(preds_map)\n            .apply(lambda x: x if pd.notna(x) else None)\n        )\n\n        run_evaluation(\n            instances, metadata, output_file, args.eval_num_workers, process_instance\n        )\n"
  },
  "requirements": null
}