{
  "repo_name": "ageitgey/face_recognition",
  "repo_url": "https://github.com/ageitgey/face_recognition",
  "description": "The world's simplest facial recognition api for Python and the command line",
  "stars": 54375,
  "language": "Python",
  "created_at": "2017-03-03T21:52:39Z",
  "updated_at": "2025-03-19T04:11:13Z",
  "files": {
    "tests/__init__.py": "# -*- coding: utf-8 -*-\n",
    "tests/test_face_recognition.py": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\n\"\"\"\ntest_face_recognition\n----------------------------------\n\nTests for `face_recognition` module.\n\"\"\"\n\n\nimport unittest\nimport os\nimport numpy as np\nfrom click.testing import CliRunner\n\nfrom face_recognition import api\nfrom face_recognition import face_recognition_cli\nfrom face_recognition import face_detection_cli\n\n\nclass Test_face_recognition(unittest.TestCase):\n\n    def test_load_image_file(self):\n        img = api.load_image_file(os.path.join(os.path.dirname(__file__), 'test_images', 'obama.jpg'))\n        self.assertEqual(img.shape, (1137, 910, 3))\n\n    def test_load_image_file_32bit(self):\n        img = api.load_image_file(os.path.join(os.path.dirname(__file__), 'test_images', '32bit.png'))\n        self.assertEqual(img.shape, (1200, 626, 3))\n\n    def test_raw_face_locations(self):\n        img = api.load_image_file(os.path.join(os.path.dirname(__file__), 'test_images', 'obama.jpg'))\n        detected_faces = api._raw_face_locations(img)\n\n        self.assertEqual(len(detected_faces), 1)\n        self.assertEqual(detected_faces[0].top(), 142)\n        self.assertEqual(detected_faces[0].bottom(), 409)\n\n    def test_cnn_raw_face_locations(self):\n        img = api.load_image_file(os.path.join(os.path.dirname(__file__), 'test_images', 'obama.jpg'))\n        detected_faces = api._raw_face_locations(img, model=\"cnn\")\n\n        self.assertEqual(len(detected_faces), 1)\n        self.assertAlmostEqual(detected_faces[0].rect.top(), 144, delta=25)\n        self.assertAlmostEqual(detected_faces[0].rect.bottom(), 389, delta=25)\n\n    def test_raw_face_locations_32bit_image(self):\n        img = api.load_image_file(os.path.join(os.path.dirname(__file__), 'test_images', '32bit.png'))\n        detected_faces = api._raw_face_locations(img)\n\n        self.assertEqual(len(detected_faces), 1)\n        self.assertEqual(detected_faces[0].top(), 290)\n        self.assertEqual(detected_faces[0].bottom(), 558)\n\n    def test_cnn_raw_face_locations_32bit_image(self):\n        img = api.load_image_file(os.path.join(os.path.dirname(__file__), 'test_images', '32bit.png'))\n        detected_faces = api._raw_face_locations(img, model=\"cnn\")\n\n        self.assertEqual(len(detected_faces), 1)\n        self.assertAlmostEqual(detected_faces[0].rect.top(), 259, delta=25)\n        self.assertAlmostEqual(detected_faces[0].rect.bottom(), 552, delta=25)\n\n    def test_face_locations(self):\n        img = api.load_image_file(os.path.join(os.path.dirname(__file__), 'test_images', 'obama.jpg'))\n        detected_faces = api.face_locations(img)\n\n        self.assertEqual(len(detected_faces), 1)\n        self.assertEqual(detected_faces[0], (142, 617, 409, 349))\n\n    def test_cnn_face_locations(self):\n        img = api.load_image_file(os.path.join(os.path.dirname(__file__), 'test_images', 'obama.jpg'))\n        detected_faces = api.face_locations(img, model=\"cnn\")\n\n        self.assertEqual(len(detected_faces), 1)\n        self.assertAlmostEqual(detected_faces[0][0], 144, delta=25)\n        self.assertAlmostEqual(detected_faces[0][1], 608, delta=25)\n        self.assertAlmostEqual(detected_faces[0][2], 389, delta=25)\n        self.assertAlmostEqual(detected_faces[0][3], 363, delta=25)\n\n    def test_partial_face_locations(self):\n        img = api.load_image_file(os.path.join(os.path.dirname(__file__), 'test_images', 'obama_partial_face.jpg'))\n        detected_faces = api.face_locations(img)\n\n        self.assertEqual(len(detected_faces), 1)\n        self.assertEqual(detected_faces[0], (142, 191, 365, 0))\n\n        img = api.load_image_file(os.path.join(os.path.dirname(__file__), 'test_images', 'obama_partial_face2.jpg'))\n        detected_faces = api.face_locations(img)\n\n        self.assertEqual(len(detected_faces), 1)\n        self.assertEqual(detected_faces[0], (142, 551, 409, 349))\n\n    def test_raw_face_locations_batched(self):\n        img = api.load_image_file(os.path.join(os.path.dirname(__file__), 'test_images', 'obama.jpg'))\n        images = [img, img, img]\n        batched_detected_faces = api._raw_face_locations_batched(images, number_of_times_to_upsample=0)\n\n        for detected_faces in batched_detected_faces:\n            self.assertEqual(len(detected_faces), 1)\n            self.assertEqual(detected_faces[0].rect.top(), 154)\n            self.assertEqual(detected_faces[0].rect.bottom(), 390)\n\n    def test_batched_face_locations(self):\n        img = api.load_image_file(os.path.join(os.path.dirname(__file__), 'test_images', 'obama.jpg'))\n        images = [img, img, img]\n\n        batched_detected_faces = api.batch_face_locations(images, number_of_times_to_upsample=0)\n\n        for detected_faces in batched_detected_faces:\n            self.assertEqual(len(detected_faces), 1)\n            self.assertEqual(detected_faces[0], (154, 611, 390, 375))\n\n    def test_raw_face_landmarks(self):\n        img = api.load_image_file(os.path.join(os.path.dirname(__file__), 'test_images', 'obama.jpg'))\n        face_landmarks = api._raw_face_landmarks(img)\n        example_landmark = face_landmarks[0].parts()[10]\n\n        self.assertEqual(len(face_landmarks), 1)\n        self.assertEqual(face_landmarks[0].num_parts, 68)\n        self.assertEqual((example_landmark.x, example_landmark.y), (552, 399))\n\n    def test_face_landmarks(self):\n        img = api.load_image_file(os.path.join(os.path.dirname(__file__), 'test_images', 'obama.jpg'))\n        face_landmarks = api.face_landmarks(img)\n\n        self.assertEqual(\n            set(face_landmarks[0].keys()),\n            set(['chin', 'left_eyebrow', 'right_eyebrow', 'nose_bridge',\n                 'nose_tip', 'left_eye', 'right_eye', 'top_lip',\n                 'bottom_lip']))\n        self.assertEqual(\n            face_landmarks[0]['chin'],\n            [(369, 220), (372, 254), (378, 289), (384, 322), (395, 353),\n             (414, 382), (437, 407), (464, 424), (495, 428), (527, 420),\n             (552, 399), (576, 372), (594, 344), (604, 314), (610, 282),\n             (613, 250), (615, 219)])\n\n    def test_face_landmarks_small_model(self):\n        img = api.load_image_file(os.path.join(os.path.dirname(__file__), 'test_images', 'obama.jpg'))\n        face_landmarks = api.face_landmarks(img, model=\"small\")\n\n        self.assertEqual(\n            set(face_landmarks[0].keys()),\n            set(['nose_tip', 'left_eye', 'right_eye']))\n        self.assertEqual(face_landmarks[0]['nose_tip'], [(496, 295)])\n\n    def test_face_encodings(self):\n        img = api.load_image_file(os.path.join(os.path.dirname(__file__), 'test_images', 'obama.jpg'))\n        encodings = api.face_encodings(img)\n\n        self.assertEqual(len(encodings), 1)\n        self.assertEqual(len(encodings[0]), 128)\n\n    def test_face_encodings_large_model(self):\n        img = api.load_image_file(os.path.join(os.path.dirname(__file__), 'test_images', 'obama.jpg'))\n        encodings = api.face_encodings(img, model='large')\n\n        self.assertEqual(len(encodings), 1)\n        self.assertEqual(len(encodings[0]), 128)\n\n    def test_face_distance(self):\n        img_a1 = api.load_image_file(os.path.join(os.path.dirname(__file__), 'test_images', 'obama.jpg'))\n        img_a2 = api.load_image_file(os.path.join(os.path.dirname(__file__), 'test_images', 'obama2.jpg'))\n        img_a3 = api.load_image_file(os.path.join(os.path.dirname(__file__), 'test_images', 'obama3.jpg'))\n\n        img_b1 = api.load_image_file(os.path.join(os.path.dirname(__file__), 'test_images', 'biden.jpg'))\n\n        face_encoding_a1 = api.face_encodings(img_a1)[0]\n        face_encoding_a2 = api.face_encodings(img_a2)[0]\n        face_encoding_a3 = api.face_encodings(img_a3)[0]\n        face_encoding_b1 = api.face_encodings(img_b1)[0]\n\n        faces_to_compare = [\n            face_encoding_a2,\n            face_encoding_a3,\n            face_encoding_b1]\n\n        distance_results = api.face_distance(faces_to_compare, face_encoding_a1)\n\n        # 0.6 is the default face distance match threshold. So we'll spot-check that the numbers returned\n        # are above or below that based on if they should match (since the exact numbers could vary).\n        self.assertEqual(type(distance_results), np.ndarray)\n        self.assertLessEqual(distance_results[0], 0.6)\n        self.assertLessEqual(distance_results[1], 0.6)\n        self.assertGreater(distance_results[2], 0.6)\n\n    def test_face_distance_empty_lists(self):\n        img = api.load_image_file(os.path.join(os.path.dirname(__file__), 'test_images', 'biden.jpg'))\n        face_encoding = api.face_encodings(img)[0]\n\n        # empty python list\n        faces_to_compare = []\n\n        distance_results = api.face_distance(faces_to_compare, face_encoding)\n        self.assertEqual(type(distance_results), np.ndarray)\n        self.assertEqual(len(distance_results), 0)\n\n        # empty numpy list\n        faces_to_compare = np.array([])\n\n        distance_results = api.face_distance(faces_to_compare, face_encoding)\n        self.assertEqual(type(distance_results), np.ndarray)\n        self.assertEqual(len(distance_results), 0)\n\n    def test_compare_faces(self):\n        img_a1 = api.load_image_file(os.path.join(os.path.dirname(__file__), 'test_images', 'obama.jpg'))\n        img_a2 = api.load_image_file(os.path.join(os.path.dirname(__file__), 'test_images', 'obama2.jpg'))\n        img_a3 = api.load_image_file(os.path.join(os.path.dirname(__file__), 'test_images', 'obama3.jpg'))\n\n        img_b1 = api.load_image_file(os.path.join(os.path.dirname(__file__), 'test_images', 'biden.jpg'))\n\n        face_encoding_a1 = api.face_encodings(img_a1)[0]\n        face_encoding_a2 = api.face_encodings(img_a2)[0]\n        face_encoding_a3 = api.face_encodings(img_a3)[0]\n        face_encoding_b1 = api.face_encodings(img_b1)[0]\n\n        faces_to_compare = [\n            face_encoding_a2,\n            face_encoding_a3,\n            face_encoding_b1]\n\n        match_results = api.compare_faces(faces_to_compare, face_encoding_a1)\n\n        self.assertEqual(type(match_results), list)\n        self.assertTrue(match_results[0])\n        self.assertTrue(match_results[1])\n        self.assertFalse(match_results[2])\n\n    def test_compare_faces_empty_lists(self):\n        img = api.load_image_file(os.path.join(os.path.dirname(__file__), 'test_images', 'biden.jpg'))\n        face_encoding = api.face_encodings(img)[0]\n\n        # empty python list\n        faces_to_compare = []\n\n        match_results = api.compare_faces(faces_to_compare, face_encoding)\n        self.assertEqual(type(match_results), list)\n        self.assertListEqual(match_results, [])\n\n        # empty numpy list\n        faces_to_compare = np.array([])\n\n        match_results = api.compare_faces(faces_to_compare, face_encoding)\n        self.assertEqual(type(match_results), list)\n        self.assertListEqual(match_results, [])\n\n    def test_command_line_interface_options(self):\n        target_string = 'Show this message and exit.'\n        runner = CliRunner()\n        help_result = runner.invoke(face_recognition_cli.main, ['--help'])\n        self.assertEqual(help_result.exit_code, 0)\n        self.assertTrue(target_string in help_result.output)\n\n    def test_command_line_interface(self):\n        target_string = 'obama.jpg,obama'\n        runner = CliRunner()\n        image_folder = os.path.join(os.path.dirname(__file__), 'test_images')\n        image_file = os.path.join(os.path.dirname(__file__), 'test_images', 'obama.jpg')\n\n        result = runner.invoke(face_recognition_cli.main, args=[image_folder, image_file])\n\n        self.assertEqual(result.exit_code, 0)\n        self.assertTrue(target_string in result.output)\n\n    def test_command_line_interface_big_image(self):\n        target_string = 'obama3.jpg,obama'\n        runner = CliRunner()\n        image_folder = os.path.join(os.path.dirname(__file__), 'test_images')\n        image_file = os.path.join(os.path.dirname(__file__), 'test_images', 'obama3.jpg')\n\n        result = runner.invoke(face_recognition_cli.main, args=[image_folder, image_file])\n\n        self.assertEqual(result.exit_code, 0)\n        self.assertTrue(target_string in result.output)\n\n    def test_command_line_interface_tolerance(self):\n        target_string = 'obama.jpg,obama'\n        runner = CliRunner()\n        image_folder = os.path.join(os.path.dirname(__file__), 'test_images')\n        image_file = os.path.join(os.path.dirname(__file__), 'test_images', 'obama.jpg')\n\n        result = runner.invoke(face_recognition_cli.main, args=[image_folder, image_file, \"--tolerance\", \"0.55\"])\n\n        self.assertEqual(result.exit_code, 0)\n        self.assertTrue(target_string in result.output)\n\n    def test_command_line_interface_show_distance(self):\n        target_string = 'obama.jpg,obama,0.0'\n        runner = CliRunner()\n        image_folder = os.path.join(os.path.dirname(__file__), 'test_images')\n        image_file = os.path.join(os.path.dirname(__file__), 'test_images', 'obama.jpg')\n\n        result = runner.invoke(face_recognition_cli.main, args=[image_folder, image_file, \"--show-distance\", \"1\"])\n\n        self.assertEqual(result.exit_code, 0)\n        self.assertTrue(target_string in result.output)\n\n    def test_fd_command_line_interface_options(self):\n        target_string = 'Show this message and exit.'\n        runner = CliRunner()\n        help_result = runner.invoke(face_detection_cli.main, ['--help'])\n        self.assertEqual(help_result.exit_code, 0)\n        self.assertTrue(target_string in help_result.output)\n\n    def test_fd_command_line_interface(self):\n        runner = CliRunner()\n        image_file = os.path.join(os.path.dirname(__file__), 'test_images', 'obama.jpg')\n\n        result = runner.invoke(face_detection_cli.main, args=[image_file])\n        self.assertEqual(result.exit_code, 0)\n        parts = result.output.split(\",\")\n        self.assertTrue(\"obama.jpg\" in parts[0])\n        self.assertEqual(len(parts), 5)\n\n    def test_fd_command_line_interface_folder(self):\n        runner = CliRunner()\n        image_file = os.path.join(os.path.dirname(__file__), 'test_images')\n\n        result = runner.invoke(face_detection_cli.main, args=[image_file])\n        self.assertEqual(result.exit_code, 0)\n        self.assertTrue(\"obama_partial_face2.jpg\" in result.output)\n        self.assertTrue(\"obama.jpg\" in result.output)\n        self.assertTrue(\"obama2.jpg\" in result.output)\n        self.assertTrue(\"obama3.jpg\" in result.output)\n        self.assertTrue(\"biden.jpg\" in result.output)\n\n    def test_fd_command_line_interface_hog_model(self):\n        target_string = 'obama.jpg'\n        runner = CliRunner()\n        image_file = os.path.join(os.path.dirname(__file__), 'test_images', 'obama.jpg')\n\n        result = runner.invoke(face_detection_cli.main, args=[image_file, \"--model\", \"hog\"])\n        self.assertEqual(result.exit_code, 0)\n        self.assertTrue(target_string in result.output)\n\n    def test_fd_command_line_interface_cnn_model(self):\n        target_string = 'obama.jpg'\n        runner = CliRunner()\n        image_file = os.path.join(os.path.dirname(__file__), 'test_images', 'obama.jpg')\n\n        result = runner.invoke(face_detection_cli.main, args=[image_file, \"--model\", \"cnn\"])\n        self.assertEqual(result.exit_code, 0)\n        self.assertTrue(target_string in result.output)\n",
    "docs/conf.py": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n#\n# face_recognition documentation build configuration file, created by\n# sphinx-quickstart on Tue Jul  9 22:26:36 2013.\n#\n# This file is execfile()d with the current directory set to its\n# containing dir.\n#\n# Note that not all possible configuration values are present in this\n# autogenerated file.\n#\n# All configuration values have a default; values that are commented out\n# serve to show the default.\n\nimport sys\nimport os\nfrom unittest.mock import MagicMock\n\nclass Mock(MagicMock):\n    @classmethod\n    def __getattr__(cls, name):\n            return MagicMock()\n\nMOCK_MODULES = ['face_recognition_models', 'Click', 'dlib', 'numpy', 'PIL']\nsys.modules.update((mod_name, Mock()) for mod_name in MOCK_MODULES)\n\n# If extensions (or modules to document with autodoc) are in another\n# directory, add these directories to sys.path here. If the directory is\n# relative to the documentation root, use os.path.abspath to make it\n# absolute, like shown here.\n#sys.path.insert(0, os.path.abspath('.'))\n\n# Get the project root dir, which is the parent dir of this\ncwd = os.getcwd()\nproject_root = os.path.dirname(cwd)\n\n# Insert the project root dir as the first element in the PYTHONPATH.\n# This lets us ensure that the source package is imported, and that its\n# version is used.\nsys.path.insert(0, project_root)\n\nimport face_recognition\n\n# -- General configuration ---------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#needs_sphinx = '1.0'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named 'sphinx.ext.*') or your custom ones.\nextensions = ['sphinx.ext.autodoc', 'sphinx.ext.viewcode']\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = ['_templates']\n\n# The suffix of source filenames.\nsource_suffix = '.rst'\n\n# The encoding of source files.\n#source_encoding = 'utf-8-sig'\n\n# The master toctree document.\nmaster_doc = 'index'\n\n# General information about the project.\nproject = u'Face Recognition'\ncopyright = u\"2017, Adam Geitgey\"\n\n# The version info for the project you're documenting, acts as replacement\n# for |version| and |release|, also used in various other places throughout\n# the built documents.\n#\n# The short X.Y version.\nversion = face_recognition.__version__\n# The full version, including alpha/beta/rc tags.\nrelease = face_recognition.__version__\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#language = None\n\n# There are two options for replacing |today|: either, you set today to\n# some non-false value, then it is used:\n#today = ''\n# Else, today_fmt is used as the format for a strftime call.\n#today_fmt = '%B %d, %Y'\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\nexclude_patterns = ['_build']\n\n# The reST default role (used for this markup: `text`) to use for all\n# documents.\n#default_role = None\n\n# If true, '()' will be appended to :func: etc. cross-reference text.\n#add_function_parentheses = True\n\n# If true, the current module name will be prepended to all description\n# unit titles (such as .. function::).\n#add_module_names = True\n\n# If true, sectionauthor and moduleauthor directives will be shown in the\n# output. They are ignored by default.\n#show_authors = False\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = 'sphinx'\n\n# A list of ignored prefixes for module index sorting.\n#modindex_common_prefix = []\n\n# If true, keep warnings as \"system message\" paragraphs in the built\n# documents.\n#keep_warnings = False\n\n\n# -- Options for HTML output -------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\nhtml_theme = 'default'\n\n# Theme options are theme-specific and customize the look and feel of a\n# theme further.  For a list of options available for each theme, see the\n# documentation.\n#html_theme_options = {}\n\n# Add any paths that contain custom themes here, relative to this directory.\n#html_theme_path = []\n\n# The name for this set of Sphinx documents.  If None, it defaults to\n# \"<project> v<release> documentation\".\n#html_title = None\n\n# A shorter title for the navigation bar.  Default is the same as\n# html_title.\n#html_short_title = None\n\n# The name of an image file (relative to this directory) to place at the\n# top of the sidebar.\n#html_logo = None\n\n# The name of an image file (within the static path) to use as favicon\n# of the docs.  This file should be a Windows icon file (.ico) being\n# 16x16 or 32x32 pixels large.\n#html_favicon = None\n\n# Add any paths that contain custom static files (such as style sheets)\n# here, relative to this directory. They are copied after the builtin\n# static files, so a file named \"default.css\" will overwrite the builtin\n# \"default.css\".\nhtml_static_path = ['_static']\n\n# If not '', a 'Last updated on:' timestamp is inserted at every page\n# bottom, using the given strftime format.\n#html_last_updated_fmt = '%b %d, %Y'\n\n# If true, SmartyPants will be used to convert quotes and dashes to\n# typographically correct entities.\n#html_use_smartypants = True\n\n# Custom sidebar templates, maps document names to template names.\n#html_sidebars = {}\n\n# Additional templates that should be rendered to pages, maps page names\n# to template names.\n#html_additional_pages = {}\n\n# If false, no module index is generated.\n#html_domain_indices = True\n\n# If false, no index is generated.\n#html_use_index = True\n\n# If true, the index is split into individual pages for each letter.\n#html_split_index = False\n\n# If true, links to the reST sources are added to the pages.\n#html_show_sourcelink = True\n\n# If true, \"Created using Sphinx\" is shown in the HTML footer.\n# Default is True.\n#html_show_sphinx = True\n\n# If true, \"(C) Copyright ...\" is shown in the HTML footer.\n# Default is True.\n#html_show_copyright = True\n\n# If true, an OpenSearch description file will be output, and all pages\n# will contain a <link> tag referring to it.  The value of this option\n# must be the base URL from which the finished HTML is served.\n#html_use_opensearch = ''\n\n# This is the file name suffix for HTML files (e.g. \".xhtml\").\n#html_file_suffix = None\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = 'face_recognitiondoc'\n\n\n# -- Options for LaTeX output ------------------------------------------\n\nlatex_elements = {\n    # The paper size ('letterpaper' or 'a4paper').\n    #'papersize': 'letterpaper',\n\n    # The font size ('10pt', '11pt' or '12pt').\n    #'pointsize': '10pt',\n\n    # Additional stuff for the LaTeX preamble.\n    #'preamble': '',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title, author, documentclass\n# [howto/manual]).\nlatex_documents = [\n    ('index', 'face_recognition.tex',\n     u'Face Recognition Documentation',\n     u'Adam Geitgey', 'manual'),\n]\n\n# The name of an image file (relative to this directory) to place at\n# the top of the title page.\n#latex_logo = None\n\n# For \"manual\" documents, if this is true, then toplevel headings\n# are parts, not chapters.\n#latex_use_parts = False\n\n# If true, show page references after internal links.\n#latex_show_pagerefs = False\n\n# If true, show URL addresses after external links.\n#latex_show_urls = False\n\n# Documents to append as an appendix to all manuals.\n#latex_appendices = []\n\n# If false, no module index is generated.\n#latex_domain_indices = True\n\n\n# -- Options for manual page output ------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [\n    ('index', 'face_recognition',\n     u'Face Recognition Documentation',\n     [u'Adam Geitgey'], 1)\n]\n\n# If true, show URL addresses after external links.\n#man_show_urls = False\n\n\n# -- Options for Texinfo output ----------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n    ('index', 'face_recognition',\n     u'Face Recognition Documentation',\n     u'Adam Geitgey',\n     'face_recognition',\n     'One line description of project.',\n     'Miscellaneous'),\n]\n\n# Documents to append as an appendix to all manuals.\n#texinfo_appendices = []\n\n# If false, no module index is generated.\n#texinfo_domain_indices = True\n\n# How to display URL addresses: 'footnote', 'no', or 'inline'.\n#texinfo_show_urls = 'footnote'\n\n# If true, do not generate a @detailmenu in the \"Top\" node's menu.\n#texinfo_no_detailmenu = False\n",
    "examples/benchmark.py": "import timeit\n\n# Note: This example is only tested with Python 3 (not Python 2)\n\n# This is a very simple benchmark to give you an idea of how fast each step of face recognition will run on your system.\n# Notice that face detection gets very slow at large image sizes. So you might consider running face detection on a\n# scaled down version of your image and then running face encodings on the the full size image.\n\nTEST_IMAGES = [\n    \"obama-240p.jpg\",\n    \"obama-480p.jpg\",\n    \"obama-720p.jpg\",\n    \"obama-1080p.jpg\"\n]\n\n\ndef run_test(setup, test, iterations_per_test=5, tests_to_run=10):\n    fastest_execution = min(timeit.Timer(test, setup=setup).repeat(tests_to_run, iterations_per_test))\n    execution_time = fastest_execution / iterations_per_test\n    fps = 1.0 / execution_time\n    return execution_time, fps\n\n\nsetup_locate_faces = \"\"\"\nimport face_recognition\n\nimage = face_recognition.load_image_file(\"{}\")\n\"\"\"\n\ntest_locate_faces = \"\"\"\nface_locations = face_recognition.face_locations(image)\n\"\"\"\n\nsetup_face_landmarks = \"\"\"\nimport face_recognition\n\nimage = face_recognition.load_image_file(\"{}\")\nface_locations = face_recognition.face_locations(image)\n\"\"\"\n\ntest_face_landmarks = \"\"\"\nlandmarks = face_recognition.face_landmarks(image, face_locations=face_locations)[0]\n\"\"\"\n\nsetup_encode_face = \"\"\"\nimport face_recognition\n\nimage = face_recognition.load_image_file(\"{}\")\nface_locations = face_recognition.face_locations(image)\n\"\"\"\n\ntest_encode_face = \"\"\"\nencoding = face_recognition.face_encodings(image, known_face_locations=face_locations)[0]\n\"\"\"\n\nsetup_end_to_end = \"\"\"\nimport face_recognition\n\nimage = face_recognition.load_image_file(\"{}\")\n\"\"\"\n\ntest_end_to_end = \"\"\"\nencoding = face_recognition.face_encodings(image)[0]\n\"\"\"\n\nprint(\"Benchmarks (Note: All benchmarks are only using a single CPU core)\")\nprint()\n\nfor image in TEST_IMAGES:\n    size = image.split(\"-\")[1].split(\".\")[0]\n    print(\"Timings at {}:\".format(size))\n\n    print(\" - Face locations: {:.4f}s ({:.2f} fps)\".format(*run_test(setup_locate_faces.format(image), test_locate_faces)))\n    print(\" - Face landmarks: {:.4f}s ({:.2f} fps)\".format(*run_test(setup_face_landmarks.format(image), test_face_landmarks)))\n    print(\" - Encode face (inc. landmarks): {:.4f}s ({:.2f} fps)\".format(*run_test(setup_encode_face.format(image), test_encode_face)))\n    print(\" - End-to-end: {:.4f}s ({:.2f} fps)\".format(*run_test(setup_end_to_end.format(image), test_end_to_end)))\n    print()\n",
    "examples/blink_detection.py": "#!/usr/bin/env python3\n\n\n# This is a demo of detecting eye status from the users camera. If the users eyes are closed for EYES_CLOSED seconds, the system will start printing out \"EYES CLOSED\"\n# to the terminal until the user presses and holds the spacebar to acknowledge\n\n# this demo must be run with sudo privileges for the keyboard module to work\n\n# PLEASE NOTE: This example requires OpenCV (the `cv2` library) to be installed only to read from your webcam.\n# OpenCV is *not* required to use the face_recognition library. It's only required if you want to run this\n# specific demo. If you have trouble installing it, try any of the other demos that don't require it instead.\n\n# imports\nimport face_recognition\nimport cv2\nimport time\nfrom scipy.spatial import distance as dist\n\nEYES_CLOSED_SECONDS = 5\n\ndef main():\n    closed_count = 0\n    video_capture = cv2.VideoCapture(0)\n\n    ret, frame = video_capture.read(0)\n    # cv2.VideoCapture.release()\n    small_frame = cv2.resize(frame, (0, 0), fx=0.25, fy=0.25)\n    rgb_small_frame = small_frame[:, :, ::-1]\n\n    face_landmarks_list = face_recognition.face_landmarks(rgb_small_frame)\n    process = True\n\n    while True:\n        ret, frame = video_capture.read(0)\n\n        # get it into the correct format\n        small_frame = cv2.resize(frame, (0, 0), fx=0.25, fy=0.25)\n        rgb_small_frame = small_frame[:, :, ::-1]\n\n\n\n        # get the correct face landmarks\n        \n        if process:\n            face_landmarks_list = face_recognition.face_landmarks(rgb_small_frame)\n\n            # get eyes\n            for face_landmark in face_landmarks_list:\n                left_eye = face_landmark['left_eye']\n                right_eye = face_landmark['right_eye']\n\n\n                color = (255,0,0)\n                thickness = 2\n\n                cv2.rectangle(small_frame, left_eye[0], right_eye[-1], color, thickness)\n\n                cv2.imshow('Video', small_frame)\n\n                ear_left = get_ear(left_eye)\n                ear_right = get_ear(right_eye)\n\n                closed = ear_left < 0.2 and ear_right < 0.2\n\n                if (closed):\n                    closed_count += 1\n\n                else:\n                    closed_count = 0\n\n                if (closed_count >= EYES_CLOSED_SECONDS):\n                    asleep = True\n                    while (asleep): #continue this loop until they wake up and acknowledge music\n                        print(\"EYES CLOSED\")\n\n                        if cv2.waitKey(1) == 32: #Wait for space key  \n                            asleep = False\n                            print(\"EYES OPENED\")\n                    closed_count = 0\n\n        process = not process\n        key = cv2.waitKey(1) & 0xFF\n        if key == ord(\"q\"):\n            break\n\ndef get_ear(eye):\n\n\t# compute the euclidean distances between the two sets of\n\t# vertical eye landmarks (x, y)-coordinates\n\tA = dist.euclidean(eye[1], eye[5])\n\tB = dist.euclidean(eye[2], eye[4])\n \n\t# compute the euclidean distance between the horizontal\n\t# eye landmark (x, y)-coordinates\n\tC = dist.euclidean(eye[0], eye[3])\n \n\t# compute the eye aspect ratio\n\tear = (A + B) / (2.0 * C)\n \n\t# return the eye aspect ratio\n\treturn ear\n\nif __name__ == \"__main__\":\n    main()\n\n",
    "examples/blur_faces_on_webcam.py": "import face_recognition\nimport cv2\n\n# This is a demo of blurring faces in video.\n\n# PLEASE NOTE: This example requires OpenCV (the `cv2` library) to be installed only to read from your webcam.\n# OpenCV is *not* required to use the face_recognition library. It's only required if you want to run this\n# specific demo. If you have trouble installing it, try any of the other demos that don't require it instead.\n\n# Get a reference to webcam #0 (the default one)\nvideo_capture = cv2.VideoCapture(0)\n\n# Initialize some variables\nface_locations = []\n\nwhile True:\n    # Grab a single frame of video\n    ret, frame = video_capture.read()\n\n    # Resize frame of video to 1/4 size for faster face detection processing\n    small_frame = cv2.resize(frame, (0, 0), fx=0.25, fy=0.25)\n\n    # Find all the faces and face encodings in the current frame of video\n    face_locations = face_recognition.face_locations(small_frame, model=\"cnn\")\n\n    # Display the results\n    for top, right, bottom, left in face_locations:\n        # Scale back up face locations since the frame we detected in was scaled to 1/4 size\n        top *= 4\n        right *= 4\n        bottom *= 4\n        left *= 4\n\n        # Extract the region of the image that contains the face\n        face_image = frame[top:bottom, left:right]\n\n        # Blur the face image\n        face_image = cv2.GaussianBlur(face_image, (99, 99), 30)\n\n        # Put the blurred face region back into the frame image\n        frame[top:bottom, left:right] = face_image\n\n    # Display the resulting image\n    cv2.imshow('Video', frame)\n\n    # Hit 'q' on the keyboard to quit!\n    if cv2.waitKey(1) & 0xFF == ord('q'):\n        break\n\n# Release handle to the webcam\nvideo_capture.release()\ncv2.destroyAllWindows()\n",
    "examples/digital_makeup.py": "from PIL import Image, ImageDraw\nimport face_recognition\n\n# Load the jpg file into a numpy array\nimage = face_recognition.load_image_file(\"biden.jpg\")\n\n# Find all facial features in all the faces in the image\nface_landmarks_list = face_recognition.face_landmarks(image)\n\npil_image = Image.fromarray(image)\nfor face_landmarks in face_landmarks_list:\n    d = ImageDraw.Draw(pil_image, 'RGBA')\n\n    # Make the eyebrows into a nightmare\n    d.polygon(face_landmarks['left_eyebrow'], fill=(68, 54, 39, 128))\n    d.polygon(face_landmarks['right_eyebrow'], fill=(68, 54, 39, 128))\n    d.line(face_landmarks['left_eyebrow'], fill=(68, 54, 39, 150), width=5)\n    d.line(face_landmarks['right_eyebrow'], fill=(68, 54, 39, 150), width=5)\n\n    # Gloss the lips\n    d.polygon(face_landmarks['top_lip'], fill=(150, 0, 0, 128))\n    d.polygon(face_landmarks['bottom_lip'], fill=(150, 0, 0, 128))\n    d.line(face_landmarks['top_lip'], fill=(150, 0, 0, 64), width=8)\n    d.line(face_landmarks['bottom_lip'], fill=(150, 0, 0, 64), width=8)\n\n    # Sparkle the eyes\n    d.polygon(face_landmarks['left_eye'], fill=(255, 255, 255, 30))\n    d.polygon(face_landmarks['right_eye'], fill=(255, 255, 255, 30))\n\n    # Apply some eyeliner\n    d.line(face_landmarks['left_eye'] + [face_landmarks['left_eye'][0]], fill=(0, 0, 0, 110), width=6)\n    d.line(face_landmarks['right_eye'] + [face_landmarks['right_eye'][0]], fill=(0, 0, 0, 110), width=6)\n\n    pil_image.show()\n",
    "examples/face_recognition_knn.py": "\"\"\"\nThis is an example of using the k-nearest-neighbors (KNN) algorithm for face recognition.\n\nWhen should I use this example?\nThis example is useful when you wish to recognize a large set of known people,\nand make a prediction for an unknown person in a feasible computation time.\n\nAlgorithm Description:\nThe knn classifier is first trained on a set of labeled (known) faces and can then predict the person\nin an unknown image by finding the k most similar faces (images with closet face-features under euclidean distance)\nin its training set, and performing a majority vote (possibly weighted) on their label.\n\nFor example, if k=3, and the three closest face images to the given image in the training set are one image of Biden\nand two images of Obama, The result would be 'Obama'.\n\n* This implementation uses a weighted vote, such that the votes of closer-neighbors are weighted more heavily.\n\nUsage:\n\n1. Prepare a set of images of the known people you want to recognize. Organize the images in a single directory\n   with a sub-directory for each known person.\n\n2. Then, call the 'train' function with the appropriate parameters. Make sure to pass in the 'model_save_path' if you\n   want to save the model to disk so you can re-use the model without having to re-train it.\n\n3. Call 'predict' and pass in your trained model to recognize the people in an unknown image.\n\nNOTE: This example requires scikit-learn to be installed! You can install it with pip:\n\n$ pip3 install scikit-learn\n\n\"\"\"\n\nimport math\nfrom sklearn import neighbors\nimport os\nimport os.path\nimport pickle\nfrom PIL import Image, ImageDraw\nimport face_recognition\nfrom face_recognition.face_recognition_cli import image_files_in_folder\n\nALLOWED_EXTENSIONS = {'png', 'jpg', 'jpeg'}\n\n\ndef train(train_dir, model_save_path=None, n_neighbors=None, knn_algo='ball_tree', verbose=False):\n    \"\"\"\n    Trains a k-nearest neighbors classifier for face recognition.\n\n    :param train_dir: directory that contains a sub-directory for each known person, with its name.\n\n     (View in source code to see train_dir example tree structure)\n\n     Structure:\n        <train_dir>/\n        ├── <person1>/\n        │   ├── <somename1>.jpeg\n        │   ├── <somename2>.jpeg\n        │   ├── ...\n        ├── <person2>/\n        │   ├── <somename1>.jpeg\n        │   └── <somename2>.jpeg\n        └── ...\n\n    :param model_save_path: (optional) path to save model on disk\n    :param n_neighbors: (optional) number of neighbors to weigh in classification. Chosen automatically if not specified\n    :param knn_algo: (optional) underlying data structure to support knn.default is ball_tree\n    :param verbose: verbosity of training\n    :return: returns knn classifier that was trained on the given data.\n    \"\"\"\n    X = []\n    y = []\n\n    # Loop through each person in the training set\n    for class_dir in os.listdir(train_dir):\n        if not os.path.isdir(os.path.join(train_dir, class_dir)):\n            continue\n\n        # Loop through each training image for the current person\n        for img_path in image_files_in_folder(os.path.join(train_dir, class_dir)):\n            image = face_recognition.load_image_file(img_path)\n            face_bounding_boxes = face_recognition.face_locations(image)\n\n            if len(face_bounding_boxes) != 1:\n                # If there are no people (or too many people) in a training image, skip the image.\n                if verbose:\n                    print(\"Image {} not suitable for training: {}\".format(img_path, \"Didn't find a face\" if len(face_bounding_boxes) < 1 else \"Found more than one face\"))\n            else:\n                # Add face encoding for current image to the training set\n                X.append(face_recognition.face_encodings(image, known_face_locations=face_bounding_boxes)[0])\n                y.append(class_dir)\n\n    # Determine how many neighbors to use for weighting in the KNN classifier\n    if n_neighbors is None:\n        n_neighbors = int(round(math.sqrt(len(X))))\n        if verbose:\n            print(\"Chose n_neighbors automatically:\", n_neighbors)\n\n    # Create and train the KNN classifier\n    knn_clf = neighbors.KNeighborsClassifier(n_neighbors=n_neighbors, algorithm=knn_algo, weights='distance')\n    knn_clf.fit(X, y)\n\n    # Save the trained KNN classifier\n    if model_save_path is not None:\n        with open(model_save_path, 'wb') as f:\n            pickle.dump(knn_clf, f)\n\n    return knn_clf\n\n\ndef predict(X_img_path, knn_clf=None, model_path=None, distance_threshold=0.6):\n    \"\"\"\n    Recognizes faces in given image using a trained KNN classifier\n\n    :param X_img_path: path to image to be recognized\n    :param knn_clf: (optional) a knn classifier object. if not specified, model_save_path must be specified.\n    :param model_path: (optional) path to a pickled knn classifier. if not specified, model_save_path must be knn_clf.\n    :param distance_threshold: (optional) distance threshold for face classification. the larger it is, the more chance\n           of mis-classifying an unknown person as a known one.\n    :return: a list of names and face locations for the recognized faces in the image: [(name, bounding box), ...].\n        For faces of unrecognized persons, the name 'unknown' will be returned.\n    \"\"\"\n    if not os.path.isfile(X_img_path) or os.path.splitext(X_img_path)[1][1:] not in ALLOWED_EXTENSIONS:\n        raise Exception(\"Invalid image path: {}\".format(X_img_path))\n\n    if knn_clf is None and model_path is None:\n        raise Exception(\"Must supply knn classifier either thourgh knn_clf or model_path\")\n\n    # Load a trained KNN model (if one was passed in)\n    if knn_clf is None:\n        with open(model_path, 'rb') as f:\n            knn_clf = pickle.load(f)\n\n    # Load image file and find face locations\n    X_img = face_recognition.load_image_file(X_img_path)\n    X_face_locations = face_recognition.face_locations(X_img)\n\n    # If no faces are found in the image, return an empty result.\n    if len(X_face_locations) == 0:\n        return []\n\n    # Find encodings for faces in the test iamge\n    faces_encodings = face_recognition.face_encodings(X_img, known_face_locations=X_face_locations)\n\n    # Use the KNN model to find the best matches for the test face\n    closest_distances = knn_clf.kneighbors(faces_encodings, n_neighbors=1)\n    are_matches = [closest_distances[0][i][0] <= distance_threshold for i in range(len(X_face_locations))]\n\n    # Predict classes and remove classifications that aren't within the threshold\n    return [(pred, loc) if rec else (\"unknown\", loc) for pred, loc, rec in zip(knn_clf.predict(faces_encodings), X_face_locations, are_matches)]\n\n\ndef show_prediction_labels_on_image(img_path, predictions):\n    \"\"\"\n    Shows the face recognition results visually.\n\n    :param img_path: path to image to be recognized\n    :param predictions: results of the predict function\n    :return:\n    \"\"\"\n    pil_image = Image.open(img_path).convert(\"RGB\")\n    draw = ImageDraw.Draw(pil_image)\n\n    for name, (top, right, bottom, left) in predictions:\n        # Draw a box around the face using the Pillow module\n        draw.rectangle(((left, top), (right, bottom)), outline=(0, 0, 255))\n\n        # There's a bug in Pillow where it blows up with non-UTF-8 text\n        # when using the default bitmap font\n        name = name.encode(\"UTF-8\")\n\n        # Draw a label with a name below the face\n        text_width, text_height = draw.textsize(name)\n        draw.rectangle(((left, bottom - text_height - 10), (right, bottom)), fill=(0, 0, 255), outline=(0, 0, 255))\n        draw.text((left + 6, bottom - text_height - 5), name, fill=(255, 255, 255, 255))\n\n    # Remove the drawing library from memory as per the Pillow docs\n    del draw\n\n    # Display the resulting image\n    pil_image.show()\n\n\nif __name__ == \"__main__\":\n    # STEP 1: Train the KNN classifier and save it to disk\n    # Once the model is trained and saved, you can skip this step next time.\n    print(\"Training KNN classifier...\")\n    classifier = train(\"knn_examples/train\", model_save_path=\"trained_knn_model.clf\", n_neighbors=2)\n    print(\"Training complete!\")\n\n    # STEP 2: Using the trained classifier, make predictions for unknown images\n    for image_file in os.listdir(\"knn_examples/test\"):\n        full_file_path = os.path.join(\"knn_examples/test\", image_file)\n\n        print(\"Looking for faces in {}\".format(image_file))\n\n        # Find all people in the image using a trained classifier model\n        # Note: You can pass in either a classifier file name or a classifier model instance\n        predictions = predict(full_file_path, model_path=\"trained_knn_model.clf\")\n\n        # Print results on the console\n        for name, (top, right, bottom, left) in predictions:\n            print(\"- Found {} at ({}, {})\".format(name, left, top))\n\n        # Display results overlaid on an image\n        show_prediction_labels_on_image(os.path.join(\"knn_examples/test\", image_file), predictions)\n",
    "examples/face_recognition_svm.py": "# Train multiple images per person\n# Find and recognize faces in an image using a SVC with scikit-learn\n\n\"\"\"\nStructure:\n        <test_image>.jpg\n        <train_dir>/\n            <person_1>/\n                <person_1_face-1>.jpg\n                <person_1_face-2>.jpg\n                .\n                .\n                <person_1_face-n>.jpg\n           <person_2>/\n                <person_2_face-1>.jpg\n                <person_2_face-2>.jpg\n                .\n                .\n                <person_2_face-n>.jpg\n            .\n            .\n            <person_n>/\n                <person_n_face-1>.jpg\n                <person_n_face-2>.jpg\n                .\n                .\n                <person_n_face-n>.jpg\n\"\"\"\n\nimport face_recognition\nfrom sklearn import svm\nimport os\n\n# Training the SVC classifier\n\n# The training data would be all the face encodings from all the known images and the labels are their names\nencodings = []\nnames = []\n\n# Training directory\ntrain_dir = os.listdir('/train_dir/')\n\n# Loop through each person in the training directory\nfor person in train_dir:\n    pix = os.listdir(\"/train_dir/\" + person)\n\n    # Loop through each training image for the current person\n    for person_img in pix:\n        # Get the face encodings for the face in each image file\n        face = face_recognition.load_image_file(\"/train_dir/\" + person + \"/\" + person_img)\n        face_bounding_boxes = face_recognition.face_locations(face)\n\n        #If training image contains exactly one face\n        if len(face_bounding_boxes) == 1:\n            face_enc = face_recognition.face_encodings(face)[0]\n            # Add face encoding for current image with corresponding label (name) to the training data\n            encodings.append(face_enc)\n            names.append(person)\n        else:\n            print(person + \"/\" + person_img + \" was skipped and can't be used for training\")\n\n# Create and train the SVC classifier\nclf = svm.SVC(gamma='scale')\nclf.fit(encodings,names)\n\n# Load the test image with unknown faces into a numpy array\ntest_image = face_recognition.load_image_file('test_image.jpg')\n\n# Find all the faces in the test image using the default HOG-based model\nface_locations = face_recognition.face_locations(test_image)\nno = len(face_locations)\nprint(\"Number of faces detected: \", no)\n\n# Predict all the faces in the test image using the trained classifier\nprint(\"Found:\")\nfor i in range(no):\n    test_image_enc = face_recognition.face_encodings(test_image)[i]\n    name = clf.predict([test_image_enc])\n    print(*name)\n",
    "examples/facerec_from_video_file.py": "import face_recognition\nimport cv2\n\n# This is a demo of running face recognition on a video file and saving the results to a new video file.\n#\n# PLEASE NOTE: This example requires OpenCV (the `cv2` library) to be installed only to read from your webcam.\n# OpenCV is *not* required to use the face_recognition library. It's only required if you want to run this\n# specific demo. If you have trouble installing it, try any of the other demos that don't require it instead.\n\n# Open the input movie file\ninput_movie = cv2.VideoCapture(\"hamilton_clip.mp4\")\nlength = int(input_movie.get(cv2.CAP_PROP_FRAME_COUNT))\n\n# Create an output movie file (make sure resolution/frame rate matches input video!)\nfourcc = cv2.VideoWriter_fourcc(*'XVID')\noutput_movie = cv2.VideoWriter('output.avi', fourcc, 29.97, (640, 360))\n\n# Load some sample pictures and learn how to recognize them.\nlmm_image = face_recognition.load_image_file(\"lin-manuel-miranda.png\")\nlmm_face_encoding = face_recognition.face_encodings(lmm_image)[0]\n\nal_image = face_recognition.load_image_file(\"alex-lacamoire.png\")\nal_face_encoding = face_recognition.face_encodings(al_image)[0]\n\nknown_faces = [\n    lmm_face_encoding,\n    al_face_encoding\n]\n\n# Initialize some variables\nface_locations = []\nface_encodings = []\nface_names = []\nframe_number = 0\n\nwhile True:\n    # Grab a single frame of video\n    ret, frame = input_movie.read()\n    frame_number += 1\n\n    # Quit when the input video file ends\n    if not ret:\n        break\n\n    # Convert the image from BGR color (which OpenCV uses) to RGB color (which face_recognition uses)\n    rgb_frame = frame[:, :, ::-1]\n\n    # Find all the faces and face encodings in the current frame of video\n    face_locations = face_recognition.face_locations(rgb_frame)\n    face_encodings = face_recognition.face_encodings(rgb_frame, face_locations)\n\n    face_names = []\n    for face_encoding in face_encodings:\n        # See if the face is a match for the known face(s)\n        match = face_recognition.compare_faces(known_faces, face_encoding, tolerance=0.50)\n\n        # If you had more than 2 faces, you could make this logic a lot prettier\n        # but I kept it simple for the demo\n        name = None\n        if match[0]:\n            name = \"Lin-Manuel Miranda\"\n        elif match[1]:\n            name = \"Alex Lacamoire\"\n\n        face_names.append(name)\n\n    # Label the results\n    for (top, right, bottom, left), name in zip(face_locations, face_names):\n        if not name:\n            continue\n\n        # Draw a box around the face\n        cv2.rectangle(frame, (left, top), (right, bottom), (0, 0, 255), 2)\n\n        # Draw a label with a name below the face\n        cv2.rectangle(frame, (left, bottom - 25), (right, bottom), (0, 0, 255), cv2.FILLED)\n        font = cv2.FONT_HERSHEY_DUPLEX\n        cv2.putText(frame, name, (left + 6, bottom - 6), font, 0.5, (255, 255, 255), 1)\n\n    # Write the resulting image to the output video file\n    print(\"Writing frame {} / {}\".format(frame_number, length))\n    output_movie.write(frame)\n\n# All done!\ninput_movie.release()\ncv2.destroyAllWindows()\n"
  },
  "requirements": "face_recognition_models\nClick>=6.0\ndlib>=19.3.0\nnumpy\nPillow\nscipy>=0.17.0\n"
}