{
  "repo_name": "scrapy/scrapy",
  "repo_url": "https://github.com/scrapy/scrapy",
  "description": "Scrapy, a fast high-level web crawling & scraping framework for Python.",
  "stars": 54573,
  "language": "Python",
  "created_at": "2010-02-22T02:01:14Z",
  "updated_at": "2025-03-19T06:37:49Z",
  "files": {
    "conftest.py": "from pathlib import Path\n\nimport pytest\nfrom twisted.web.http import H2_ENABLED\n\nfrom scrapy.utils.reactor import install_reactor\nfrom tests.keys import generate_keys\n\n\ndef _py_files(folder):\n    return (str(p) for p in Path(folder).rglob(\"*.py\"))\n\n\ncollect_ignore = [\n    # not a test, but looks like a test\n    \"scrapy/utils/testproc.py\",\n    \"scrapy/utils/testsite.py\",\n    \"tests/ftpserver.py\",\n    \"tests/mockserver.py\",\n    \"tests/pipelines.py\",\n    \"tests/spiders.py\",\n    # contains scripts to be run by tests/test_crawler.py::CrawlerProcessSubprocess\n    *_py_files(\"tests/CrawlerProcess\"),\n    # contains scripts to be run by tests/test_crawler.py::CrawlerRunnerSubprocess\n    *_py_files(\"tests/CrawlerRunner\"),\n]\n\nbase_dir = Path(__file__).parent\nignore_file_path = base_dir / \"tests\" / \"ignores.txt\"\nwith ignore_file_path.open(encoding=\"utf-8\") as reader:\n    for line in reader:\n        file_path = line.strip()\n        if file_path and file_path[0] != \"#\":\n            collect_ignore.append(file_path)\n\nif not H2_ENABLED:\n    collect_ignore.extend(\n        (\n            \"scrapy/core/downloader/handlers/http2.py\",\n            *_py_files(\"scrapy/core/http2\"),\n        )\n    )\n\n\n@pytest.fixture\ndef chdir(tmpdir):\n    \"\"\"Change to pytest-provided temporary directory\"\"\"\n    tmpdir.chdir()\n\n\ndef pytest_addoption(parser):\n    parser.addoption(\n        \"--reactor\",\n        default=\"asyncio\",\n        choices=[\"default\", \"asyncio\"],\n    )\n\n\n@pytest.fixture(scope=\"class\")\ndef reactor_pytest(request):\n    if not request.cls:\n        # doctests\n        return None\n    request.cls.reactor_pytest = request.config.getoption(\"--reactor\")\n    return request.cls.reactor_pytest\n\n\n@pytest.fixture(autouse=True)\ndef only_asyncio(request, reactor_pytest):\n    if request.node.get_closest_marker(\"only_asyncio\") and reactor_pytest == \"default\":\n        pytest.skip(\"This test is only run without --reactor=default\")\n\n\n@pytest.fixture(autouse=True)\ndef only_not_asyncio(request, reactor_pytest):\n    if (\n        request.node.get_closest_marker(\"only_not_asyncio\")\n        and reactor_pytest != \"default\"\n    ):\n        pytest.skip(\"This test is only run with --reactor=default\")\n\n\n@pytest.fixture(autouse=True)\ndef requires_uvloop(request):\n    if not request.node.get_closest_marker(\"requires_uvloop\"):\n        return\n    try:\n        import uvloop\n\n        del uvloop\n    except ImportError:\n        pytest.skip(\"uvloop is not installed\")\n\n\n@pytest.fixture(autouse=True)\ndef requires_botocore(request):\n    if not request.node.get_closest_marker(\"requires_botocore\"):\n        return\n    try:\n        import botocore\n\n        del botocore\n    except ImportError:\n        pytest.skip(\"botocore is not installed\")\n\n\n@pytest.fixture(autouse=True)\ndef requires_boto3(request):\n    if not request.node.get_closest_marker(\"requires_boto3\"):\n        return\n    try:\n        import boto3\n\n        del boto3\n    except ImportError:\n        pytest.skip(\"boto3 is not installed\")\n\n\ndef pytest_configure(config):\n    if config.getoption(\"--reactor\") != \"default\":\n        install_reactor(\"twisted.internet.asyncioreactor.AsyncioSelectorReactor\")\n    else:\n        # install the reactor explicitly\n        from twisted.internet import reactor  # noqa: F401\n\n\n# Generate localhost certificate files, needed by some tests\ngenerate_keys()\n",
    "docs/conftest.py": "from doctest import ELLIPSIS, NORMALIZE_WHITESPACE\nfrom pathlib import Path\n\nfrom sybil import Sybil\nfrom sybil.parsers.doctest import DocTestParser\nfrom sybil.parsers.skip import skip\n\ntry:\n    # >2.0.1\n    from sybil.parsers.codeblock import PythonCodeBlockParser\nexcept ImportError:\n    from sybil.parsers.codeblock import CodeBlockParser as PythonCodeBlockParser\n\nfrom scrapy.http.response.html import HtmlResponse\n\n\ndef load_response(url: str, filename: str) -> HtmlResponse:\n    input_path = Path(__file__).parent / \"_tests\" / filename\n    return HtmlResponse(url, body=input_path.read_bytes())\n\n\ndef setup(namespace):\n    namespace[\"load_response\"] = load_response\n\n\npytest_collect_file = Sybil(\n    parsers=[\n        DocTestParser(optionflags=ELLIPSIS | NORMALIZE_WHITESPACE),\n        PythonCodeBlockParser(future_imports=[\"print_function\"]),\n        skip,\n    ],\n    pattern=\"*.rst\",\n    setup=setup,\n).pytest()\n",
    "scrapy/utils/test.py": "\"\"\"\nThis module contains some assorted functions used in tests\n\"\"\"\n\nfrom __future__ import annotations\n\nimport asyncio\nimport os\nimport warnings\nfrom importlib import import_module\nfrom pathlib import Path\nfrom posixpath import split\nfrom typing import TYPE_CHECKING, Any, TypeVar\nfrom unittest import TestCase, mock\n\nfrom twisted.trial.unittest import SkipTest\n\nfrom scrapy.exceptions import ScrapyDeprecationWarning\nfrom scrapy.utils.boto import is_botocore_available\nfrom scrapy.utils.deprecate import create_deprecated_class\nfrom scrapy.utils.reactor import is_asyncio_reactor_installed\nfrom scrapy.utils.spider import DefaultSpider\n\nif TYPE_CHECKING:\n    from collections.abc import Awaitable\n\n    from twisted.internet.defer import Deferred\n    from twisted.web.client import Response as TxResponse\n\n    from scrapy import Spider\n    from scrapy.crawler import Crawler\n\n\n_T = TypeVar(\"_T\")\n\n\ndef assert_gcs_environ() -> None:\n    warnings.warn(\n        \"The assert_gcs_environ() function is deprecated and will be removed in a future version of Scrapy.\"\n        \" Check GCS_PROJECT_ID directly.\",\n        category=ScrapyDeprecationWarning,\n        stacklevel=2,\n    )\n    if \"GCS_PROJECT_ID\" not in os.environ:\n        raise SkipTest(\"GCS_PROJECT_ID not found\")\n\n\ndef skip_if_no_boto() -> None:\n    warnings.warn(\n        \"The skip_if_no_boto() function is deprecated and will be removed in a future version of Scrapy.\"\n        \" Check scrapy.utils.boto.is_botocore_available() directly.\",\n        category=ScrapyDeprecationWarning,\n        stacklevel=2,\n    )\n    if not is_botocore_available():\n        raise SkipTest(\"missing botocore library\")\n\n\ndef get_gcs_content_and_delete(\n    bucket: Any, path: str\n) -> tuple[bytes, list[dict[str, str]], Any]:\n    from google.cloud import storage\n\n    warnings.warn(\n        \"The get_gcs_content_and_delete() function is deprecated and will be removed in a future version of Scrapy.\",\n        category=ScrapyDeprecationWarning,\n        stacklevel=2,\n    )\n    client = storage.Client(project=os.environ.get(\"GCS_PROJECT_ID\"))\n    bucket = client.get_bucket(bucket)\n    blob = bucket.get_blob(path)\n    content = blob.download_as_string()\n    acl = list(blob.acl)  # loads acl before it will be deleted\n    bucket.delete_blob(path)\n    return content, acl, blob\n\n\ndef get_ftp_content_and_delete(\n    path: str,\n    host: str,\n    port: int,\n    username: str,\n    password: str,\n    use_active_mode: bool = False,\n) -> bytes:\n    from ftplib import FTP\n\n    warnings.warn(\n        \"The get_ftp_content_and_delete() function is deprecated and will be removed in a future version of Scrapy.\",\n        category=ScrapyDeprecationWarning,\n        stacklevel=2,\n    )\n    ftp = FTP()\n    ftp.connect(host, port)\n    ftp.login(username, password)\n    if use_active_mode:\n        ftp.set_pasv(False)\n    ftp_data: list[bytes] = []\n\n    def buffer_data(data: bytes) -> None:\n        ftp_data.append(data)\n\n    ftp.retrbinary(f\"RETR {path}\", buffer_data)\n    dirname, filename = split(path)\n    ftp.cwd(dirname)\n    ftp.delete(filename)\n    return b\"\".join(ftp_data)\n\n\nTestSpider = create_deprecated_class(\"TestSpider\", DefaultSpider)\n\n\ndef get_reactor_settings() -> dict[str, Any]:\n    \"\"\"Return a settings dict that works with the installed reactor.\n\n    ``Crawler._apply_settings()`` checks that the installed reactor matches the\n    settings, so tests that run the crawler in the current process may need to\n    pass a correct ``\"TWISTED_REACTOR\"`` setting value when creating it.\n    \"\"\"\n    settings: dict[str, Any] = {}\n    if not is_asyncio_reactor_installed():\n        settings[\"TWISTED_REACTOR\"] = None\n    return settings\n\n\ndef get_crawler(\n    spidercls: type[Spider] | None = None,\n    settings_dict: dict[str, Any] | None = None,\n    prevent_warnings: bool = True,\n) -> Crawler:\n    \"\"\"Return an unconfigured Crawler object. If settings_dict is given, it\n    will be used to populate the crawler settings with a project level\n    priority.\n    \"\"\"\n    from scrapy.crawler import CrawlerRunner\n\n    # When needed, useful settings can be added here, e.g. ones that prevent\n    # deprecation warnings.\n    settings: dict[str, Any] = {\n        **get_reactor_settings(),\n        **(settings_dict or {}),\n    }\n    runner = CrawlerRunner(settings)\n    crawler = runner.create_crawler(spidercls or DefaultSpider)\n    crawler._apply_settings()\n    return crawler\n\n\ndef get_pythonpath() -> str:\n    \"\"\"Return a PYTHONPATH suitable to use in processes so that they find this\n    installation of Scrapy\"\"\"\n    scrapy_path = import_module(\"scrapy\").__path__[0]\n    return str(Path(scrapy_path).parent) + os.pathsep + os.environ.get(\"PYTHONPATH\", \"\")\n\n\ndef get_testenv() -> dict[str, str]:\n    \"\"\"Return a OS environment dict suitable to fork processes that need to import\n    this installation of Scrapy, instead of a system installed one.\n    \"\"\"\n    env = os.environ.copy()\n    env[\"PYTHONPATH\"] = get_pythonpath()\n    return env\n\n\ndef assert_samelines(\n    testcase: TestCase, text1: str, text2: str, msg: str | None = None\n) -> None:\n    \"\"\"Asserts text1 and text2 have the same lines, ignoring differences in\n    line endings between platforms\n    \"\"\"\n    warnings.warn(\n        \"The assert_samelines function is deprecated and will be removed in a future version of Scrapy.\",\n        category=ScrapyDeprecationWarning,\n        stacklevel=2,\n    )\n    testcase.assertEqual(text1.splitlines(), text2.splitlines(), msg)  # noqa: PT009\n\n\ndef get_from_asyncio_queue(value: _T) -> Awaitable[_T]:\n    q: asyncio.Queue[_T] = asyncio.Queue()\n    getter = q.get()\n    q.put_nowait(value)\n    return getter\n\n\ndef mock_google_cloud_storage() -> tuple[Any, Any, Any]:\n    \"\"\"Creates autospec mocks for google-cloud-storage Client, Bucket and Blob\n    classes and set their proper return values.\n    \"\"\"\n    from google.cloud.storage import Blob, Bucket, Client\n\n    warnings.warn(\n        \"The mock_google_cloud_storage() function is deprecated and will be removed in a future version of Scrapy.\",\n        category=ScrapyDeprecationWarning,\n        stacklevel=2,\n    )\n\n    client_mock = mock.create_autospec(Client)\n\n    bucket_mock = mock.create_autospec(Bucket)\n    client_mock.get_bucket.return_value = bucket_mock\n\n    blob_mock = mock.create_autospec(Blob)\n    bucket_mock.blob.return_value = blob_mock\n\n    return (client_mock, bucket_mock, blob_mock)\n\n\ndef get_web_client_agent_req(url: str) -> Deferred[TxResponse]:\n    from twisted.internet import reactor\n    from twisted.web.client import Agent  # imports twisted.internet.reactor\n\n    agent = Agent(reactor)\n    return agent.request(b\"GET\", url.encode(\"utf-8\"))\n",
    "scrapy/utils/testproc.py": "from __future__ import annotations\n\nimport os\nimport sys\nimport warnings\nfrom typing import TYPE_CHECKING, cast\n\nfrom twisted.internet.defer import Deferred\nfrom twisted.internet.error import ProcessTerminated\nfrom twisted.internet.protocol import ProcessProtocol\n\nfrom scrapy.exceptions import ScrapyDeprecationWarning\n\nif TYPE_CHECKING:\n    from collections.abc import Iterable\n\n    from twisted.python.failure import Failure\n\n\nwarnings.warn(\n    \"The scrapy.utils.testproc module is deprecated.\",\n    ScrapyDeprecationWarning,\n)\n\n\nclass ProcessTest:\n    command: str | None = None\n    prefix = [sys.executable, \"-m\", \"scrapy.cmdline\"]\n    cwd = os.getcwd()  # trial chdirs to temp dir  # noqa: PTH109\n\n    def execute(\n        self,\n        args: Iterable[str],\n        check_code: bool = True,\n        settings: str | None = None,\n    ) -> Deferred[TestProcessProtocol]:\n        from twisted.internet import reactor\n\n        env = os.environ.copy()\n        if settings is not None:\n            env[\"SCRAPY_SETTINGS_MODULE\"] = settings\n        assert self.command\n        cmd = [*self.prefix, self.command, *args]\n        pp = TestProcessProtocol()\n        pp.deferred.addCallback(self._process_finished, cmd, check_code)\n        reactor.spawnProcess(pp, cmd[0], cmd, env=env, path=self.cwd)\n        return pp.deferred\n\n    def _process_finished(\n        self, pp: TestProcessProtocol, cmd: list[str], check_code: bool\n    ) -> tuple[int, bytes, bytes]:\n        if pp.exitcode and check_code:\n            msg = f\"process {cmd} exit with code {pp.exitcode}\"\n            msg += f\"\\n>>> stdout <<<\\n{pp.out.decode()}\"\n            msg += \"\\n\"\n            msg += f\"\\n>>> stderr <<<\\n{pp.err.decode()}\"\n            raise RuntimeError(msg)\n        return cast(int, pp.exitcode), pp.out, pp.err\n\n\nclass TestProcessProtocol(ProcessProtocol):\n    def __init__(self) -> None:\n        self.deferred: Deferred[TestProcessProtocol] = Deferred()\n        self.out: bytes = b\"\"\n        self.err: bytes = b\"\"\n        self.exitcode: int | None = None\n\n    def outReceived(self, data: bytes) -> None:\n        self.out += data\n\n    def errReceived(self, data: bytes) -> None:\n        self.err += data\n\n    def processEnded(self, status: Failure) -> None:\n        self.exitcode = cast(ProcessTerminated, status.value).exitCode\n        self.deferred.callback(self)\n",
    "scrapy/utils/testsite.py": "import warnings\nfrom urllib.parse import urljoin\n\nfrom twisted.web import resource, server, static, util\n\nfrom scrapy.exceptions import ScrapyDeprecationWarning\n\nwarnings.warn(\n    \"The scrapy.utils.testsite module is deprecated.\",\n    ScrapyDeprecationWarning,\n)\n\n\nclass SiteTest:\n    def setUp(self):\n        from twisted.internet import reactor\n\n        super().setUp()\n        self.site = reactor.listenTCP(0, test_site(), interface=\"127.0.0.1\")\n        self.baseurl = f\"http://localhost:{self.site.getHost().port}/\"\n\n    def tearDown(self):\n        super().tearDown()\n        self.site.stopListening()\n\n    def url(self, path: str) -> str:\n        return urljoin(self.baseurl, path)\n\n\nclass NoMetaRefreshRedirect(util.Redirect):\n    def render(self, request: server.Request) -> bytes:\n        content = util.Redirect.render(self, request)\n        return content.replace(\n            b'http-equiv=\"refresh\"', b'http-no-equiv=\"do-not-refresh-me\"'\n        )\n\n\ndef test_site():\n    r = resource.Resource()\n    r.putChild(b\"text\", static.Data(b\"Works\", \"text/plain\"))\n    r.putChild(\n        b\"html\",\n        static.Data(\n            b\"<body><p class='one'>Works</p><p class='two'>World</p></body>\",\n            \"text/html\",\n        ),\n    )\n    r.putChild(\n        b\"enc-gb18030\",\n        static.Data(b\"<p>gb18030 encoding</p>\", \"text/html; charset=gb18030\"),\n    )\n    r.putChild(b\"redirect\", util.Redirect(b\"/redirected\"))\n    r.putChild(b\"redirect-no-meta-refresh\", NoMetaRefreshRedirect(b\"/redirected\"))\n    r.putChild(b\"redirected\", static.Data(b\"Redirected here\", \"text/plain\"))\n    return server.Site(r)\n\n\nif __name__ == \"__main__\":\n    from twisted.internet import reactor  # pylint: disable=ungrouped-imports\n\n    port = reactor.listenTCP(0, test_site(), interface=\"127.0.0.1\")\n    print(f\"http://localhost:{port.getHost().port}/\")\n    reactor.run()\n",
    "tests/CrawlerProcess/args_settings.py": "from typing import Any\n\nimport scrapy\nfrom scrapy.crawler import Crawler, CrawlerProcess\n\n\nclass NoRequestsSpider(scrapy.Spider):\n    name = \"no_request\"\n\n    @classmethod\n    def from_crawler(cls, crawler: Crawler, *args: Any, **kwargs: Any):\n        spider = super().from_crawler(crawler, *args, **kwargs)\n        spider.settings.set(\"FOO\", kwargs.get(\"foo\"))\n        return spider\n\n    def start_requests(self):\n        self.logger.info(f\"The value of FOO is {self.settings.getint('FOO')}\")\n        return []\n\n\nprocess = CrawlerProcess(settings={})\n\nprocess.crawl(NoRequestsSpider, foo=42)\nprocess.start()\n",
    "tests/CrawlerProcess/asyncio_custom_loop.py": "import scrapy\nfrom scrapy.crawler import CrawlerProcess\n\n\nclass NoRequestsSpider(scrapy.Spider):\n    name = \"no_request\"\n\n    def start_requests(self):\n        return []\n\n\nprocess = CrawlerProcess(\n    settings={\n        \"TWISTED_REACTOR\": \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\",\n        \"ASYNCIO_EVENT_LOOP\": \"uvloop.Loop\",\n    }\n)\nprocess.crawl(NoRequestsSpider)\nprocess.start()\n",
    "tests/CrawlerProcess/asyncio_deferred_signal.py": "from __future__ import annotations\n\nimport asyncio\nimport sys\n\nfrom scrapy import Spider\nfrom scrapy.crawler import CrawlerProcess\nfrom scrapy.utils.defer import deferred_from_coro\n\n\nclass UppercasePipeline:\n    async def _open_spider(self, spider):\n        spider.logger.info(\"async pipeline opened!\")\n        await asyncio.sleep(0.1)\n\n    def open_spider(self, spider):\n        return deferred_from_coro(self._open_spider(spider))\n\n    def process_item(self, item, spider):\n        return {\"url\": item[\"url\"].upper()}\n\n\nclass UrlSpider(Spider):\n    name = \"url_spider\"\n    start_urls = [\"data:,\"]\n    custom_settings = {\n        \"ITEM_PIPELINES\": {UppercasePipeline: 100},\n    }\n\n    def parse(self, response):\n        yield {\"url\": response.url}\n\n\nif __name__ == \"__main__\":\n    ASYNCIO_EVENT_LOOP: str | None\n    try:\n        ASYNCIO_EVENT_LOOP = sys.argv[1]\n    except IndexError:\n        ASYNCIO_EVENT_LOOP = None\n\n    process = CrawlerProcess(\n        settings={\n            \"TWISTED_REACTOR\": \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\",\n            \"ASYNCIO_EVENT_LOOP\": ASYNCIO_EVENT_LOOP,\n        }\n    )\n    process.crawl(UrlSpider)\n    process.start()\n",
    "tests/CrawlerProcess/asyncio_enabled_no_reactor.py": "import scrapy\nfrom scrapy.crawler import CrawlerProcess\nfrom scrapy.utils.reactor import is_asyncio_reactor_installed\n\n\nclass ReactorCheckExtension:\n    def __init__(self):\n        if not is_asyncio_reactor_installed():\n            raise RuntimeError(\"ReactorCheckExtension requires the asyncio reactor.\")\n\n\nclass NoRequestsSpider(scrapy.Spider):\n    name = \"no_request\"\n\n    def start_requests(self):\n        return []\n\n\nprocess = CrawlerProcess(\n    settings={\n        \"TWISTED_REACTOR\": \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\",\n        \"EXTENSIONS\": {ReactorCheckExtension: 0},\n    }\n)\nprocess.crawl(NoRequestsSpider)\nprocess.start()\n",
    "tests/CrawlerProcess/asyncio_enabled_reactor.py": "import scrapy\nfrom scrapy.crawler import CrawlerProcess\nfrom scrapy.utils.reactor import (\n    install_reactor,\n    is_asyncio_reactor_installed,\n    is_reactor_installed,\n)\n\nif is_reactor_installed():\n    raise RuntimeError(\n        \"Reactor already installed before is_asyncio_reactor_installed().\"\n    )\n\ntry:\n    is_asyncio_reactor_installed()\nexcept RuntimeError:\n    pass\nelse:\n    raise RuntimeError(\"is_asyncio_reactor_installed() did not raise RuntimeError.\")\n\nif is_reactor_installed():\n    raise RuntimeError(\n        \"Reactor already installed after is_asyncio_reactor_installed().\"\n    )\n\ninstall_reactor(\"twisted.internet.asyncioreactor.AsyncioSelectorReactor\")\n\nif not is_asyncio_reactor_installed():\n    raise RuntimeError(\"Wrong reactor installed after install_reactor().\")\n\n\nclass ReactorCheckExtension:\n    def __init__(self):\n        if not is_asyncio_reactor_installed():\n            raise RuntimeError(\"ReactorCheckExtension requires the asyncio reactor.\")\n\n\nclass NoRequestsSpider(scrapy.Spider):\n    name = \"no_request\"\n\n    def start_requests(self):\n        return []\n\n\nprocess = CrawlerProcess(\n    settings={\n        \"TWISTED_REACTOR\": \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\",\n        \"EXTENSIONS\": {ReactorCheckExtension: 0},\n    }\n)\nprocess.crawl(NoRequestsSpider)\nprocess.start()\n"
  },
  "requirements": null
}