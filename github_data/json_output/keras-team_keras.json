{
  "repo_name": "keras-team/keras",
  "repo_url": "https://github.com/keras-team/keras",
  "description": "Deep Learning for humans",
  "stars": 62709,
  "language": "Python",
  "created_at": "2015-03-28T00:35:42Z",
  "updated_at": "2025-03-19T06:47:45Z",
  "files": {
    "conftest.py": "import os\n\n# When using jax.experimental.enable_x64 in unit test, we want to keep the\n# default dtype with 32 bits, aligning it with Keras's default.\nos.environ[\"JAX_DEFAULT_DTYPE_BITS\"] = \"32\"\n\ntry:\n    # When using torch and tensorflow, torch needs to be imported first,\n    # otherwise it will segfault upon import. This should force the torch\n    # import to happen first for all tests.\n    import torch  # noqa: F401\nexcept ImportError:\n    pass\n\nimport pytest  # noqa: E402\n\nfrom keras.src.backend import backend  # noqa: E402\n\n\ndef pytest_configure(config):\n    config.addinivalue_line(\n        \"markers\",\n        \"requires_trainable_backend: mark test for trainable backend only\",\n    )\n\n\ndef pytest_collection_modifyitems(config, items):\n    openvino_skipped_tests = []\n    if backend() == \"openvino\":\n        with open(\n            \"keras/src/backend/openvino/excluded_concrete_tests.txt\", \"r\"\n        ) as file:\n            openvino_skipped_tests = file.readlines()\n            # it is necessary to check if stripped line is not empty\n            # and exclude such lines\n            openvino_skipped_tests = [\n                line.strip() for line in openvino_skipped_tests if line.strip()\n            ]\n\n    requires_trainable_backend = pytest.mark.skipif(\n        backend() == \"numpy\" or backend() == \"openvino\",\n        reason=\"Trainer not implemented for NumPy and OpenVINO backend.\",\n    )\n    for item in items:\n        if \"requires_trainable_backend\" in item.keywords:\n            item.add_marker(requires_trainable_backend)\n        # also, skip concrete tests for openvino, listed in the special file\n        # this is more granular mechanism to exclude tests rather\n        # than using --ignore option\n        for skipped_test in openvino_skipped_tests:\n            if skipped_test in item.nodeid:\n                item.add_marker(\n                    skip_if_backend(\n                        \"openvino\",\n                        \"Not supported operation by openvino backend\",\n                    )\n                )\n\n\ndef skip_if_backend(given_backend, reason):\n    return pytest.mark.skipif(backend() == given_backend, reason=reason)\n",
    "integration_tests/basic_full_flow.py": "import numpy as np\nimport pytest\n\nimport keras\nfrom keras.src import layers\nfrom keras.src import losses\nfrom keras.src import metrics\nfrom keras.src import optimizers\nfrom keras.src import testing\n\n\nclass MyModel(keras.Model):\n    def __init__(self, hidden_dim, output_dim, **kwargs):\n        super().__init__(**kwargs)\n        self.hidden_dim = hidden_dim\n        self.output_dim = output_dim\n        self.dense1 = layers.Dense(hidden_dim, activation=\"relu\")\n        self.dense2 = layers.Dense(hidden_dim, activation=\"relu\")\n        self.dense3 = layers.Dense(output_dim)\n\n    def call(self, x):\n        x = self.dense1(x)\n        x = self.dense2(x)\n        return self.dense3(x)\n\n\nclass BasicFlowTest(testing.TestCase):\n    @pytest.mark.requires_trainable_backend\n    def test_basic_fit(self):\n        model = MyModel(hidden_dim=2, output_dim=1)\n\n        x = np.random.random((128, 4))\n        y = np.random.random((128, 4))\n        batch_size = 32\n        epochs = 3\n\n        model.compile(\n            optimizer=optimizers.SGD(learning_rate=0.001),\n            loss=losses.MeanSquaredError(),\n            metrics=[metrics.MeanSquaredError()],\n        )\n        output_before_fit = model(x)\n        model.fit(\n            x, y, batch_size=batch_size, epochs=epochs, validation_split=0.2\n        )\n        output_after_fit = model(x)\n\n        self.assertNotAllClose(output_before_fit, output_after_fit)\n\n    def test_basic_fit_no_training(self):\n        model = MyModel(hidden_dim=2, output_dim=1)\n        x = np.random.random((128, 4))\n        model.predict(x)\n        model(x)\n",
    "integration_tests/dataset_tests/boston_housing_test.py": "from keras.src import testing\nfrom keras.src.datasets import boston_housing\n\n\nclass BostonHousingTest(testing.TestCase):\n    def test_load_data(self):\n        (x_train, y_train), (x_test, y_test) = boston_housing.load_data()\n        self.assertEqual(x_train.shape[1], 13)\n        self.assertEqual(x_train.shape[0] + x_test.shape[0], 506)\n\n    def test_seed_reproducibility(self):\n        seed = 123\n        first_load = boston_housing.load_data(seed=seed)\n        second_load = boston_housing.load_data(seed=seed)\n        self.assertAllClose(first_load[0][0], second_load[0][0])\n        self.assertAllClose(first_load[1][0], second_load[1][0])\n\n    def test_invalid_test_split(self):\n        with self.assertRaises(AssertionError):\n            boston_housing.load_data(test_split=-0.1)\n        with self.assertRaises(AssertionError):\n            boston_housing.load_data(test_split=1.0)\n",
    "integration_tests/dataset_tests/california_housing_test.py": "from keras.src import testing\nfrom keras.src.datasets import california_housing\n\n\nclass CaliforniaHousingTest(testing.TestCase):\n    def test_load_data_large(self):\n        (x_train, y_train), (x_test, y_test) = california_housing.load_data(\n            version=\"large\"\n        )\n        self.assertEqual(x_train.shape[1], 8)\n        # Ensure the dataset contains 20,640 samples as documented\n        self.assertEqual(x_train.shape[0] + x_test.shape[0], 20640)\n\n    def test_load_data_small(self):\n        (x_train, y_train), (x_test, y_test) = california_housing.load_data(\n            version=\"small\"\n        )\n        self.assertEqual(x_train.shape[1], 8)\n        # Ensure the small dataset contains 600 samples as documented\n        self.assertEqual(x_train.shape[0] + x_test.shape[0], 600)\n\n    def test_invalid_version(self):\n        with self.assertRaises(ValueError):\n            california_housing.load_data(version=\"invalid_version\")\n\n    def test_seed_reproducibility(self):\n        # Ensure the data is reproducible with the same seed\n        seed = 123\n        first_load = california_housing.load_data(version=\"large\", seed=seed)\n        second_load = california_housing.load_data(version=\"large\", seed=seed)\n        self.assertAllClose(first_load[0][0], second_load[0][0])\n        self.assertAllClose(first_load[1][0], second_load[1][0])\n",
    "integration_tests/dataset_tests/cifar100_test.py": "import numpy as np\n\nfrom keras.src import testing\nfrom keras.src.datasets import cifar100\n\n\nclass Cifar100LoadDataTest(testing.TestCase):\n    def test_shapes_fine_label_mode(self):\n        (x_train, y_train), (x_test, y_test) = cifar100.load_data(\n            label_mode=\"fine\"\n        )\n        self.assertEqual(x_train.shape, (50000, 32, 32, 3))\n        self.assertEqual(y_train.shape, (50000, 1))\n        self.assertEqual(x_test.shape, (10000, 32, 32, 3))\n        self.assertEqual(y_test.shape, (10000, 1))\n\n    def test_shapes_coarse_label_mode(self):\n        (x_train, y_train), (x_test, y_test) = cifar100.load_data(\n            label_mode=\"coarse\"\n        )\n        self.assertEqual(x_train.shape, (50000, 32, 32, 3))\n        self.assertEqual(y_train.shape, (50000, 1))\n        self.assertEqual(x_test.shape, (10000, 32, 32, 3))\n        self.assertEqual(y_test.shape, (10000, 1))\n\n    def test_dtypes(self):\n        (x_train, y_train), (x_test, y_test) = cifar100.load_data()\n        self.assertEqual(x_train.dtype, np.uint8)\n        self.assertEqual(y_train.dtype, np.int64)\n        self.assertEqual(x_test.dtype, np.uint8)\n        self.assertEqual(y_test.dtype, np.int64)\n\n    def test_invalid_label_mode(self):\n        with self.assertRaises(ValueError):\n            cifar100.load_data(label_mode=\"invalid\")\n",
    "integration_tests/dataset_tests/cifar10_test.py": "import numpy as np\n\nfrom keras.src import testing\nfrom keras.src.datasets import cifar10\n\n\nclass Cifar10LoadDataTest(testing.TestCase):\n    def test_x_train_shape(self):\n        (x_train, _), _ = cifar10.load_data()\n        self.assertEqual(x_train.shape, (50000, 32, 32, 3))\n\n    def test_y_train_shape(self):\n        (_, y_train), _ = cifar10.load_data()\n        self.assertEqual(y_train.shape, (50000, 1))\n\n    def test_x_test_shape(self):\n        _, (x_test, _) = cifar10.load_data()\n        self.assertEqual(x_test.shape, (10000, 32, 32, 3))\n\n    def test_y_test_shape(self):\n        _, (_, y_test) = cifar10.load_data()\n        self.assertEqual(y_test.shape, (10000, 1))\n\n    def test_x_train_dtype(self):\n        (x_train, _), _ = cifar10.load_data()\n        self.assertEqual(x_train.dtype, np.uint8)\n\n    def test_y_train_dtype(self):\n        (_, y_train), _ = cifar10.load_data()\n        self.assertEqual(y_train.dtype, np.uint8)\n\n    def test_x_test_dtype(self):\n        _, (x_test, _) = cifar10.load_data()\n        self.assertEqual(x_test.dtype, np.uint8)\n\n    def test_y_test_dtype(self):\n        _, (_, y_test) = cifar10.load_data()\n        self.assertEqual(y_test.dtype, np.uint8)\n",
    "integration_tests/dataset_tests/fashion_mnist_test.py": "import numpy as np\n\nfrom keras.src import testing\nfrom keras.src.datasets import fashion_mnist\n\n\nclass FashionMnistLoadDataTest(testing.TestCase):\n    def test_x_train_shape(self):\n        (x_train, _), _ = fashion_mnist.load_data()\n        self.assertEqual(x_train.shape, (60000, 28, 28))\n\n    def test_y_train_shape(self):\n        (_, y_train), _ = fashion_mnist.load_data()\n        self.assertEqual(y_train.shape, (60000,))\n\n    def test_x_test_shape(self):\n        _, (x_test, _) = fashion_mnist.load_data()\n        self.assertEqual(x_test.shape, (10000, 28, 28))\n\n    def test_y_test_shape(self):\n        _, (_, y_test) = fashion_mnist.load_data()\n        self.assertEqual(y_test.shape, (10000,))\n\n    def test_x_train_dtype(self):\n        (x_train, _), _ = fashion_mnist.load_data()\n        self.assertEqual(x_train.dtype, np.uint8)\n\n    def test_y_train_dtype(self):\n        (_, y_train), _ = fashion_mnist.load_data()\n        self.assertEqual(y_train.dtype, np.uint8)\n\n    def test_x_test_dtype(self):\n        _, (x_test, _) = fashion_mnist.load_data()\n        self.assertEqual(x_test.dtype, np.uint8)\n\n    def test_y_test_dtype(self):\n        _, (_, y_test) = fashion_mnist.load_data()\n        self.assertEqual(y_test.dtype, np.uint8)\n",
    "integration_tests/dataset_tests/imdb_test.py": "import numpy as np\n\nfrom keras.src import testing\nfrom keras.src.datasets import imdb\n\n\nclass ImdbLoadDataTest(testing.TestCase):\n    def test_load_data_default(self):\n        (x_train, y_train), (x_test, y_test) = imdb.load_data()\n        self.assertIsInstance(x_train, np.ndarray)\n        self.assertIsInstance(y_train, np.ndarray)\n        self.assertIsInstance(x_test, np.ndarray)\n        self.assertIsInstance(y_test, np.ndarray)\n\n        # Check lengths\n        self.assertEqual(len(x_train), 25000)\n        self.assertEqual(len(y_train), 25000)\n        self.assertEqual(len(x_test), 25000)\n        self.assertEqual(len(y_test), 25000)\n\n        # Check types within lists for x\n        self.assertIsInstance(x_train[0], list)\n        self.assertIsInstance(x_test[0], list)\n\n    def test_num_words(self):\n        # Only consider the top 1000 words\n        (x_train, _), _ = imdb.load_data(num_words=1000)\n        # Ensure that no word index exceeds 999 (0-based indexing)\n        max_index = max(max(sequence) for sequence in x_train if sequence)\n        self.assertLessEqual(max_index, 999)\n\n    def test_skip_top(self):\n        # Skip the top 10 most frequent words\n        (x_train, _), _ = imdb.load_data(skip_top=10, num_words=1000)\n        # Check if top 10 words are skipped properly\n        self.assertNotIn(1, x_train[0])  # Assuming 1 is among top 10\n\n    def test_maxlen(self):\n        # Only consider sequences shorter than 100\n        (x_train, _), _ = imdb.load_data(maxlen=100)\n        self.assertTrue(all(len(seq) <= 100 for seq in x_train))\n\n    def test_get_word_index(self):\n        word_index = imdb.get_word_index()\n        self.assertIsInstance(word_index, dict)\n        # Check if word_index contains specific known words\n        self.assertIn(\"the\", word_index)\n        self.assertIn(\"and\", word_index)\n",
    "integration_tests/dataset_tests/mnist_test.py": "import numpy as np\n\nfrom keras.src import testing\nfrom keras.src.datasets import mnist\n\n\nclass MnistLoadDataTest(testing.TestCase):\n    def test_x_train_shape(self):\n        (x_train, _), _ = mnist.load_data()\n        self.assertEqual(x_train.shape, (60000, 28, 28))\n\n    def test_y_train_shape(self):\n        (_, y_train), _ = mnist.load_data()\n        self.assertEqual(y_train.shape, (60000,))\n\n    def test_x_test_shape(self):\n        _, (x_test, _) = mnist.load_data()\n        self.assertEqual(x_test.shape, (10000, 28, 28))\n\n    def test_y_test_shape(self):\n        _, (_, y_test) = mnist.load_data()\n        self.assertEqual(y_test.shape, (10000,))\n\n    def test_x_train_dtype(self):\n        (x_train, _), _ = mnist.load_data()\n        self.assertEqual(x_train.dtype, np.uint8)\n\n    def test_y_train_dtype(self):\n        (_, y_train), _ = mnist.load_data()\n        self.assertEqual(y_train.dtype, np.uint8)\n\n    def test_x_test_dtype(self):\n        _, (x_test, _) = mnist.load_data()\n        self.assertEqual(x_test.dtype, np.uint8)\n\n    def test_y_test_dtype(self):\n        _, (_, y_test) = mnist.load_data()\n        self.assertEqual(y_test.dtype, np.uint8)\n",
    "integration_tests/dataset_tests/reuters_test.py": "import numpy as np\n\nfrom keras.src import testing\nfrom keras.src.datasets import reuters\n\n\nclass ReutersLoadDataTest(testing.TestCase):\n    def test_load_data_default(self):\n        (x_train, y_train), (x_test, y_test) = reuters.load_data()\n        # Check types\n        self.assertIsInstance(x_train, np.ndarray)\n        self.assertIsInstance(y_train, np.ndarray)\n        self.assertIsInstance(x_test, np.ndarray)\n        self.assertIsInstance(y_test, np.ndarray)\n\n        # Check shapes\n        self.assertGreater(len(x_train), 0)\n        self.assertEqual(len(x_train), len(y_train))\n        self.assertGreater(len(x_test), 0)\n        self.assertEqual(len(x_test), len(y_test))\n\n    def test_num_words(self):\n        # Only consider the top 1000 words\n        (x_train, _), _ = reuters.load_data(num_words=1000)\n        # Ensure no word index exceeds 999 (0-based indexing)\n        max_index = max(max(sequence) for sequence in x_train if sequence)\n        self.assertLessEqual(max_index, 999)\n\n    def test_skip_top(self):\n        # Skip the top 10 most frequent words\n        (x_train, _), _ = reuters.load_data(skip_top=10, num_words=1000)\n        # Assuming 1 is among top 10, check if it's skipped\n        self.assertNotIn(1, x_train[0])\n\n    def test_maxlen(self):\n        # Only consider sequences shorter than 50\n        (x_train, _), _ = reuters.load_data(maxlen=50)\n        self.assertTrue(all(len(seq) <= 50 for seq in x_train))\n\n    def test_get_word_index(self):\n        word_index = reuters.get_word_index()\n        self.assertIsInstance(word_index, dict)\n        # Check if word_index contains specific known words\n        self.assertIn(\"the\", word_index)\n\n    def test_get_label_names(self):\n        label_names = reuters.get_label_names()\n        self.assertIsInstance(label_names, tuple)\n        # Check if the tuple contains specific known labels\n        self.assertIn(\"earn\", label_names)\n        self.assertIn(\"acq\", label_names)\n"
  },
  "requirements": "# Tensorflow.\ntensorflow-cpu~=2.18.0;sys_platform != 'darwin'\ntensorflow~=2.18.0;sys_platform == 'darwin'\ntf_keras\ntf2onnx\n\n# Torch.\n--extra-index-url https://download.pytorch.org/whl/cpu\ntorch==2.6.0+cpu\ntorch-xla==2.6.0;sys_platform != 'darwin'\n\n# Jax.\n# Pinned to 0.5.0 on CPU. JAX 0.5.1 requires Tensorflow 2.19 for saved_model_test.\n# Note that we test against the latest JAX on GPU.\njax[cpu]==0.5.0\nflax\n\n# Common deps.\n-r requirements-common.txt\n"
}