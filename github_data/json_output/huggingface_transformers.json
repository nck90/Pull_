{
  "repo_name": "huggingface/transformers",
  "repo_url": "https://github.com/huggingface/transformers",
  "description": "ðŸ¤— Transformers: State-of-the-art Machine Learning for Pytorch, TensorFlow, and JAX.",
  "stars": 141465,
  "language": "Python",
  "created_at": "2018-10-29T13:56:00Z",
  "updated_at": "2025-03-19T07:07:02Z",
  "files": {
    ".circleci/parse_test_outputs.py": "import re\nimport argparse\n\ndef parse_pytest_output(file_path):\n    skipped_tests = {}\n    skipped_count = 0\n    with open(file_path, 'r') as file:\n        for line in file:\n            match = re.match(r'^SKIPPED \\[(\\d+)\\] (tests/.*): (.*)$', line)\n            if match:\n                skipped_count += 1\n                test_file, test_line, reason = match.groups()\n                skipped_tests[reason] = skipped_tests.get(reason, []) + [(test_file, test_line)]\n    for k,v in sorted(skipped_tests.items(), key=lambda x:len(x[1])):\n        print(f\"{len(v):4} skipped because: {k}\")\n    print(\"Number of skipped tests:\", skipped_count)\n\ndef parse_pytest_failure_output(file_path):\n    failed_tests = {}\n    failed_count = 0\n    with open(file_path, 'r') as file:\n        for line in file:\n            match = re.match(r'^FAILED (tests/.*) - (.*): (.*)$', line)\n            if match:\n                failed_count += 1\n                _, error, reason = match.groups()\n                failed_tests[reason] = failed_tests.get(reason, []) + [error]\n    for k,v in sorted(failed_tests.items(), key=lambda x:len(x[1])):\n        print(f\"{len(v):4} failed because `{v[0]}` -> {k}\")\n    print(\"Number of failed tests:\", failed_count)\n    if failed_count>0:\n        exit(1)\n\ndef parse_pytest_errors_output(file_path):\n    print(file_path)\n    error_tests = {}\n    error_count = 0\n    with open(file_path, 'r') as file:\n        for line in file:\n            match = re.match(r'^ERROR (tests/.*) - (.*): (.*)$', line)\n            if match:\n                error_count += 1\n                _, test_error, reason = match.groups()\n                error_tests[reason] = error_tests.get(reason, []) + [test_error]\n    for k,v in sorted(error_tests.items(), key=lambda x:len(x[1])):\n        print(f\"{len(v):4} errored out because of `{v[0]}` -> {k}\")\n    print(\"Number of errors:\", error_count)\n    if error_count>0:\n        exit(1)\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--file\", help=\"file to parse\")\n    parser.add_argument(\"--skip\", action=\"store_true\", help=\"show skipped reasons\")\n    parser.add_argument(\"--fail\", action=\"store_true\", help=\"show failed tests\")\n    parser.add_argument(\"--errors\", action=\"store_true\", help=\"show failed tests\")\n    args = parser.parse_args()\n\n    if args.skip:\n        parse_pytest_output(args.file)\n\n    if args.fail:\n        parse_pytest_failure_output(args.file)\n\n    if args.errors:\n        parse_pytest_errors_output(args.file)\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "conftest.py": "# Copyright 2020 The HuggingFace Team. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# tests directory-specific settings - this file is run automatically\n# by pytest before any tests are run\n\nimport doctest\nimport sys\nimport warnings\nfrom os.path import abspath, dirname, join\n\nimport _pytest\nimport pytest\n\nfrom transformers.testing_utils import HfDoctestModule, HfDocTestParser\n\n\nNOT_DEVICE_TESTS = {\n    \"test_tokenization\",\n    \"test_processor\",\n    \"test_processing\",\n    \"test_beam_constraints\",\n    \"test_configuration_utils\",\n    \"test_data_collator\",\n    \"test_trainer_callback\",\n    \"test_trainer_utils\",\n    \"test_feature_extraction\",\n    \"test_image_processing\",\n    \"test_image_processor\",\n    \"test_image_transforms\",\n    \"test_optimization\",\n    \"test_retrieval\",\n    \"test_config\",\n    \"test_from_pretrained_no_checkpoint\",\n    \"test_keep_in_fp32_modules\",\n    \"test_gradient_checkpointing_backward_compatibility\",\n    \"test_gradient_checkpointing_enable_disable\",\n    \"test_save_load_fast_init_from_base\",\n    \"test_fast_init_context_manager\",\n    \"test_fast_init_tied_embeddings\",\n    \"test_save_load_fast_init_to_base\",\n    \"test_torch_save_load\",\n    \"test_initialization\",\n    \"test_forward_signature\",\n    \"test_model_get_set_embeddings\",\n    \"test_model_main_input_name\",\n    \"test_correct_missing_keys\",\n    \"test_tie_model_weights\",\n    \"test_can_use_safetensors\",\n    \"test_load_save_without_tied_weights\",\n    \"test_tied_weights_keys\",\n    \"test_model_weights_reload_no_missing_tied_weights\",\n    \"test_mismatched_shapes_have_properly_initialized_weights\",\n    \"test_matched_shapes_have_loaded_weights_when_some_mismatched_shapes_exist\",\n    \"test_model_is_small\",\n    \"test_tf_from_pt_safetensors\",\n    \"test_flax_from_pt_safetensors\",\n    \"ModelTest::test_pipeline_\",  # None of the pipeline tests from PipelineTesterMixin (of which XxxModelTest inherits from) are running on device\n    \"ModelTester::test_pipeline_\",\n    \"/repo_utils/\",\n    \"/utils/\",\n    \"/agents/\",\n}\n\n# allow having multiple repository checkouts and not needing to remember to rerun\n# `pip install -e '.[dev]'` when switching between checkouts and running tests.\ngit_repo_path = abspath(join(dirname(__file__), \"src\"))\nsys.path.insert(1, git_repo_path)\n\n# silence FutureWarning warnings in tests since often we can't act on them until\n# they become normal warnings - i.e. the tests still need to test the current functionality\nwarnings.simplefilter(action=\"ignore\", category=FutureWarning)\n\n\ndef pytest_configure(config):\n    config.addinivalue_line(\"markers\", \"is_pipeline_test: mark test to run only when pipelines are tested\")\n    config.addinivalue_line(\"markers\", \"is_staging_test: mark test to run only in the staging environment\")\n    config.addinivalue_line(\"markers\", \"accelerate_tests: mark test that require accelerate\")\n    config.addinivalue_line(\"markers\", \"agent_tests: mark the agent tests that are run on their specific schedule\")\n    config.addinivalue_line(\"markers\", \"not_device_test: mark the tests always running on cpu\")\n\n\ndef pytest_collection_modifyitems(items):\n    for item in items:\n        if any(test_name in item.nodeid for test_name in NOT_DEVICE_TESTS):\n            item.add_marker(pytest.mark.not_device_test)\n\n\ndef pytest_addoption(parser):\n    from transformers.testing_utils import pytest_addoption_shared\n\n    pytest_addoption_shared(parser)\n\n\ndef pytest_terminal_summary(terminalreporter):\n    from transformers.testing_utils import pytest_terminal_summary_main\n\n    make_reports = terminalreporter.config.getoption(\"--make-reports\")\n    if make_reports:\n        pytest_terminal_summary_main(terminalreporter, id=make_reports)\n\n\ndef pytest_sessionfinish(session, exitstatus):\n    # If no tests are collected, pytest exists with code 5, which makes the CI fail.\n    if exitstatus == 5:\n        session.exitstatus = 0\n\n\n# Doctest custom flag to ignore output.\nIGNORE_RESULT = doctest.register_optionflag(\"IGNORE_RESULT\")\n\nOutputChecker = doctest.OutputChecker\n\n\nclass CustomOutputChecker(OutputChecker):\n    def check_output(self, want, got, optionflags):\n        if IGNORE_RESULT & optionflags:\n            return True\n        return OutputChecker.check_output(self, want, got, optionflags)\n\n\ndoctest.OutputChecker = CustomOutputChecker\n_pytest.doctest.DoctestModule = HfDoctestModule\ndoctest.DocTestParser = HfDocTestParser\n",
    "examples/flax/conftest.py": "# Copyright 2021 The HuggingFace Team. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# tests directory-specific settings - this file is run automatically\n# by pytest before any tests are run\n\nimport sys\nimport warnings\nfrom os.path import abspath, dirname, join\n\n\n# allow having multiple repository checkouts and not needing to remember to rerun\n# `pip install -e '.[dev]'` when switching between checkouts and running tests.\ngit_repo_path = abspath(join(dirname(dirname(dirname(__file__))), \"src\"))\nsys.path.insert(1, git_repo_path)\n\n\n# silence FutureWarning warnings in tests since often we can't act on them until\n# they become normal warnings - i.e. the tests still need to test the current functionality\nwarnings.simplefilter(action=\"ignore\", category=FutureWarning)\n\n\ndef pytest_addoption(parser):\n    from transformers.testing_utils import pytest_addoption_shared\n\n    pytest_addoption_shared(parser)\n\n\ndef pytest_terminal_summary(terminalreporter):\n    from transformers.testing_utils import pytest_terminal_summary_main\n\n    make_reports = terminalreporter.config.getoption(\"--make-reports\")\n    if make_reports:\n        pytest_terminal_summary_main(terminalreporter, id=make_reports)\n",
    "examples/flax/test_flax_examples.py": "# coding=utf-8\n# Copyright 2021 HuggingFace Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nimport argparse\nimport json\nimport logging\nimport os\nimport sys\nfrom unittest.mock import patch\n\nfrom transformers.testing_utils import TestCasePlus, get_gpu_count, slow\n\n\nSRC_DIRS = [\n    os.path.join(os.path.dirname(__file__), dirname)\n    for dirname in [\n        \"text-classification\",\n        \"language-modeling\",\n        \"summarization\",\n        \"token-classification\",\n        \"question-answering\",\n        \"speech-recognition\",\n    ]\n]\nsys.path.extend(SRC_DIRS)\n\n\nif SRC_DIRS is not None:\n    import run_clm_flax\n    import run_flax_glue\n    import run_flax_ner\n    import run_flax_speech_recognition_seq2seq\n    import run_mlm_flax\n    import run_qa\n    import run_summarization_flax\n    import run_t5_mlm_flax\n\n\nlogging.basicConfig(level=logging.DEBUG)\n\nlogger = logging.getLogger()\n\n\ndef get_setup_file():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"-f\")\n    args = parser.parse_args()\n    return args.f\n\n\ndef get_results(output_dir, split=\"eval\"):\n    path = os.path.join(output_dir, f\"{split}_results.json\")\n    if os.path.exists(path):\n        with open(path, \"r\") as f:\n            return json.load(f)\n    raise ValueError(f\"can't find {path}\")\n\n\nstream_handler = logging.StreamHandler(sys.stdout)\nlogger.addHandler(stream_handler)\n\n\nclass ExamplesTests(TestCasePlus):\n    def test_run_glue(self):\n        tmp_dir = self.get_auto_remove_tmp_dir()\n        testargs = f\"\"\"\n            run_glue.py\n            --model_name_or_path distilbert/distilbert-base-uncased\n            --output_dir {tmp_dir}\n            --train_file ./tests/fixtures/tests_samples/MRPC/train.csv\n            --validation_file ./tests/fixtures/tests_samples/MRPC/dev.csv\n            --per_device_train_batch_size=2\n            --per_device_eval_batch_size=1\n            --learning_rate=1e-4\n            --eval_steps=2\n            --warmup_steps=2\n            --seed=42\n            --max_seq_length=128\n            \"\"\".split()\n\n        with patch.object(sys, \"argv\", testargs):\n            run_flax_glue.main()\n            result = get_results(tmp_dir)\n            self.assertGreaterEqual(result[\"eval_accuracy\"], 0.75)\n\n    @slow\n    def test_run_clm(self):\n        tmp_dir = self.get_auto_remove_tmp_dir()\n        testargs = f\"\"\"\n            run_clm_flax.py\n            --model_name_or_path distilbert/distilgpt2\n            --train_file ./tests/fixtures/sample_text.txt\n            --validation_file ./tests/fixtures/sample_text.txt\n            --do_train\n            --do_eval\n            --block_size 128\n            --per_device_train_batch_size 4\n            --per_device_eval_batch_size 4\n            --num_train_epochs 2\n            --logging_steps 2 --eval_steps 2\n            --output_dir {tmp_dir}\n            --overwrite_output_dir\n            \"\"\".split()\n\n        with patch.object(sys, \"argv\", testargs):\n            run_clm_flax.main()\n            result = get_results(tmp_dir)\n            self.assertLess(result[\"eval_perplexity\"], 100)\n\n    @slow\n    def test_run_summarization(self):\n        tmp_dir = self.get_auto_remove_tmp_dir()\n        testargs = f\"\"\"\n            run_summarization.py\n            --model_name_or_path google-t5/t5-small\n            --train_file tests/fixtures/tests_samples/xsum/sample.json\n            --validation_file tests/fixtures/tests_samples/xsum/sample.json\n            --test_file tests/fixtures/tests_samples/xsum/sample.json\n            --output_dir {tmp_dir}\n            --overwrite_output_dir\n            --num_train_epochs=3\n            --warmup_steps=8\n            --do_train\n            --do_eval\n            --do_predict\n            --learning_rate=2e-4\n            --per_device_train_batch_size=2\n            --per_device_eval_batch_size=1\n            --predict_with_generate\n        \"\"\".split()\n\n        with patch.object(sys, \"argv\", testargs):\n            run_summarization_flax.main()\n            result = get_results(tmp_dir, split=\"test\")\n            self.assertGreaterEqual(result[\"test_rouge1\"], 10)\n            self.assertGreaterEqual(result[\"test_rouge2\"], 2)\n            self.assertGreaterEqual(result[\"test_rougeL\"], 7)\n            self.assertGreaterEqual(result[\"test_rougeLsum\"], 7)\n\n    @slow\n    def test_run_mlm(self):\n        tmp_dir = self.get_auto_remove_tmp_dir()\n        testargs = f\"\"\"\n            run_mlm.py\n            --model_name_or_path distilbert/distilroberta-base\n            --train_file ./tests/fixtures/sample_text.txt\n            --validation_file ./tests/fixtures/sample_text.txt\n            --output_dir {tmp_dir}\n            --overwrite_output_dir\n            --max_seq_length 128\n            --per_device_train_batch_size 4\n            --per_device_eval_batch_size 4\n            --logging_steps 2 --eval_steps 2\n            --do_train\n            --do_eval\n            --num_train_epochs=1\n        \"\"\".split()\n\n        with patch.object(sys, \"argv\", testargs):\n            run_mlm_flax.main()\n            result = get_results(tmp_dir)\n            self.assertLess(result[\"eval_perplexity\"], 42)\n\n    @slow\n    def test_run_t5_mlm(self):\n        tmp_dir = self.get_auto_remove_tmp_dir()\n        testargs = f\"\"\"\n            run_t5_mlm_flax.py\n            --model_name_or_path google-t5/t5-small\n            --train_file ./tests/fixtures/sample_text.txt\n            --validation_file ./tests/fixtures/sample_text.txt\n            --do_train\n            --do_eval\n            --max_seq_length 128\n            --per_device_train_batch_size 4\n            --per_device_eval_batch_size 4\n            --num_train_epochs 2\n            --logging_steps 2 --eval_steps 2\n            --output_dir {tmp_dir}\n            --overwrite_output_dir\n            \"\"\".split()\n\n        with patch.object(sys, \"argv\", testargs):\n            run_t5_mlm_flax.main()\n            result = get_results(tmp_dir)\n            self.assertGreaterEqual(result[\"eval_accuracy\"], 0.42)\n\n    @slow\n    def test_run_ner(self):\n        # with so little data distributed training needs more epochs to get the score on par with 0/1 gpu\n        epochs = 7 if get_gpu_count() > 1 else 2\n\n        tmp_dir = self.get_auto_remove_tmp_dir()\n        testargs = f\"\"\"\n            run_flax_ner.py\n            --model_name_or_path google-bert/bert-base-uncased\n            --train_file tests/fixtures/tests_samples/conll/sample.json\n            --validation_file tests/fixtures/tests_samples/conll/sample.json\n            --output_dir {tmp_dir}\n            --overwrite_output_dir\n            --do_train\n            --do_eval\n            --warmup_steps=2\n            --learning_rate=2e-4\n            --logging_steps 2 --eval_steps 2\n            --per_device_train_batch_size=2\n            --per_device_eval_batch_size=2\n            --num_train_epochs={epochs}\n            --seed 7\n        \"\"\".split()\n\n        with patch.object(sys, \"argv\", testargs):\n            run_flax_ner.main()\n            result = get_results(tmp_dir)\n            self.assertGreaterEqual(result[\"eval_accuracy\"], 0.75)\n            self.assertGreaterEqual(result[\"eval_f1\"], 0.3)\n\n    @slow\n    def test_run_qa(self):\n        tmp_dir = self.get_auto_remove_tmp_dir()\n        testargs = f\"\"\"\n            run_qa.py\n            --model_name_or_path google-bert/bert-base-uncased\n            --version_2_with_negative\n            --train_file tests/fixtures/tests_samples/SQUAD/sample.json\n            --validation_file tests/fixtures/tests_samples/SQUAD/sample.json\n            --output_dir {tmp_dir}\n            --overwrite_output_dir\n            --num_train_epochs=3\n            --warmup_steps=2\n            --do_train\n            --do_eval\n            --logging_steps 2 --eval_steps 2\n            --learning_rate=2e-4\n            --per_device_train_batch_size=2\n            --per_device_eval_batch_size=1\n        \"\"\".split()\n\n        with patch.object(sys, \"argv\", testargs):\n            run_qa.main()\n            result = get_results(tmp_dir)\n            self.assertGreaterEqual(result[\"eval_f1\"], 30)\n            self.assertGreaterEqual(result[\"eval_exact\"], 30)\n\n    @slow\n    def test_run_flax_speech_recognition_seq2seq(self):\n        tmp_dir = self.get_auto_remove_tmp_dir()\n        testargs = f\"\"\"\n            run_flax_speech_recognition_seq2seq.py\n            --model_name_or_path openai/whisper-tiny.en\n            --dataset_name hf-internal-testing/librispeech_asr_dummy\n            --dataset_config clean\n            --train_split_name validation\n            --eval_split_name validation\n            --trust_remote_code\n            --output_dir {tmp_dir}\n            --overwrite_output_dir\n            --num_train_epochs=2\n            --max_train_samples 10\n            --max_eval_samples 10\n            --warmup_steps=8\n            --do_train\n            --do_eval\n            --learning_rate=2e-4\n            --per_device_train_batch_size=2\n            --per_device_eval_batch_size=1\n            --predict_with_generate\n        \"\"\".split()\n\n        with patch.object(sys, \"argv\", testargs):\n            run_flax_speech_recognition_seq2seq.main()\n            result = get_results(tmp_dir, split=\"eval\")\n            self.assertLessEqual(result[\"eval_wer\"], 0.05)\n",
    "examples/legacy/seq2seq/old_test_calculate_rouge.py": "# Copyright 2020 The HuggingFace Team. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom collections import defaultdict\nfrom pathlib import Path\n\nimport pandas as pd\nfrom rouge_cli import calculate_rouge_path\n\nfrom utils import calculate_rouge\n\n\nPRED = [\n    'Prosecutor: \"No videos were used in the crash investigation\" German papers say they saw a cell phone video of the'\n    ' final seconds on board Flight 9525. The Germanwings co-pilot says he had a \"previous episode of severe'\n    \" depression\\\" German airline confirms it knew of Andreas Lubitz's depression years before he took control.\",\n    \"The Palestinian Authority officially becomes the 123rd member of the International Criminal Court. The formal\"\n    \" accession was marked with a ceremony at The Hague, in the Netherlands. The Palestinians signed the ICC's\"\n    \" founding Rome Statute in January. Israel and the United States opposed the Palestinians' efforts to join the\"\n    \" body.\",\n    \"Amnesty International releases its annual report on the death penalty. The report catalogs the use of\"\n    \" state-sanctioned killing as a punitive measure across the globe. At least 607 people were executed around the\"\n    \" world in 2014, compared to 778 in 2013. The U.S. remains one of the worst offenders for imposing capital\"\n    \" punishment.\",\n]\n\nTGT = [\n    'Marseille prosecutor says \"so far no videos were used in the crash investigation\" despite media reports .'\n    ' Journalists at Bild and Paris Match are \"very confident\" the video clip is real, an editor says . Andreas Lubitz'\n    \" had informed his Lufthansa training school of an episode of severe depression, airline says .\",\n    \"Membership gives the ICC jurisdiction over alleged crimes committed in Palestinian territories since last June .\"\n    \" Israel and the United States opposed the move, which could open the door to war crimes investigations against\"\n    \" Israelis .\",\n    \"Amnesty's annual death penalty report catalogs encouraging signs, but setbacks in numbers of those sentenced to\"\n    \" death . Organization claims that governments around the world are using the threat of terrorism to advance\"\n    \" executions . The number of executions worldwide has gone down by almost 22% compared with 2013, but death\"\n    \" sentences up by 28% .\",\n]\n\n\ndef test_disaggregated_scores_are_determinstic():\n    no_aggregation = calculate_rouge(PRED, TGT, bootstrap_aggregation=False, rouge_keys=[\"rouge2\", \"rougeL\"])\n    assert isinstance(no_aggregation, defaultdict)\n    no_aggregation_just_r2 = calculate_rouge(PRED, TGT, bootstrap_aggregation=False, rouge_keys=[\"rouge2\"])\n    assert (\n        pd.DataFrame(no_aggregation[\"rouge2\"]).fmeasure.mean()\n        == pd.DataFrame(no_aggregation_just_r2[\"rouge2\"]).fmeasure.mean()\n    )\n\n\ndef test_newline_cnn_improvement():\n    k = \"rougeLsum\"\n    score = calculate_rouge(PRED, TGT, newline_sep=True, rouge_keys=[k])[k]\n    score_no_sep = calculate_rouge(PRED, TGT, newline_sep=False, rouge_keys=[k])[k]\n    assert score > score_no_sep\n\n\ndef test_newline_irrelevant_for_other_metrics():\n    k = [\"rouge1\", \"rouge2\", \"rougeL\"]\n    score_sep = calculate_rouge(PRED, TGT, newline_sep=True, rouge_keys=k)\n    score_no_sep = calculate_rouge(PRED, TGT, newline_sep=False, rouge_keys=k)\n    assert score_sep == score_no_sep\n\n\ndef test_single_sent_scores_dont_depend_on_newline_sep():\n    pred = [\n        \"Her older sister, Margot Frank, died in 1945, a month earlier than previously thought.\",\n        'Marseille prosecutor says \"so far no videos were used in the crash investigation\" despite media reports .',\n    ]\n    tgt = [\n        \"Margot Frank, died in 1945, a month earlier than previously thought.\",\n        'Prosecutor: \"No videos were used in the crash investigation\" German papers say they saw a cell phone video of'\n        \" the final seconds on board Flight 9525.\",\n    ]\n    assert calculate_rouge(pred, tgt, newline_sep=True) == calculate_rouge(pred, tgt, newline_sep=False)\n\n\ndef test_pegasus_newline():\n    pred = [\n        \"\"\"\" \"a person who has such a video needs to immediately give it to the investigators,\" prosecutor says .<n> \"it is a very disturbing scene,\" editor-in-chief of bild online tells \"erin burnett: outfront\" \"\"\"\n    ]\n    tgt = [\n        \"\"\" Marseille prosecutor says \"so far no videos were used in the crash investigation\" despite media reports . Journalists at Bild and Paris Match are \"very confident\" the video clip is real, an editor says . Andreas Lubitz had informed his Lufthansa training school of an episode of severe depression, airline says .\"\"\"\n    ]\n\n    prev_score = calculate_rouge(pred, tgt, rouge_keys=[\"rougeLsum\"], newline_sep=False)[\"rougeLsum\"]\n    new_score = calculate_rouge(pred, tgt, rouge_keys=[\"rougeLsum\"])[\"rougeLsum\"]\n    assert new_score > prev_score\n\n\ndef test_rouge_cli():\n    data_dir = Path(\"examples/seq2seq/test_data/wmt_en_ro\")\n    metrics = calculate_rouge_path(data_dir.joinpath(\"test.source\"), data_dir.joinpath(\"test.target\"))\n    assert isinstance(metrics, dict)\n    metrics_default_dict = calculate_rouge_path(\n        data_dir.joinpath(\"test.source\"), data_dir.joinpath(\"test.target\"), bootstrap_aggregation=False\n    )\n    assert isinstance(metrics_default_dict, defaultdict)\n",
    "examples/legacy/seq2seq/old_test_datasets.py": "# Copyright 2020 The HuggingFace Team. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport os\nfrom pathlib import Path\n\nimport numpy as np\nimport pytest\nfrom pack_dataset import pack_data_dir\nfrom parameterized import parameterized\nfrom save_len_file import save_len_file\nfrom torch.utils.data import DataLoader\n\nfrom transformers import AutoTokenizer\nfrom transformers.models.mbart.modeling_mbart import shift_tokens_right\nfrom transformers.testing_utils import TestCasePlus, slow\nfrom utils import FAIRSEQ_AVAILABLE, DistributedSortishSampler, LegacySeq2SeqDataset, Seq2SeqDataset\n\n\nBERT_BASE_CASED = \"google-bert/bert-base-cased\"\nPEGASUS_XSUM = \"google/pegasus-xsum\"\nARTICLES = [\" Sam ate lunch today.\", \"Sams lunch ingredients.\"]\nSUMMARIES = [\"A very interesting story about what I ate for lunch.\", \"Avocado, celery, turkey, coffee\"]\nT5_TINY = \"patrickvonplaten/t5-tiny-random\"\nBART_TINY = \"sshleifer/bart-tiny-random\"\nMBART_TINY = \"sshleifer/tiny-mbart\"\nMARIAN_TINY = \"sshleifer/tiny-marian-en-de\"\n\n\ndef _dump_articles(path: Path, articles: list):\n    content = \"\\n\".join(articles)\n    Path(path).open(\"w\").writelines(content)\n\n\ndef make_test_data_dir(tmp_dir):\n    for split in [\"train\", \"val\", \"test\"]:\n        _dump_articles(os.path.join(tmp_dir, f\"{split}.source\"), ARTICLES)\n        _dump_articles(os.path.join(tmp_dir, f\"{split}.target\"), SUMMARIES)\n    return tmp_dir\n\n\nclass TestAll(TestCasePlus):\n    @parameterized.expand(\n        [\n            MBART_TINY,\n            MARIAN_TINY,\n            T5_TINY,\n            BART_TINY,\n            PEGASUS_XSUM,\n        ],\n    )\n    @slow\n    def test_seq2seq_dataset_truncation(self, tok_name):\n        tokenizer = AutoTokenizer.from_pretrained(tok_name)\n        tmp_dir = make_test_data_dir(tmp_dir=self.get_auto_remove_tmp_dir())\n        max_len_source = max(len(tokenizer.encode(a)) for a in ARTICLES)\n        max_len_target = max(len(tokenizer.encode(a)) for a in SUMMARIES)\n        max_src_len = 4\n        max_tgt_len = 8\n        assert max_len_target > max_src_len  # Will be truncated\n        assert max_len_source > max_src_len  # Will be truncated\n        src_lang, tgt_lang = \"ro_RO\", \"de_DE\"  # ignored for all but mbart, but never causes error.\n        train_dataset = Seq2SeqDataset(\n            tokenizer,\n            data_dir=tmp_dir,\n            type_path=\"train\",\n            max_source_length=max_src_len,\n            max_target_length=max_tgt_len,  # ignored\n            src_lang=src_lang,\n            tgt_lang=tgt_lang,\n        )\n        dataloader = DataLoader(train_dataset, batch_size=2, collate_fn=train_dataset.collate_fn)\n        for batch in dataloader:\n            assert isinstance(batch, dict)\n            assert batch[\"attention_mask\"].shape == batch[\"input_ids\"].shape\n            # show that articles were trimmed.\n            assert batch[\"input_ids\"].shape[1] == max_src_len\n            # show that targets are the same len\n            assert batch[\"labels\"].shape[1] == max_tgt_len\n            if tok_name != MBART_TINY:\n                continue\n            # check language codes in correct place\n            batch[\"decoder_input_ids\"] = shift_tokens_right(batch[\"labels\"], tokenizer.pad_token_id)\n            assert batch[\"decoder_input_ids\"][0, 0].item() == tokenizer.lang_code_to_id[tgt_lang]\n            assert batch[\"decoder_input_ids\"][0, -1].item() == tokenizer.eos_token_id\n            assert batch[\"input_ids\"][0, -2].item() == tokenizer.eos_token_id\n            assert batch[\"input_ids\"][0, -1].item() == tokenizer.lang_code_to_id[src_lang]\n\n            break  # No need to test every batch\n\n    @parameterized.expand([BART_TINY, BERT_BASE_CASED])\n    def test_legacy_dataset_truncation(self, tok):\n        tokenizer = AutoTokenizer.from_pretrained(tok)\n        tmp_dir = make_test_data_dir(tmp_dir=self.get_auto_remove_tmp_dir())\n        max_len_source = max(len(tokenizer.encode(a)) for a in ARTICLES)\n        max_len_target = max(len(tokenizer.encode(a)) for a in SUMMARIES)\n        trunc_target = 4\n        train_dataset = LegacySeq2SeqDataset(\n            tokenizer,\n            data_dir=tmp_dir,\n            type_path=\"train\",\n            max_source_length=20,\n            max_target_length=trunc_target,\n        )\n        dataloader = DataLoader(train_dataset, batch_size=2, collate_fn=train_dataset.collate_fn)\n        for batch in dataloader:\n            assert batch[\"attention_mask\"].shape == batch[\"input_ids\"].shape\n            # show that articles were trimmed.\n            assert batch[\"input_ids\"].shape[1] == max_len_source\n            assert 20 >= batch[\"input_ids\"].shape[1]  # trimmed significantly\n            # show that targets were truncated\n            assert batch[\"labels\"].shape[1] == trunc_target  # Truncated\n            assert max_len_target > trunc_target  # Truncated\n            break  # No need to test every batch\n\n    def test_pack_dataset(self):\n        tokenizer = AutoTokenizer.from_pretrained(\"facebook/mbart-large-cc25\")\n\n        tmp_dir = Path(make_test_data_dir(tmp_dir=self.get_auto_remove_tmp_dir()))\n        orig_examples = tmp_dir.joinpath(\"train.source\").open().readlines()\n        save_dir = Path(make_test_data_dir(tmp_dir=self.get_auto_remove_tmp_dir()))\n        pack_data_dir(tokenizer, tmp_dir, 128, save_dir)\n        orig_paths = {x.name for x in tmp_dir.iterdir()}\n        new_paths = {x.name for x in save_dir.iterdir()}\n        packed_examples = save_dir.joinpath(\"train.source\").open().readlines()\n        # orig: [' Sam ate lunch today.\\n', 'Sams lunch ingredients.']\n        # desired_packed: [' Sam ate lunch today.\\n Sams lunch ingredients.']\n        assert len(packed_examples) < len(orig_examples)\n        assert len(packed_examples) == 1\n        assert len(packed_examples[0]) == sum(len(x) for x in orig_examples)\n        assert orig_paths == new_paths\n\n    @pytest.mark.skipif(not FAIRSEQ_AVAILABLE, reason=\"This test requires fairseq\")\n    def test_dynamic_batch_size(self):\n        if not FAIRSEQ_AVAILABLE:\n            return\n        ds, max_tokens, tokenizer = self._get_dataset(max_len=64)\n        required_batch_size_multiple = 64\n        batch_sampler = ds.make_dynamic_sampler(max_tokens, required_batch_size_multiple=required_batch_size_multiple)\n        batch_sizes = [len(x) for x in batch_sampler]\n        assert len(set(batch_sizes)) > 1  # it's not dynamic batch size if every batch is the same length\n        assert sum(batch_sizes) == len(ds)  # no dropped or added examples\n        data_loader = DataLoader(ds, batch_sampler=batch_sampler, collate_fn=ds.collate_fn, num_workers=2)\n        failures = []\n        num_src_per_batch = []\n        for batch in data_loader:\n            src_shape = batch[\"input_ids\"].shape\n            bs = src_shape[0]\n            assert bs % required_batch_size_multiple == 0 or bs < required_batch_size_multiple\n            num_src_tokens = np.product(batch[\"input_ids\"].shape)\n            num_src_per_batch.append(num_src_tokens)\n            if num_src_tokens > (max_tokens * 1.1):\n                failures.append(num_src_tokens)\n        assert num_src_per_batch[0] == max(num_src_per_batch)\n        if failures:\n            raise AssertionError(f\"too many tokens in {len(failures)} batches\")\n\n    def test_sortish_sampler_reduces_padding(self):\n        ds, _, tokenizer = self._get_dataset(max_len=512)\n        bs = 2\n        sortish_sampler = ds.make_sortish_sampler(bs, shuffle=False)\n\n        naive_dl = DataLoader(ds, batch_size=bs, collate_fn=ds.collate_fn, num_workers=2)\n        sortish_dl = DataLoader(ds, batch_size=bs, collate_fn=ds.collate_fn, num_workers=2, sampler=sortish_sampler)\n\n        pad = tokenizer.pad_token_id\n\n        def count_pad_tokens(data_loader, k=\"input_ids\"):\n            return [batch[k].eq(pad).sum().item() for batch in data_loader]\n\n        assert sum(count_pad_tokens(sortish_dl, k=\"labels\")) < sum(count_pad_tokens(naive_dl, k=\"labels\"))\n        assert sum(count_pad_tokens(sortish_dl)) < sum(count_pad_tokens(naive_dl))\n        assert len(sortish_dl) == len(naive_dl)\n\n    def _get_dataset(self, n_obs=1000, max_len=128):\n        if os.getenv(\"USE_REAL_DATA\", False):\n            data_dir = \"examples/seq2seq/wmt_en_ro\"\n            max_tokens = max_len * 2 * 64\n            if not Path(data_dir).joinpath(\"train.len\").exists():\n                save_len_file(MARIAN_TINY, data_dir)\n        else:\n            data_dir = \"examples/seq2seq/test_data/wmt_en_ro\"\n            max_tokens = max_len * 4\n            save_len_file(MARIAN_TINY, data_dir)\n\n        tokenizer = AutoTokenizer.from_pretrained(MARIAN_TINY)\n        ds = Seq2SeqDataset(\n            tokenizer,\n            data_dir=data_dir,\n            type_path=\"train\",\n            max_source_length=max_len,\n            max_target_length=max_len,\n            n_obs=n_obs,\n        )\n        return ds, max_tokens, tokenizer\n\n    def test_distributed_sortish_sampler_splits_indices_between_procs(self):\n        ds, max_tokens, tokenizer = self._get_dataset()\n        ids1 = set(DistributedSortishSampler(ds, 256, num_replicas=2, rank=0, add_extra_examples=False))\n        ids2 = set(DistributedSortishSampler(ds, 256, num_replicas=2, rank=1, add_extra_examples=False))\n        assert ids1.intersection(ids2) == set()\n\n    @parameterized.expand(\n        [\n            MBART_TINY,\n            MARIAN_TINY,\n            T5_TINY,\n            BART_TINY,\n            PEGASUS_XSUM,\n        ],\n    )\n    def test_dataset_kwargs(self, tok_name):\n        tokenizer = AutoTokenizer.from_pretrained(tok_name, use_fast=False)\n        if tok_name == MBART_TINY:\n            train_dataset = Seq2SeqDataset(\n                tokenizer,\n                data_dir=make_test_data_dir(tmp_dir=self.get_auto_remove_tmp_dir()),\n                type_path=\"train\",\n                max_source_length=4,\n                max_target_length=8,\n                src_lang=\"EN\",\n                tgt_lang=\"FR\",\n            )\n            kwargs = train_dataset.dataset_kwargs\n            assert \"src_lang\" in kwargs and \"tgt_lang\" in kwargs\n        else:\n            train_dataset = Seq2SeqDataset(\n                tokenizer,\n                data_dir=make_test_data_dir(tmp_dir=self.get_auto_remove_tmp_dir()),\n                type_path=\"train\",\n                max_source_length=4,\n                max_target_length=8,\n            )\n            kwargs = train_dataset.dataset_kwargs\n            assert \"add_prefix_space\" not in kwargs if tok_name != BART_TINY else \"add_prefix_space\" in kwargs\n            assert len(kwargs) == 1 if tok_name == BART_TINY else len(kwargs) == 0\n",
    "examples/legacy/seq2seq/old_test_fsmt_bleu_score.py": "# coding=utf-8\n# Copyright 2020 Huggingface\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport io\nimport json\nimport unittest\n\nfrom parameterized import parameterized\n\nfrom transformers import FSMTForConditionalGeneration, FSMTTokenizer\nfrom transformers.testing_utils import get_tests_dir, require_torch, slow, torch_device\nfrom utils import calculate_bleu\n\n\nfilename = get_tests_dir() + \"/test_data/fsmt/fsmt_val_data.json\"\nwith io.open(filename, \"r\", encoding=\"utf-8\") as f:\n    bleu_data = json.load(f)\n\n\n@require_torch\nclass ModelEvalTester(unittest.TestCase):\n    def get_tokenizer(self, mname):\n        return FSMTTokenizer.from_pretrained(mname)\n\n    def get_model(self, mname):\n        model = FSMTForConditionalGeneration.from_pretrained(mname).to(torch_device)\n        if torch_device == \"cuda\":\n            model.half()\n        return model\n\n    @parameterized.expand(\n        [\n            [\"en-ru\", 26.0],\n            [\"ru-en\", 22.0],\n            [\"en-de\", 22.0],\n            [\"de-en\", 29.0],\n        ]\n    )\n    @slow\n    def test_bleu_scores(self, pair, min_bleu_score):\n        # note: this test is not testing the best performance since it only evals a small batch\n        # but it should be enough to detect a regression in the output quality\n        mname = f\"facebook/wmt19-{pair}\"\n        tokenizer = self.get_tokenizer(mname)\n        model = self.get_model(mname)\n\n        src_sentences = bleu_data[pair][\"src\"]\n        tgt_sentences = bleu_data[pair][\"tgt\"]\n\n        batch = tokenizer(src_sentences, return_tensors=\"pt\", truncation=True, padding=\"longest\").to(torch_device)\n        outputs = model.generate(\n            input_ids=batch.input_ids,\n            num_beams=8,\n        )\n        decoded_sentences = tokenizer.batch_decode(\n            outputs, skip_special_tokens=True, clean_up_tokenization_spaces=False\n        )\n        scores = calculate_bleu(decoded_sentences, tgt_sentences)\n        print(scores)\n        self.assertGreaterEqual(scores[\"bleu\"], min_bleu_score)\n",
    "examples/legacy/seq2seq/old_test_seq2seq_examples.py": "# Copyright 2020 The HuggingFace Team. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport logging\nimport os\nimport sys\nfrom pathlib import Path\nfrom unittest.mock import patch\n\nfrom parameterized import parameterized\nfrom run_eval import run_generate\nfrom run_eval_search import run_search\n\nfrom transformers.testing_utils import CaptureStdout, TestCasePlus, slow\nfrom utils import ROUGE_KEYS\n\n\nlogging.basicConfig(level=logging.DEBUG)\nlogger = logging.getLogger()\n\n\ndef _dump_articles(path: Path, articles: list):\n    content = \"\\n\".join(articles)\n    Path(path).open(\"w\").writelines(content)\n\n\nT5_TINY = \"patrickvonplaten/t5-tiny-random\"\nBART_TINY = \"sshleifer/bart-tiny-random\"\nMBART_TINY = \"sshleifer/tiny-mbart\"\n\nstream_handler = logging.StreamHandler(sys.stdout)\nlogger.addHandler(stream_handler)\nlogging.disable(logging.CRITICAL)  # remove noisy download output from tracebacks\n\n\nclass TestTheRest(TestCasePlus):\n    def run_eval_tester(self, model):\n        input_file_name = Path(self.get_auto_remove_tmp_dir()) / \"utest_input.source\"\n        output_file_name = input_file_name.parent / \"utest_output.txt\"\n        assert not output_file_name.exists()\n        articles = [\" New York (CNN)When Liana Barrientos was 23 years old, she got married in Westchester County.\"]\n        _dump_articles(input_file_name, articles)\n\n        score_path = str(Path(self.get_auto_remove_tmp_dir()) / \"scores.json\")\n        task = \"translation_en_to_de\" if model == T5_TINY else \"summarization\"\n        testargs = f\"\"\"\n            run_eval_search.py\n            {model}\n            {input_file_name}\n            {output_file_name}\n            --score_path {score_path}\n            --task {task}\n            --num_beams 2\n            --length_penalty 2.0\n            \"\"\".split()\n\n        with patch.object(sys, \"argv\", testargs):\n            run_generate()\n            assert Path(output_file_name).exists()\n            # os.remove(Path(output_file_name))\n\n    # test one model to quickly (no-@slow) catch simple problems and do an\n    # extensive testing of functionality with multiple models as @slow separately\n    def test_run_eval(self):\n        self.run_eval_tester(T5_TINY)\n\n    # any extra models should go into the list here - can be slow\n    @parameterized.expand([BART_TINY, MBART_TINY])\n    @slow\n    def test_run_eval_slow(self, model):\n        self.run_eval_tester(model)\n\n    # testing with 2 models to validate: 1. translation (t5) 2. summarization (mbart)\n    @parameterized.expand([T5_TINY, MBART_TINY])\n    @slow\n    def test_run_eval_search(self, model):\n        input_file_name = Path(self.get_auto_remove_tmp_dir()) / \"utest_input.source\"\n        output_file_name = input_file_name.parent / \"utest_output.txt\"\n        assert not output_file_name.exists()\n\n        text = {\n            \"en\": [\"Machine learning is great, isn't it?\", \"I like to eat bananas\", \"Tomorrow is another great day!\"],\n            \"de\": [\n                \"Maschinelles Lernen ist groÃŸartig, oder?\",\n                \"Ich esse gerne Bananen\",\n                \"Morgen ist wieder ein toller Tag!\",\n            ],\n        }\n\n        tmp_dir = Path(self.get_auto_remove_tmp_dir())\n        score_path = str(tmp_dir / \"scores.json\")\n        reference_path = str(tmp_dir / \"val.target\")\n        _dump_articles(input_file_name, text[\"en\"])\n        _dump_articles(reference_path, text[\"de\"])\n        task = \"translation_en_to_de\" if model == T5_TINY else \"summarization\"\n        testargs = f\"\"\"\n            run_eval_search.py\n            {model}\n            {str(input_file_name)}\n            {str(output_file_name)}\n            --score_path {score_path}\n            --reference_path {reference_path}\n            --task {task}\n            \"\"\".split()\n        testargs.extend([\"--search\", \"num_beams=1:2 length_penalty=0.9:1.0\"])\n\n        with patch.object(sys, \"argv\", testargs):\n            with CaptureStdout() as cs:\n                run_search()\n            expected_strings = [\" num_beams | length_penalty\", model, \"Best score args\"]\n            un_expected_strings = [\"Info\"]\n            if \"translation\" in task:\n                expected_strings.append(\"bleu\")\n            else:\n                expected_strings.extend(ROUGE_KEYS)\n            for w in expected_strings:\n                assert w in cs.out\n            for w in un_expected_strings:\n                assert w not in cs.out\n            assert Path(output_file_name).exists()\n            os.remove(Path(output_file_name))\n",
    "examples/legacy/seq2seq/old_test_seq2seq_examples_multi_gpu.py": "# Copyright 2020 The HuggingFace Team. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# as due to their complexity multi-gpu tests could impact other tests, and to aid debug we have those in a separate module.\n\nimport os\nimport sys\n\nfrom transformers.testing_utils import TestCasePlus, execute_subprocess_async, get_gpu_count, require_torch_gpu, slow\n\nfrom .utils import load_json\n\n\nclass TestSummarizationDistillerMultiGPU(TestCasePlus):\n    @classmethod\n    def setUpClass(cls):\n        return cls\n\n    @slow\n    @require_torch_gpu\n    def test_distributed_eval(self):\n        output_dir = self.get_auto_remove_tmp_dir()\n        args = f\"\"\"\n            --model_name Helsinki-NLP/opus-mt-en-ro\n            --save_dir {output_dir}\n            --data_dir {self.test_file_dir_str}/test_data/wmt_en_ro\n            --num_beams 2\n            --task translation\n        \"\"\".split()\n\n        # we want this test to run even if there is only one GPU, but if there are more we use them all\n        n_gpu = get_gpu_count()\n        distributed_args = f\"\"\"\n            -m torch.distributed.launch\n            --nproc_per_node={n_gpu}\n            {self.test_file_dir}/run_distributed_eval.py\n        \"\"\".split()\n        cmd = [sys.executable] + distributed_args + args\n        execute_subprocess_async(cmd, env=self.get_env())\n\n        metrics_save_path = os.path.join(output_dir, \"test_bleu.json\")\n        metrics = load_json(metrics_save_path)\n        # print(metrics)\n        self.assertGreaterEqual(metrics[\"bleu\"], 25)\n",
    "examples/legacy/seq2seq/old_test_tatoeba_conversion.py": "# Copyright 2020 The HuggingFace Team. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport os\nimport tempfile\nimport unittest\n\nfrom transformers.models.marian.convert_marian_tatoeba_to_pytorch import DEFAULT_REPO, TatoebaConverter\nfrom transformers.testing_utils import slow\nfrom transformers.utils import cached_property\n\n\n@unittest.skipUnless(os.path.exists(DEFAULT_REPO), \"Tatoeba directory does not exist.\")\nclass TatoebaConversionTester(unittest.TestCase):\n    @cached_property\n    def resolver(self):\n        tmp_dir = tempfile.mkdtemp()\n        return TatoebaConverter(save_dir=tmp_dir)\n\n    @slow\n    def test_resolver(self):\n        self.resolver.convert_models([\"heb-eng\"])\n\n    @slow\n    def test_model_card(self):\n        content, mmeta = self.resolver.write_model_card(\"opus-mt-he-en\", dry_run=True)\n        assert mmeta[\"long_pair\"] == \"heb-eng\"\n"
  },
  "requirements": null
}