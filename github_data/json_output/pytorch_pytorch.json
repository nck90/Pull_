{
  "repo_name": "pytorch/pytorch",
  "repo_url": "https://github.com/pytorch/pytorch",
  "description": "Tensors and Dynamic neural networks in Python with strong GPU acceleration",
  "stars": 88011,
  "language": "Python",
  "created_at": "2016-08-13T05:26:41Z",
  "updated_at": "2025-03-19T07:06:54Z",
  "files": {
    ".ci/pytorch/create_test_cert.py": "from datetime import datetime, timedelta, timezone\nfrom tempfile import mkdtemp\n\nfrom cryptography import x509\nfrom cryptography.hazmat.primitives import hashes, serialization\nfrom cryptography.hazmat.primitives.asymmetric import rsa\nfrom cryptography.x509.oid import NameOID\n\n\ntemp_dir = mkdtemp()\nprint(temp_dir)\n\n\ndef genrsa(path):\n    key = rsa.generate_private_key(\n        public_exponent=65537,\n        key_size=2048,\n    )\n    with open(path, \"wb\") as f:\n        f.write(\n            key.private_bytes(\n                encoding=serialization.Encoding.PEM,\n                format=serialization.PrivateFormat.TraditionalOpenSSL,\n                encryption_algorithm=serialization.NoEncryption(),\n            )\n        )\n    return key\n\n\ndef create_cert(path, C, ST, L, O, key):\n    subject = issuer = x509.Name(\n        [\n            x509.NameAttribute(NameOID.COUNTRY_NAME, C),\n            x509.NameAttribute(NameOID.STATE_OR_PROVINCE_NAME, ST),\n            x509.NameAttribute(NameOID.LOCALITY_NAME, L),\n            x509.NameAttribute(NameOID.ORGANIZATION_NAME, O),\n        ]\n    )\n    cert = (\n        x509.CertificateBuilder()\n        .subject_name(subject)\n        .issuer_name(issuer)\n        .public_key(key.public_key())\n        .serial_number(x509.random_serial_number())\n        .not_valid_before(datetime.now(timezone.utc))\n        .not_valid_after(\n            # Our certificate will be valid for 10 days\n            datetime.now(timezone.utc) + timedelta(days=10)\n        )\n        .add_extension(\n            x509.BasicConstraints(ca=True, path_length=None),\n            critical=True,\n        )\n        .sign(key, hashes.SHA256())\n    )\n    # Write our certificate out to disk.\n    with open(path, \"wb\") as f:\n        f.write(cert.public_bytes(serialization.Encoding.PEM))\n    return cert\n\n\ndef create_req(path, C, ST, L, O, key):\n    csr = (\n        x509.CertificateSigningRequestBuilder()\n        .subject_name(\n            x509.Name(\n                [\n                    # Provide various details about who we are.\n                    x509.NameAttribute(NameOID.COUNTRY_NAME, C),\n                    x509.NameAttribute(NameOID.STATE_OR_PROVINCE_NAME, ST),\n                    x509.NameAttribute(NameOID.LOCALITY_NAME, L),\n                    x509.NameAttribute(NameOID.ORGANIZATION_NAME, O),\n                ]\n            )\n        )\n        .sign(key, hashes.SHA256())\n    )\n    with open(path, \"wb\") as f:\n        f.write(csr.public_bytes(serialization.Encoding.PEM))\n    return csr\n\n\ndef sign_certificate_request(path, csr_cert, ca_cert, private_ca_key):\n    cert = (\n        x509.CertificateBuilder()\n        .subject_name(csr_cert.subject)\n        .issuer_name(ca_cert.subject)\n        .public_key(csr_cert.public_key())\n        .serial_number(x509.random_serial_number())\n        .not_valid_before(datetime.now(timezone.utc))\n        .not_valid_after(\n            # Our certificate will be valid for 10 days\n            datetime.now(timezone.utc) + timedelta(days=10)\n            # Sign our certificate with our private key\n        )\n        .sign(private_ca_key, hashes.SHA256())\n    )\n    with open(path, \"wb\") as f:\n        f.write(cert.public_bytes(serialization.Encoding.PEM))\n    return cert\n\n\nca_key = genrsa(temp_dir + \"/ca.key\")\nca_cert = create_cert(\n    temp_dir + \"/ca.pem\",\n    \"US\",\n    \"New York\",\n    \"New York\",\n    \"Gloo Certificate Authority\",\n    ca_key,\n)\n\npkey = genrsa(temp_dir + \"/pkey.key\")\ncsr = create_req(\n    temp_dir + \"/csr.csr\",\n    \"US\",\n    \"California\",\n    \"San Francisco\",\n    \"Gloo Testing Company\",\n    pkey,\n)\n\ncert = sign_certificate_request(temp_dir + \"/cert.pem\", csr, ca_cert, ca_key)\n",
    ".ci/pytorch/perf_test/compare_with_baseline.py": "import argparse\nimport json\nimport math\nimport sys\n\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\n    \"--test-name\", dest=\"test_name\", action=\"store\", required=True, help=\"test name\"\n)\nparser.add_argument(\n    \"--sample-stats\",\n    dest=\"sample_stats\",\n    action=\"store\",\n    required=True,\n    help=\"stats from sample\",\n)\nparser.add_argument(\n    \"--update\",\n    action=\"store_true\",\n    help=\"whether to update baseline using stats from sample\",\n)\nargs = parser.parse_args()\n\ntest_name = args.test_name\n\nif \"cpu\" in test_name:\n    backend = \"cpu\"\nelif \"gpu\" in test_name:\n    backend = \"gpu\"\n\ndata_file_path = f\"../{backend}_runtime.json\"\n\nwith open(data_file_path) as data_file:\n    data = json.load(data_file)\n\nif test_name in data:\n    mean = float(data[test_name][\"mean\"])\n    sigma = float(data[test_name][\"sigma\"])\nelse:\n    # Let the test pass if baseline number doesn't exist\n    mean = sys.maxsize\n    sigma = 0.001\n\nprint(\"population mean: \", mean)\nprint(\"population sigma: \", sigma)\n\n# Let the test pass if baseline number is NaN (which happened in\n# the past when we didn't have logic for catching NaN numbers)\nif math.isnan(mean) or math.isnan(sigma):\n    mean = sys.maxsize\n    sigma = 0.001\n\nsample_stats_data = json.loads(args.sample_stats)\n\nsample_mean = float(sample_stats_data[\"mean\"])\nsample_sigma = float(sample_stats_data[\"sigma\"])\n\nprint(\"sample mean: \", sample_mean)\nprint(\"sample sigma: \", sample_sigma)\n\nif math.isnan(sample_mean):\n    raise Exception(\"\"\"Error: sample mean is NaN\"\"\")  # noqa: TRY002\nelif math.isnan(sample_sigma):\n    raise Exception(\"\"\"Error: sample sigma is NaN\"\"\")  # noqa: TRY002\n\nz_value = (sample_mean - mean) / sigma\n\nprint(\"z-value: \", z_value)\n\nif z_value >= 3:\n    raise Exception(  # noqa: TRY002\n        f\"\"\"\\n\nz-value >= 3, there is high chance of perf regression.\\n\nTo reproduce this regression, run\n`cd .ci/pytorch/perf_test/ && bash {test_name}.sh` on your local machine\nand compare the runtime before/after your code change.\n\"\"\"\n    )\nelse:\n    print(\"z-value < 3, no perf regression detected.\")\n    if args.update:\n        print(\"We will use these numbers as new baseline.\")\n        new_data_file_path = f\"../new_{backend}_runtime.json\"\n        with open(new_data_file_path) as new_data_file:\n            new_data = json.load(new_data_file)\n        new_data[test_name] = {}\n        new_data[test_name][\"mean\"] = sample_mean\n        new_data[test_name][\"sigma\"] = max(sample_sigma, sample_mean * 0.1)\n        with open(new_data_file_path, \"w\") as new_data_file:\n            json.dump(new_data, new_data_file, indent=4)\n",
    ".ci/pytorch/perf_test/get_stats.py": "import json\nimport sys\n\nimport numpy\n\n\nsample_data_list = sys.argv[1:]\nsample_data_list = [float(v.strip()) for v in sample_data_list]\n\nsample_mean = numpy.mean(sample_data_list)\nsample_sigma = numpy.std(sample_data_list)\n\ndata = {\n    \"mean\": sample_mean,\n    \"sigma\": sample_sigma,\n}\n\nprint(json.dumps(data))\n",
    ".ci/pytorch/perf_test/update_commit_hash.py": "import json\nimport sys\n\n\ndata_file_path = sys.argv[1]\ncommit_hash = sys.argv[2]\n\nwith open(data_file_path) as data_file:\n    data = json.load(data_file)\n\ndata[\"commit\"] = commit_hash\n\nwith open(data_file_path, \"w\") as data_file:\n    json.dump(data, data_file)\n",
    ".ci/pytorch/smoke_test/check_binary_symbols.py": "#!/usr/bin/env python3\nimport concurrent.futures\nimport distutils.sysconfig\nimport functools\nimport itertools\nimport os\nimport re\nfrom pathlib import Path\nfrom typing import Any\n\n\n# We also check that there are [not] cxx11 symbols in libtorch\n#\n# To check whether it is using cxx11 ABI, check non-existence of symbol:\nPRE_CXX11_SYMBOLS = (\n    \"std::basic_string<\",\n    \"std::list\",\n)\n# To check whether it is using pre-cxx11 ABI, check non-existence of symbol:\nCXX11_SYMBOLS = (\n    \"std::__cxx11::basic_string\",\n    \"std::__cxx11::list\",\n)\n# NOTE: Checking the above symbols in all namespaces doesn't work, because\n# devtoolset7 always produces some cxx11 symbols even if we build with old ABI,\n# and CuDNN always has pre-cxx11 symbols even if we build with new ABI using gcc 5.4.\n# Instead, we *only* check the above symbols in the following namespaces:\nLIBTORCH_NAMESPACE_LIST = (\n    \"c10::\",\n    \"at::\",\n    \"caffe2::\",\n    \"torch::\",\n)\n\n\ndef _apply_libtorch_symbols(symbols):\n    return [\n        re.compile(f\"{x}.*{y}\")\n        for (x, y) in itertools.product(LIBTORCH_NAMESPACE_LIST, symbols)\n    ]\n\n\nLIBTORCH_CXX11_PATTERNS = _apply_libtorch_symbols(CXX11_SYMBOLS)\n\nLIBTORCH_PRE_CXX11_PATTERNS = _apply_libtorch_symbols(PRE_CXX11_SYMBOLS)\n\n\n@functools.lru_cache(100)\ndef get_symbols(lib: str) -> list[tuple[str, str, str]]:\n    from subprocess import check_output\n\n    lines = check_output(f'nm \"{lib}\"|c++filt', shell=True)\n    return [x.split(\" \", 2) for x in lines.decode(\"latin1\").split(\"\\n\")[:-1]]\n\n\ndef grep_symbols(lib: str, patterns: list[Any]) -> list[str]:\n    def _grep_symbols(\n        symbols: list[tuple[str, str, str]], patterns: list[Any]\n    ) -> list[str]:\n        rc = []\n        for _s_addr, _s_type, s_name in symbols:\n            for pattern in patterns:\n                if pattern.match(s_name):\n                    rc.append(s_name)\n                    continue\n        return rc\n\n    all_symbols = get_symbols(lib)\n    num_workers = 32\n    chunk_size = (len(all_symbols) + num_workers - 1) // num_workers\n\n    def _get_symbols_chunk(i):\n        return all_symbols[i * chunk_size : (i + 1) * chunk_size]\n\n    with concurrent.futures.ThreadPoolExecutor(max_workers=32) as executor:\n        tasks = [\n            executor.submit(_grep_symbols, _get_symbols_chunk(i), patterns)\n            for i in range(num_workers)\n        ]\n        return functools.reduce(list.__add__, (x.result() for x in tasks), [])\n\n\ndef check_lib_symbols_for_abi_correctness(lib: str, pre_cxx11_abi: bool = True) -> None:\n    print(f\"lib: {lib}\")\n    cxx11_symbols = grep_symbols(lib, LIBTORCH_CXX11_PATTERNS)\n    pre_cxx11_symbols = grep_symbols(lib, LIBTORCH_PRE_CXX11_PATTERNS)\n    num_cxx11_symbols = len(cxx11_symbols)\n    num_pre_cxx11_symbols = len(pre_cxx11_symbols)\n    print(f\"num_cxx11_symbols: {num_cxx11_symbols}\")\n    print(f\"num_pre_cxx11_symbols: {num_pre_cxx11_symbols}\")\n    if pre_cxx11_abi:\n        if num_cxx11_symbols > 0:\n            raise RuntimeError(\n                f\"Found cxx11 symbols, but there shouldn't be any, see: {cxx11_symbols[:100]}\"\n            )\n        if num_pre_cxx11_symbols < 1000:\n            raise RuntimeError(\"Didn't find enough pre-cxx11 symbols.\")\n        # Check for no recursive iterators, regression test for https://github.com/pytorch/pytorch/issues/133437\n        rec_iter_symbols = grep_symbols(\n            lib, [re.compile(\"std::filesystem::recursive_directory_iterator.*\")]\n        )\n        if len(rec_iter_symbols) > 0:\n            raise RuntimeError(\n                f\"recursive_directory_iterator in used pre-CXX11 binaries, see; {rec_iter_symbols}\"\n            )\n    else:\n        if num_pre_cxx11_symbols > 0:\n            raise RuntimeError(\n                f\"Found pre-cxx11 symbols, but there shouldn't be any, see: {pre_cxx11_symbols[:100]}\"\n            )\n        if num_cxx11_symbols < 100:\n            raise RuntimeError(\"Didn't find enought cxx11 symbols\")\n\n\ndef main() -> None:\n    if \"install_root\" in os.environ:\n        install_root = Path(os.getenv(\"install_root\"))  # noqa: SIM112\n    else:\n        if os.getenv(\"PACKAGE_TYPE\") == \"libtorch\":\n            install_root = Path(os.getcwd())\n        else:\n            install_root = Path(distutils.sysconfig.get_python_lib()) / \"torch\"\n\n    libtorch_cpu_path = str(install_root / \"lib\" / \"libtorch_cpu.so\")\n    # NOTE: All binaries are built with cxx11abi now\n    check_lib_symbols_for_abi_correctness(libtorch_cpu_path, False)\n\n\nif __name__ == \"__main__\":\n    main()\n",
    ".ci/pytorch/smoke_test/max_autotune.py": "import argparse\n\nfrom torchvision import datasets, transforms\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import StepLR\n\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()  # noqa: UP008\n        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n        self.dropout1 = nn.Dropout(0.25)\n        self.dropout2 = nn.Dropout(0.5)\n        self.fc1 = nn.Linear(9216, 128)\n        self.fc2 = nn.Linear(128, 10)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = F.relu(x)\n        x = self.conv2(x)\n        x = F.relu(x)\n        x = F.max_pool2d(x, 2)\n        x = self.dropout1(x)\n        x = torch.flatten(x, 1)\n        x = self.fc1(x)\n        x = F.relu(x)\n        x = self.dropout2(x)\n        x = self.fc2(x)\n        output = F.log_softmax(x, dim=1)\n        return output\n\n\ndef train(args, model, device, train_loader, optimizer, epoch):\n    model.train()\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data, target = data.to(device), target.to(device)\n        optimizer.zero_grad()\n        output = model(data)\n        loss = F.nll_loss(output, target)\n        loss.backward()\n        optimizer.step()\n        if batch_idx % args.log_interval == 0:\n            print(\n                f\"Train Epoch: {epoch} \"\n                f\"[{batch_idx * len(data)}/{len(train_loader.dataset)} \"\n                f\"({100.0 * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}\"\n            )\n            if args.dry_run:\n                break\n\n\ndef test(model, device, test_loader):\n    model.eval()\n    test_loss = 0\n    correct = 0\n    with torch.no_grad():\n        for data, target in test_loader:\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            test_loss += F.nll_loss(\n                output, target, reduction=\"sum\"\n            ).item()  # sum up batch loss\n            pred = output.argmax(\n                dim=1, keepdim=True\n            )  # get the index of the max log-probability\n            correct += pred.eq(target.view_as(pred)).sum().item()\n\n    test_loss /= len(test_loader.dataset)\n\n    print(\n        f\"\\nTest set: Average loss: {test_loss:.4f}, \"\n        f\"Accuracy: {correct}/{len(test_loader.dataset)} \"\n        f\"({100.0 * correct / len(test_loader.dataset):.0f}%)\\n\"\n    )\n\n\ndef timed(fn):\n    start = torch.cuda.Event(enable_timing=True)\n    end = torch.cuda.Event(enable_timing=True)\n    start.record()\n    result = fn()\n    end.record()\n    torch.cuda.synchronize()\n    return result, start.elapsed_time(end) / 1000\n\n\ndef main():\n    # Training settings\n    parser = argparse.ArgumentParser(description=\"PyTorch MNIST Example\")\n    parser.add_argument(\n        \"--batch-size\",\n        type=int,\n        default=64,\n        metavar=\"N\",\n        help=\"input batch size for training (default: 64)\",\n    )\n    parser.add_argument(\n        \"--test-batch-size\",\n        type=int,\n        default=1000,\n        metavar=\"N\",\n        help=\"input batch size for testing (default: 1000)\",\n    )\n    parser.add_argument(\n        \"--epochs\",\n        type=int,\n        default=4,\n        metavar=\"N\",\n        help=\"number of epochs to train (default: 14)\",\n    )\n    parser.add_argument(\n        \"--lr\",\n        type=float,\n        default=1.0,\n        metavar=\"LR\",\n        help=\"learning rate (default: 1.0)\",\n    )\n    parser.add_argument(\n        \"--gamma\",\n        type=float,\n        default=0.7,\n        metavar=\"M\",\n        help=\"Learning rate step gamma (default: 0.7)\",\n    )\n    parser.add_argument(\n        \"--no-cuda\", action=\"store_true\", default=False, help=\"disables CUDA training\"\n    )\n    parser.add_argument(\n        \"--no-mps\",\n        action=\"store_true\",\n        default=False,\n        help=\"disables macOS GPU training\",\n    )\n    parser.add_argument(\n        \"--dry-run\",\n        action=\"store_true\",\n        default=False,\n        help=\"quickly check a single pass\",\n    )\n    parser.add_argument(\n        \"--seed\", type=int, default=1, metavar=\"S\", help=\"random seed (default: 1)\"\n    )\n    parser.add_argument(\n        \"--log-interval\",\n        type=int,\n        default=100,\n        metavar=\"N\",\n        help=\"how many batches to wait before logging training status\",\n    )\n    parser.add_argument(\n        \"--save-model\",\n        action=\"store_true\",\n        default=False,\n        help=\"For Saving the current Model\",\n    )\n    args = parser.parse_args()\n    use_cuda = not args.no_cuda and torch.cuda.is_available()\n    use_mps = not args.no_mps and torch.backends.mps.is_available()\n\n    torch.manual_seed(args.seed)\n    torch.backends.cuda.matmul.allow_tf32 = True\n\n    if use_cuda:\n        device = torch.device(\"cuda\")\n    elif use_mps:\n        device = torch.device(\"mps\")\n    else:\n        device = torch.device(\"cpu\")\n\n    train_kwargs = {\"batch_size\": args.batch_size}\n    test_kwargs = {\"batch_size\": args.test_batch_size}\n    if use_cuda:\n        cuda_kwargs = {\"num_workers\": 1, \"pin_memory\": True, \"shuffle\": True}\n        train_kwargs.update(cuda_kwargs)\n        test_kwargs.update(cuda_kwargs)\n\n    transform = transforms.Compose(\n        [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n    )\n    dataset1 = datasets.MNIST(\"../data\", train=True, download=True, transform=transform)\n    dataset2 = datasets.MNIST(\"../data\", train=False, transform=transform)\n    train_loader = torch.utils.data.DataLoader(dataset1, **train_kwargs)\n    test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)\n\n    model = Net().to(device)\n    opt_model = torch.compile(model, mode=\"max-autotune\")\n    optimizer = optim.Adadelta(opt_model.parameters(), lr=args.lr)\n\n    scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)\n    for epoch in range(1, args.epochs + 1):\n        print(\n            f\"Training Time: {timed(lambda: train(args, opt_model, device, train_loader, optimizer, epoch))[1]}\"\n        )\n        print(\n            f\"Evaluation Time: {timed(lambda: test(opt_model, device, test_loader))[1]}\"\n        )\n        scheduler.step()\n\n    if args.save_model:\n        torch.save(opt_model.state_dict(), \"mnist_cnn.pt\")\n\n\nif __name__ == \"__main__\":\n    main()\n",
    ".ci/pytorch/smoke_test/smoke_test.py": "import argparse\nimport importlib\nimport json\nimport os\nimport re\nimport subprocess\nimport sys\nfrom pathlib import Path\nfrom tempfile import NamedTemporaryFile\n\nimport torch\nimport torch._dynamo\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nif \"MATRIX_GPU_ARCH_VERSION\" in os.environ:\n    gpu_arch_ver = os.getenv(\"MATRIX_GPU_ARCH_VERSION\")\nelse:\n    gpu_arch_ver = os.getenv(\"GPU_ARCH_VERSION\")  # Use fallback if available\ngpu_arch_type = os.getenv(\"MATRIX_GPU_ARCH_TYPE\")\nchannel = os.getenv(\"MATRIX_CHANNEL\")\npackage_type = os.getenv(\"MATRIX_PACKAGE_TYPE\")\ntarget_os = os.getenv(\"TARGET_OS\", sys.platform)\nBASE_DIR = Path(__file__).parent.parent.parent\n\nis_cuda_system = gpu_arch_type == \"cuda\"\nNIGHTLY_ALLOWED_DELTA = 3\n\nMODULES = [\n    {\n        \"name\": \"torchvision\",\n        \"repo\": \"https://github.com/pytorch/vision.git\",\n        \"smoke_test\": \"./vision/test/smoke_test.py\",\n        \"extension\": \"extension\",\n        \"repo_name\": \"vision\",\n    },\n    {\n        \"name\": \"torchaudio\",\n        \"repo\": \"https://github.com/pytorch/audio.git\",\n        \"smoke_test\": \"./audio/test/smoke_test/smoke_test.py --no-ffmpeg\",\n        \"extension\": \"_extension\",\n        \"repo_name\": \"audio\",\n    },\n]\n\n\nclass Net(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n        self.fc1 = nn.Linear(9216, 1)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = F.max_pool2d(x, 2)\n        x = torch.flatten(x, 1)\n        output = self.fc1(x)\n        return output\n\n\ndef load_json_from_basedir(filename: str):\n    try:\n        with open(BASE_DIR / filename) as fptr:\n            return json.load(fptr)\n    except FileNotFoundError as exc:\n        raise ImportError(f\"File {filename} not found error: {exc.strerror}\") from exc\n    except json.JSONDecodeError as exc:\n        raise ImportError(f\"Invalid JSON {filename}\") from exc\n\n\ndef read_release_matrix():\n    return load_json_from_basedir(\"release_matrix.json\")\n\n\ndef test_numpy():\n    try:\n        import numpy as np\n\n        x = np.arange(5)\n        torch.tensor(x)\n    except ImportError:\n        print(\"Numpy check skipped. Numpy is not installed.\")\n\n\ndef check_version(package: str) -> None:\n    release_version = os.getenv(\"RELEASE_VERSION\")\n    # if release_version is specified, use it to validate the packages\n    if release_version:\n        release_matrix = read_release_matrix()\n        stable_version = release_matrix[\"torch\"]\n    else:\n        stable_version = os.getenv(\"MATRIX_STABLE_VERSION\")\n\n    # only makes sense to check nightly package where dates are known\n    if channel == \"nightly\":\n        check_nightly_binaries_date(package)\n    elif stable_version is not None:\n        if not torch.__version__.startswith(stable_version):\n            raise RuntimeError(\n                f\"Torch version mismatch, expected {stable_version} for channel {channel}. But its {torch.__version__}\"\n            )\n\n        if release_version and package == \"all\":\n            for module in MODULES:\n                imported_module = importlib.import_module(module[\"name\"])\n                module_version = imported_module.__version__\n                if not module_version.startswith(release_matrix[module[\"name\"]]):\n                    raise RuntimeError(\n                        f\"{module['name']} version mismatch, expected: \\\n                            {release_matrix[module['name']]} for channel {channel}. But its {module_version}\"\n                    )\n                else:\n                    print(\n                        f\"{module['name']} version actual: {module_version} expected: \\\n                        {release_matrix[module['name']]} for channel {channel}.\"\n                    )\n\n    else:\n        print(f\"Skip version check for channel {channel} as stable version is None\")\n\n\ndef check_nightly_binaries_date(package: str) -> None:\n    from datetime import datetime\n\n    format_dt = \"%Y%m%d\"\n\n    date_t_str = re.findall(\"dev\\\\d+\", torch.__version__)\n    date_t_delta = datetime.now() - datetime.strptime(date_t_str[0][3:], format_dt)\n    if date_t_delta.days >= NIGHTLY_ALLOWED_DELTA:\n        raise RuntimeError(\n            f\"the binaries are from {date_t_str} and are more than {NIGHTLY_ALLOWED_DELTA} days old!\"\n        )\n\n    if package == \"all\":\n        for module in MODULES:\n            imported_module = importlib.import_module(module[\"name\"])\n            module_version = imported_module.__version__\n            date_m_str = re.findall(\"dev\\\\d+\", module_version)\n            date_m_delta = datetime.now() - datetime.strptime(\n                date_m_str[0][3:], format_dt\n            )\n            print(f\"Nightly date check for {module['name']} version {module_version}\")\n            if date_m_delta.days > NIGHTLY_ALLOWED_DELTA:\n                raise RuntimeError(\n                    f\"Expected {module['name']} to be less then {NIGHTLY_ALLOWED_DELTA} days. But its {date_m_delta}\"\n                )\n\n\ndef test_cuda_runtime_errors_captured() -> None:\n    cuda_exception_missed = True\n    try:\n        print(\"Testing test_cuda_runtime_errors_captured\")\n        torch._assert_async(torch.tensor(0, device=\"cuda\"))\n        torch._assert_async(torch.tensor(0 + 0j, device=\"cuda\"))\n    except RuntimeError as e:\n        if re.search(\"CUDA\", f\"{e}\"):\n            print(f\"Caught CUDA exception with success: {e}\")\n            cuda_exception_missed = False\n        else:\n            raise e\n    if cuda_exception_missed:\n        raise RuntimeError(\"Expected CUDA RuntimeError but have not received!\")\n\n\ndef test_cuda_gds_errors_captured() -> None:\n    major_version = int(torch.version.cuda.split(\".\")[0])\n    minor_version = int(torch.version.cuda.split(\".\")[1])\n\n    if target_os == \"windows\":\n        print(f\"{target_os} is not supported for GDS smoke test\")\n        return\n\n    if major_version < 12 or (major_version == 12 and minor_version < 6):\n        print(\"CUDA version is not supported for GDS smoke test\")\n        return\n\n    cuda_exception_missed = True\n    try:\n        print(\"Testing test_cuda_gds_errors_captured\")\n        with NamedTemporaryFile() as f:\n            torch.cuda.gds.GdsFile(f.name, os.O_CREAT | os.O_RDWR)\n    except RuntimeError as e:\n        expected_error = \"cuFileHandleRegister failed\"\n        if re.search(expected_error, f\"{e}\"):\n            print(f\"Caught CUDA exception with success: {e}\")\n            cuda_exception_missed = False\n        else:\n            raise e\n    if cuda_exception_missed:\n        raise RuntimeError(\n            \"Expected cuFileHandleRegister failed RuntimeError but have not received!\"\n        )\n\n\ndef smoke_test_cuda(\n    package: str, runtime_error_check: str, torch_compile_check: str\n) -> None:\n    if not torch.cuda.is_available() and is_cuda_system:\n        raise RuntimeError(f\"Expected CUDA {gpu_arch_ver}. However CUDA is not loaded.\")\n\n    if package == \"all\" and is_cuda_system:\n        for module in MODULES:\n            imported_module = importlib.import_module(module[\"name\"])\n            # TBD for vision move extension module to private so it will\n            # be _extention.\n            version = \"N/A\"\n            if module[\"extension\"] == \"extension\":\n                version = imported_module.extension._check_cuda_version()\n            else:\n                version = imported_module._extension._check_cuda_version()\n            print(f\"{module['name']} CUDA: {version}\")\n\n    # torch.compile is available on macos-arm64 and Linux for python 3.8-3.13\n    if (\n        torch_compile_check == \"enabled\"\n        and sys.version_info < (3, 14, 0)\n        and target_os in [\"linux\", \"linux-aarch64\", \"macos-arm64\", \"darwin\"]\n    ):\n        smoke_test_compile(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    if torch.cuda.is_available():\n        if torch.version.cuda != gpu_arch_ver:\n            raise RuntimeError(\n                f\"Wrong CUDA version. Loaded: {torch.version.cuda} Expected: {gpu_arch_ver}\"\n            )\n        print(f\"torch cuda: {torch.version.cuda}\")\n        # todo add cudnn version validation\n        print(f\"torch cudnn: {torch.backends.cudnn.version()}\")\n        print(f\"cuDNN enabled? {torch.backends.cudnn.enabled}\")\n\n        torch.cuda.init()\n        print(\"CUDA initialized successfully\")\n        print(f\"Number of CUDA devices: {torch.cuda.device_count()}\")\n        for i in range(torch.cuda.device_count()):\n            print(f\"Device {i}: {torch.cuda.get_device_name(i)}\")\n\n        # nccl is availbale only on Linux\n        if sys.platform in [\"linux\", \"linux2\"]:\n            print(f\"torch nccl version: {torch.cuda.nccl.version()}\")\n\n        if runtime_error_check == \"enabled\":\n            test_cuda_runtime_errors_captured()\n\n\ndef smoke_test_conv2d() -> None:\n    import torch.nn as nn\n\n    print(\"Testing smoke_test_conv2d\")\n    # With square kernels and equal stride\n    m = nn.Conv2d(16, 33, 3, stride=2)\n    # non-square kernels and unequal stride and with padding\n    m = nn.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2))\n    assert m is not None\n    # non-square kernels and unequal stride and with padding and dilation\n    basic_conv = nn.Conv2d(\n        16, 33, (3, 5), stride=(2, 1), padding=(4, 2), dilation=(3, 1)\n    )\n    input = torch.randn(20, 16, 50, 100)\n    output = basic_conv(input)\n\n    if is_cuda_system:\n        print(\"Testing smoke_test_conv2d with cuda\")\n        conv = nn.Conv2d(3, 3, 3).cuda()\n        x = torch.randn(1, 3, 24, 24, device=\"cuda\")\n        with torch.cuda.amp.autocast():\n            out = conv(x)\n        assert out is not None\n\n        supported_dtypes = [torch.float16, torch.float32, torch.float64]\n        for dtype in supported_dtypes:\n            print(f\"Testing smoke_test_conv2d with cuda for {dtype}\")\n            conv = basic_conv.to(dtype).cuda()\n            input = torch.randn(20, 16, 50, 100, device=\"cuda\").type(dtype)\n            output = conv(input)\n            assert output is not None\n\n\ndef test_linalg(device=\"cpu\") -> None:\n    print(f\"Testing smoke_test_linalg on {device}\")\n    A = torch.randn(5, 3, device=device)\n    U, S, Vh = torch.linalg.svd(A, full_matrices=False)\n    assert (\n        U.shape == A.shape\n        and S.shape == torch.Size([3])\n        and Vh.shape == torch.Size([3, 3])\n    )\n    torch.dist(A, U @ torch.diag(S) @ Vh)\n\n    U, S, Vh = torch.linalg.svd(A)\n    assert (\n        U.shape == torch.Size([5, 5])\n        and S.shape == torch.Size([3])\n        and Vh.shape == torch.Size([3, 3])\n    )\n    torch.dist(A, U[:, :3] @ torch.diag(S) @ Vh)\n\n    A = torch.randn(7, 5, 3, device=device)\n    U, S, Vh = torch.linalg.svd(A, full_matrices=False)\n    torch.dist(A, U @ torch.diag_embed(S) @ Vh)\n\n    if device == \"cuda\":\n        supported_dtypes = [torch.float32, torch.float64]\n        for dtype in supported_dtypes:\n            print(f\"Testing smoke_test_linalg with cuda for {dtype}\")\n            A = torch.randn(20, 16, 50, 100, device=device, dtype=dtype)\n            torch.linalg.svd(A)\n\n\ndef smoke_test_compile(device: str = \"cpu\") -> None:\n    supported_dtypes = [torch.float16, torch.float32, torch.float64]\n\n    def foo(x: torch.Tensor) -> torch.Tensor:\n        return torch.sin(x) + torch.cos(x)\n\n    for dtype in supported_dtypes:\n        print(f\"Testing smoke_test_compile for {device} and {dtype}\")\n        x = torch.rand(3, 3, device=device).type(dtype)\n        x_eager = foo(x)\n        x_pt2 = torch.compile(foo)(x)\n        torch.testing.assert_close(x_eager, x_pt2)\n\n    # Check that SIMD were detected for the architecture\n    if device == \"cpu\":\n        from torch._inductor.codecache import pick_vec_isa\n\n        isa = pick_vec_isa()\n        if not isa:\n            raise RuntimeError(\"Can't detect vectorized ISA for CPU\")\n        print(f\"Picked CPU ISA {type(isa).__name__} bit width {isa.bit_width()}\")\n\n    # Reset torch dynamo since we are changing mode\n    torch._dynamo.reset()\n    dtype = torch.float32\n    torch.set_float32_matmul_precision(\"high\")\n    print(f\"Testing smoke_test_compile with mode 'max-autotune' for {dtype}\")\n    x = torch.rand(64, 1, 28, 28, device=device).type(torch.float32)\n    model = Net().to(device=device)\n    x_pt2 = torch.compile(model, mode=\"max-autotune\")(x)\n\n\ndef smoke_test_modules():\n    cwd = os.getcwd()\n    for module in MODULES:\n        if module[\"repo\"]:\n            if not os.path.exists(f\"{cwd}/{module['repo_name']}\"):\n                print(f\"Path does not exist: {cwd}/{module['repo_name']}\")\n                try:\n                    subprocess.check_output(\n                        f\"git clone --depth 1 {module['repo']}\",\n                        stderr=subprocess.STDOUT,\n                        shell=True,\n                    )\n                except subprocess.CalledProcessError as exc:\n                    raise RuntimeError(\n                        f\"Cloning {module['repo']} FAIL: {exc.returncode} Output: {exc.output}\"\n                    ) from exc\n            try:\n                smoke_test_command = f\"python3 {module['smoke_test']}\"\n                if target_os == \"windows\":\n                    smoke_test_command = f\"python {module['smoke_test']}\"\n                output = subprocess.check_output(\n                    smoke_test_command,\n                    stderr=subprocess.STDOUT,\n                    shell=True,\n                    universal_newlines=True,\n                )\n            except subprocess.CalledProcessError as exc:\n                raise RuntimeError(\n                    f\"Module {module['name']} FAIL: {exc.returncode} Output: {exc.output}\"\n                ) from exc\n            else:\n                print(f\"Output: \\n{output}\\n\")\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--package\",\n        help=\"Package to include in smoke testing\",\n        type=str,\n        choices=[\"all\", \"torchonly\"],\n        default=\"all\",\n    )\n    parser.add_argument(\n        \"--runtime-error-check\",\n        help=\"No Runtime Error check\",\n        type=str,\n        choices=[\"enabled\", \"disabled\"],\n        default=\"enabled\",\n    )\n    parser.add_argument(\n        \"--torch-compile-check\",\n        help=\"Check torch compile\",\n        type=str,\n        choices=[\"enabled\", \"disabled\"],\n        default=\"enabled\",\n    )\n    return parser.parse_args()\n\n\ndef main() -> None:\n    options = parse_args()\n    print(f\"torch: {torch.__version__}\")\n    print(torch.__config__.parallel_info())\n    # All PyTorch binary builds should be built with OpenMP\n    if not torch.backends.openmp.is_available():\n        raise RuntimeError(\"PyTorch must be built with OpenMP support\")\n\n    check_version(options.package)\n    smoke_test_conv2d()\n    test_linalg()\n    test_numpy()\n\n    if is_cuda_system:\n        test_linalg(\"cuda\")\n        test_cuda_gds_errors_captured()\n\n    if options.package == \"all\":\n        smoke_test_modules()\n\n    smoke_test_cuda(\n        options.package, options.runtime_error_check, options.torch_compile_check\n    )\n\n\nif __name__ == \"__main__\":\n    main()\n",
    ".ci/pytorch/test_example_code/cnn_smoke.py": "r\"\"\"\nIt's used to check basic rnn features with cuda.\nFor example, it would throw exception if some components are missing\n\"\"\"\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\n\nclass SimpleCNN(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = nn.Conv2d(1, 1, 3)\n        self.pool = nn.MaxPool2d(2, 2)\n\n    def forward(self, inputs):\n        output = self.pool(F.relu(self.conv(inputs)))\n        output = output.view(1)\n        return output\n\n\n# Mock one infer\ndevice = torch.device(\"cuda:0\")\nnet = SimpleCNN().to(device)\nnet_inputs = torch.rand((1, 1, 5, 5), device=device)\noutputs = net(net_inputs)\nprint(outputs)\n\ncriterion = nn.MSELoss()\noptimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.1)\n\n# Mock one step training\nlabel = torch.full((1,), 1.0, dtype=torch.float, device=device)\nloss = criterion(outputs, label)\nloss.backward()\noptimizer.step()\n",
    ".ci/pytorch/test_example_code/cnn_smoke_win_arm64.py": "r\"\"\"\nIt's used to check basic rnn features with cpu-only.\nFor example, it would throw exception if some components are missing\n\"\"\"\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\n\nclass SimpleCNN(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv = nn.Conv2d(1, 1, 3)\n        self.pool = nn.MaxPool2d(2, 2)\n\n    def forward(self, inputs):\n        output = self.pool(F.relu(self.conv(inputs)))\n        output = output.view(1)\n        return output\n\n\ntry:\n    # Mock one infer\n    net = SimpleCNN()\n    net_inputs = torch.rand((1, 1, 5, 5))\n    outputs = net(net_inputs)\n    print(outputs)\n\n    criterion = nn.MSELoss()\n    optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.1)\n\n    # Mock one step training\n    label = torch.full((1,), 1.0, dtype=torch.float)\n    loss = criterion(outputs, label)\n    loss.backward()\n    optimizer.step()\n\nexcept Exception as e:\n    print(f\"An error occurred: {e}\")\n",
    ".ci/pytorch/test_example_code/rnn_smoke.py": "r\"\"\"\nIt's used to check basic rnn features with cuda.\nFor example, it would throw exception if missing some components are missing\n\"\"\"\n\nimport torch\nimport torch.nn as nn\n\n\ndevice = torch.device(\"cuda:0\")\nrnn = nn.RNN(10, 20, 2).to(device)\ninputs = torch.randn(5, 3, 10).to(device)\nh0 = torch.randn(2, 3, 20).to(device)\noutput, hn = rnn(inputs, h0)\n"
  },
  "requirements": "# Python dependencies required for development\nastunparse\ncmake\nexpecttest>=0.3.0\nfilelock\nfsspec\nhypothesis\njinja2\nlintrunner ; platform_machine != \"s390x\"\nnetworkx\nninja\nnumpy\noptree>=0.13.0\npackaging\npsutil\npyyaml\nrequests\n# issue on Windows after >= 75.8.2 - https://github.com/pytorch/pytorch/issues/148877\nsetuptools>=62.3.0,<75.9\nsympy>=1.13.3\ntypes-dataclasses\ntyping-extensions>=4.10.0\n"
}